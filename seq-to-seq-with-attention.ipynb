{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"_cell_guid":"f4ce1158-c562-476f-8f7d-0325f0b787c4","_uuid":"549ce73a-e98b-42bd-a1d8-2cab5faae7d5","collapsed":false,"execution":{"iopub.execute_input":"2024-05-17T11:10:26.947482Z","iopub.status.busy":"2024-05-17T11:10:26.947013Z","iopub.status.idle":"2024-05-17T11:10:26.956335Z","shell.execute_reply":"2024-05-17T11:10:26.955271Z","shell.execute_reply.started":"2024-05-17T11:10:26.947440Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","import copy\n","from torch.utils.data import Dataset, DataLoader\n","import gc\n","import random\n","import wandb\n","import csv\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from matplotlib.font_manager import FontProperties"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T09:23:53.083417Z","iopub.status.busy":"2024-05-17T09:23:53.083123Z","iopub.status.idle":"2024-05-17T09:23:55.368391Z","shell.execute_reply":"2024-05-17T09:23:55.367434Z","shell.execute_reply.started":"2024-05-17T09:23:53.083391Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login(key=\"62cfafb7157dfba7fdd6132ac9d757ccd913aaaf\")"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"c7dcb64a-f20a-4fa2-8b31-9801c3a762b8","_uuid":"b6363452-eb4f-4705-aa3f-fdec65f8e9db","collapsed":false,"execution":{"iopub.execute_input":"2024-05-17T09:23:55.370091Z","iopub.status.busy":"2024-05-17T09:23:55.369619Z","iopub.status.idle":"2024-05-17T09:23:55.624451Z","shell.execute_reply":"2024-05-17T09:23:55.623602Z","shell.execute_reply.started":"2024-05-17T09:23:55.370066Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Checking if CUDA is available, else use CPU\n","print(device)  # Printing the device being used (CUDA or CPU)\n","END_TOKEN = '>'  # Defining the end token for sequences\n","START_TOKEN = '<'  # Defining the start token for sequences\n","PAD_TOKEN = '_'  # Defining the padding token for sequences\n","TEACHER_FORCING_RATIO = 0.5  # Ratio of teacher forcing during training\n","\n","# Paths to the train, test, and validation CSV files\n","train_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_train.csv\"\n","test_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_test.csv\"\n","val_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_valid.csv\"\n","\n","# Reading the train, test, and validation CSV files into pandas dataframes\n","train_df = pd.read_csv(train_csv, header=None)\n","test_df = pd.read_csv(test_csv, header=None)\n","val_df = pd.read_csv(val_csv, header=None)\n","\n","# Extracting source and target sequences from train, test, and validation dataframes\n","train_source, train_target = train_df[0].to_numpy(), train_df[1].to_numpy()\n","val_source, val_target = val_df[0].to_numpy(), val_df[1].to_numpy()\n","test_source, test_target = test_df[0].to_numpy(), test_df[1].to_numpy()"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"fb22b602-e1fc-496b-93dc-f4d3178870d5","_uuid":"3708abf3-c2df-4e8d-8cfe-af3faab7bc53","collapsed":false,"execution":{"iopub.execute_input":"2024-05-17T09:23:55.627052Z","iopub.status.busy":"2024-05-17T09:23:55.626769Z","iopub.status.idle":"2024-05-17T09:23:55.644813Z","shell.execute_reply":"2024-05-17T09:23:55.643875Z","shell.execute_reply.started":"2024-05-17T09:23:55.627028Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Function to add padding to source sequences\n","def add_padding(source_data, MAX_LENGTH):\n","    \"\"\"\n","    Add padding to source sequences and truncate if necessary.\n","    \n","    Args:\n","    - source_data: List of source sequences\n","    - MAX_LENGTH: Maximum length of source sequences\n","    \n","    Returns:\n","    - padded_source_strings: List of padded source sequences\n","    \"\"\"\n","    padded_source_strings = []\n","    for i in range(len(source_data)):\n","        source_str = START_TOKEN + source_data[i] + END_TOKEN  # Add start and end tokens\n","        source_str = source_str[:MAX_LENGTH]  # Truncate if longer than MAX_LENGTH\n","        source_str += PAD_TOKEN * (MAX_LENGTH - len(source_str))  # Pad with PAD_TOKEN\n","\n","        padded_source_strings.append(source_str)\n","        \n","    return padded_source_strings\n","\n","\n","# Function to convert source strings to sequences of indices\n","def generate_string_to_sequence(source_data, source_char_index_dict):\n","    \"\"\"\n","    Convert source strings to sequences of indices using char_index_dict.\n","    \n","    Args:\n","    - source_data: List of padded source strings\n","    - source_char_index_dict: Dictionary mapping characters to their indices\n","    \n","    Returns:\n","    - source_sequences: Padded sequence of character indices\n","    \"\"\"\n","    source_sequences = []\n","    for i in range(len(source_data)):\n","        source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","    source_sequences = pad_sequence(source_sequences, batch_first=True, padding_value=2)\n","    return source_sequences\n","\n","\n","# Function to convert characters to their corresponding indices\n","def get_chars(string, char_index_dict):\n","    \"\"\"\n","    Convert characters in a string to their corresponding indices using char_index_dict.\n","    \n","    Args:\n","    - string: Input string\n","    - char_index_dict: Dictionary mapping characters to their indices\n","    \n","    Returns:\n","    - chars_indexes: List of character indices\n","    \"\"\"\n","    chars_indexes = []\n","    for char in string:\n","        chars_indexes.append(char_index_dict[char])\n","    return torch.tensor(chars_indexes, device=device)\n","\n","\n","# Preprocess the data, including adding padding, generating sequences, and updating dictionaries\n","def preprocess_data(source_data, target_data):\n","    \"\"\"\n","    Preprocess source and target data.\n","    \n","    Args:\n","    - source_data: List of source strings\n","    - target_data: List of target strings\n","    \n","    Returns:\n","    - data: Preprocessed data dictionary\n","    \"\"\"\n","    data = {\n","        \"source_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n","        \"target_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n","        \"source_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n","        \"source_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n","        \"target_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n","        \"target_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n","        \"source_len\": 3,\n","        \"target_len\": 3,\n","        \"source_data\": source_data,\n","        \"target_data\": target_data,\n","        \"source_data_seq\": [],\n","        \"target_data_seq\": []\n","    }\n","    \n","    # Calculate the maximum length of input and output sequences\n","    data[\"INPUT_MAX_LENGTH\"] = max(len(string) for string in source_data) + 2\n","    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n","\n","    # Pad the source and target sequences and update character dictionaries\n","    padded_source_strings = add_padding(source_data, data[\"INPUT_MAX_LENGTH\"])\n","    padded_target_strings = add_padding(target_data, data[\"OUTPUT_MAX_LENGTH\"])\n","    \n","    for i in range(len(padded_source_strings)):\n","        for char in padded_source_strings[i]:\n","            if data[\"source_char_index\"].get(char) is None:\n","                data[\"source_chars\"].append(char)\n","                idx = len(data[\"source_chars\"]) - 1\n","                data[\"source_char_index\"][char] = idx\n","                data[\"source_index_char\"][idx] = char\n","        for char in padded_target_strings[i]:\n","            if data[\"target_char_index\"].get(char) is None:\n","                data[\"target_chars\"].append(char)\n","                idx = len(data[\"target_chars\"]) - 1\n","                data[\"target_char_index\"][char] = idx\n","                data[\"target_index_char\"][idx] = char\n","\n","    # Generate sequences of indexes for source and target data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings, data['source_char_index'])\n","    data['target_data_seq'] = generate_string_to_sequence(padded_target_strings, data['target_char_index'])\n","    \n","    # Update lengths of source and target character lists\n","    data[\"source_len\"] = len(data[\"source_chars\"])\n","    data[\"target_len\"] = len(data[\"target_chars\"])\n","    \n","    return data\n"]},{"cell_type":"code","execution_count":27,"metadata":{"_cell_guid":"52035e98-753c-4cd0-8dec-2cec35aab863","_uuid":"58e2b696-e336-4cb3-b6e6-d14eac238c96","execution":{"iopub.execute_input":"2024-05-17T10:25:39.055318Z","iopub.status.busy":"2024-05-17T10:25:39.054926Z","iopub.status.idle":"2024-05-17T10:25:39.090372Z","shell.execute_reply":"2024-05-17T10:25:39.089381Z","shell.execute_reply.started":"2024-05-17T10:25:39.055290Z"},"trusted":true},"outputs":[],"source":["def get_cell_type(cell_type):\n","    # Function to return the appropriate RNN cell based on the specified type\n","    if(cell_type == \"RNN\"):\n","        return nn.RNN\n","    elif(cell_type == \"LSTM\"):\n","        return nn.LSTM\n","    elif(cell_type == \"GRU\"):\n","        return nn.GRU\n","    else:\n","        print(\"Specify correct cell type\")\n","\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        # Initialize the attention mechanism module\n","        super(Attention, self).__init__()\n","        self.Wa = nn.Linear(hidden_size, hidden_size)\n","        self.Ua = nn.Linear(hidden_size, hidden_size)\n","        self.Va = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, query, keys):\n","        # Forward pass of the attention mechanism\n","        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n","        scores = scores.squeeze().unsqueeze(1)\n","        weights = F.softmax(scores, dim=0)\n","        weights = weights.permute(2,1,0)\n","        keys = keys.permute(1,0,2)\n","        context = torch.bmm(weights, keys)\n","        return context, weights\n","\n","class Encoder(nn.Module):\n","    def __init__(self, h_params, data, device ):\n","        # Initialize the Encoder module\n","        super(Encoder, self).__init__()\n","        # Embedding layer for input characters\n","        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embd_dim\"])\n","        # RNN cell for encoding\n","        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],num_layers=h_params[\"number_of_layers\"], batch_first=True)\n","        self.device=device\n","        self.h_params = h_params\n","        self.data = data\n","        \n","    def forward(self, input , encoder_curr_state):\n","        # Forward pass of the Encoder module\n","        input_length = self.data[\"INPUT_MAX_LENGTH\"]\n","        batch_size = self.h_params[\"batch_size\"]\n","        hidden_neurons = self.h_params[\"hidden_layer_neurons\"]\n","        layers = self.h_params[\"number_of_layers\"]\n","        encoder_states  = torch.zeros(input_length, layers, batch_size, hidden_neurons, device=self.device )\n","        for i in range(input_length):\n","            current_input = input[:, i].view(batch_size,1)\n","            _, encoder_curr_state = self.forward_step(current_input, encoder_curr_state)\n","            if self.h_params[\"cell_type\"] == \"LSTM\":\n","                encoder_states[i] = encoder_curr_state[1]\n","            else:\n","                encoder_states[i] = encoder_curr_state\n","        return encoder_states, encoder_curr_state\n","    \n","    def forward_step(self, current_input, prev_state):\n","        # Perform forward pass for one time step\n","        embd_input = self.embedding(current_input)\n","        output, prev_state = self.cell(embd_input, prev_state)\n","        return output, prev_state\n","        \n","    def getInitialState(self):\n","        # Initialize initial hidden state for encoder\n","        return torch.zeros(self.h_params[\"number_of_layers\"],self.h_params[\"batch_size\"],self.h_params[\"hidden_layer_neurons\"], device=self.device)\n","\n","class Decoder(nn.Module):\n","    def __init__(self, h_params, data,device):\n","        # Initialize the Decoder module\n","        super(Decoder, self).__init__()\n","        # Attention mechanism\n","        self.attention = Attention(h_params[\"hidden_layer_neurons\"]).to(device)\n","        # Embedding layer for target characters\n","        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embd_dim\"])\n","        # RNN cell for decoding\n","        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"hidden_layer_neurons\"] +h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],num_layers=h_params[\"number_of_layers\"], batch_first=True)\n","        # Fully connected layer for output\n","        self.fc = nn.Linear(h_params[\"hidden_layer_neurons\"], data[\"target_len\"])\n","        # Softmax activation for output probabilities\n","        self.softmax = nn.LogSoftmax(dim=2)\n","        self.h_params = h_params\n","        self.data = data\n","        self.device = device\n","\n","    def forward(self, decoder_current_state, encoder_final_layers, target_batch, loss_fn, teacher_forcing_enabled=True):\n","        # Forward pass of the Decoder module\n","        batch_size = self.h_params[\"batch_size\"]\n","        decoder_current_input = torch.full((batch_size,1),self.data[\"target_char_index\"][START_TOKEN], device=self.device)\n","        embd_input = self.embedding(decoder_current_input)\n","        curr_embd = F.relu(embd_input)\n","        decoder_actual_output = []\n","        attentions = []\n","        loss = 0\n","        \n","        use_teacher_forcing = False\n","        if(teacher_forcing_enabled):\n","            use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n","        for i in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n","            # Perform one step of decoding\n","            decoder_output, decoder_current_state, attn_weights = self.forward_step(decoder_current_input, decoder_current_state, encoder_final_layers)\n","            attentions.append(attn_weights)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_current_input = topi.squeeze().detach()\n","            decoder_actual_output.append(decoder_current_input)\n","\n","            if(target_batch==None):\n","                decoder_current_input = decoder_current_input.view(self.h_params[\"batch_size\"], 1)\n","            else:\n","                curr_target_chars = target_batch[:, i]\n","                if(i<self.data[\"OUTPUT_MAX_LENGTH\"]-1):\n","                    if use_teacher_forcing:\n","                        decoder_current_input = target_batch[:, i+1].view(self.h_params[\"batch_size\"], 1)\n","                    else:\n","                        decoder_current_input = decoder_current_input.view(self.h_params[\"batch_size\"], 1)\n","                decoder_output = decoder_output[:, -1, :]\n","                loss+=(loss_fn(decoder_output, curr_target_chars))\n","\n","        decoder_actual_output = torch.cat(decoder_actual_output,dim=0).view(self.data[\"OUTPUT_MAX_LENGTH\"], self.h_params[\"batch_size\"]).transpose(0,1)\n","\n","        correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n","        return decoder_actual_output, attentions, loss, correct\n","    \n","    def forward_step(self, current_input, prev_state, encoder_final_layers):\n","        # Perform one step of decoding\n","        embd_input = self.embedding(current_input)\n","        if self.h_params[\"cell_type\"] == \"LSTM\":\n","            context , attn_weights = self.attention(prev_state[1][-1,:,:], encoder_final_layers)\n","        else:\n","            context , attn_weights = self.attention(prev_state[-1,:,:], encoder_final_layers)\n","        curr_embd = F.relu(embd_input)\n","        input_gru = torch.cat((curr_embd, context), dim=2)\n","        output, prev_state = self.cell(input_gru, prev_state)\n","        output = self.softmax(self.fc(output))\n","        return output, prev_state, attn_weights\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T09:23:55.681580Z","iopub.status.busy":"2024-05-17T09:23:55.681207Z","iopub.status.idle":"2024-05-17T09:23:55.697357Z","shell.execute_reply":"2024-05-17T09:23:55.696555Z","shell.execute_reply.started":"2024-05-17T09:23:55.681551Z"},"trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, data):\n","        self.source_data_seq = data[0]\n","        self.target_data_seq = data[1]\n","    \n","    def __len__(self):\n","        return len(self.source_data_seq)\n","    \n","    def __getitem__(self, idx):\n","        source_data = self.source_data_seq[idx]\n","        target_data = self.target_data_seq[idx]\n","        return source_data, target_data\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T09:23:55.698792Z","iopub.status.busy":"2024-05-17T09:23:55.698486Z","iopub.status.idle":"2024-05-17T09:23:55.709323Z","shell.execute_reply":"2024-05-17T09:23:55.708461Z","shell.execute_reply.started":"2024-05-17T09:23:55.698768Z"},"trusted":true},"outputs":[],"source":["def evaluate(encoder, decoder, data, dataloader, device, h_params, loss_fn, use_teacher_forcing = False):\n","    # Function to evaluate the performance of the model on a dataset\n","    correct_predictions = 0\n","    total_loss = 0\n","    total_predictions = len(dataloader.dataset)\n","    number_of_batches = len(dataloader)\n","    encoder.eval()\n","    decoder.eval()\n","    \n","    with torch.no_grad():\n","        for batch_num, (source_batch, target_batch) in enumerate(dataloader):\n","\n","            encoder_initial_state = encoder.getInitialState()\n","            if h_params[\"cell_type\"] == \"LSTM\":\n","                encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n","            encoder_states, encoder_final_state = encoder(source_batch,encoder_initial_state)\n","\n","            decoder_current_state = encoder_final_state\n","            encoder_final_layer_states = encoder_states[:, -1, :, :]\n","\n","            loss = 0\n","            correct = 0\n","\n","            decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n","\n","            correct_predictions+=correct\n","            total_loss +=loss\n","\n","        accuracy = correct_predictions / total_predictions\n","        total_loss /= number_of_batches\n","\n","        return accuracy, total_loss\n"]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"0dc20b89-db7b-4230-a7ec-e76d1896adee","_uuid":"c743c5b1-1106-4388-a594-e4e301225bad","execution":{"iopub.execute_input":"2024-05-17T09:23:55.710947Z","iopub.status.busy":"2024-05-17T09:23:55.710635Z","iopub.status.idle":"2024-05-17T09:23:55.725021Z","shell.execute_reply":"2024-05-17T09:23:55.724181Z","shell.execute_reply.started":"2024-05-17T09:23:55.710922Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["def make_strings(data, source, target, output):\n","    # Function to convert indices to strings for source, target, and output sequences\n","    source_string = \"\"\n","    target_string = \"\"\n","    output_string = \"\"\n","    for i in source:\n","        source_string+=(data['source_index_char'][i.item()])\n","    for i in target:\n","        target_string+=(data['target_index_char'][i.item()])\n","    for i in output:\n","        output_string+=(data['target_index_char'][i.item()])\n","    return source_string, target_string, output_string\n","\n","\n","def train_loop(encoder, decoder,h_params, data, data_loader, device, val_dataloader, use_teacher_forcing=True):\n","    # Function to train the encoder-decoder model\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n","    \n","    loss_fn = nn.NLLLoss()\n","    \n","    total_predictions = len(data_loader.dataset)\n","    total_batches = len(data_loader)\n","    \n","    for ep in range(h_params[\"epochs\"]):\n","        total_correct = 0\n","        total_loss = 0\n","        encoder.train()\n","        decoder.train()\n","        for batch_num, (source_batch, target_batch) in enumerate(data_loader):\n","            encoder_initial_state = encoder.getInitialState()\n","            \n","            if h_params[\"cell_type\"] == \"LSTM\":\n","                encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n","            encoder_states, encoder_final_state = encoder(source_batch,encoder_initial_state)\n","            \n","            decoder_current_state = encoder_final_state\n","            encoder_final_layer_states = encoder_states[:, -1, :, :]\n","            \n","            \n","            loss = 0\n","            correct = 0\n","            \n","            decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n","            total_correct +=correct\n","            total_loss += loss.item()/data[\"OUTPUT_MAX_LENGTH\"]\n","            \n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            \n","            \n","        train_acc = total_correct/total_predictions\n","        train_loss = total_loss/total_batches\n","        val_acc, val_loss = evaluate(encoder, decoder, data, val_dataloader,device, h_params, loss_fn, False)\n","        print(\"ep: \", ep, \" train acc:\", train_acc, \" train loss:\", train_loss, \" val acc:\", val_acc, \" val loss:\", val_loss.item()/data[\"OUTPUT_MAX_LENGTH\"])\n","        wandb.log({\"train_accuracy\":train_acc, \"train_loss\":train_loss, \"val_accuracy\":val_acc, \"val_loss\":val_loss, \"epoch\":ep})\n","    return loss_fn"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T09:23:55.726441Z","iopub.status.busy":"2024-05-17T09:23:55.726151Z","iopub.status.idle":"2024-05-17T09:23:55.741128Z","shell.execute_reply":"2024-05-17T09:23:55.740402Z","shell.execute_reply.started":"2024-05-17T09:23:55.726401Z"},"trusted":true},"outputs":[],"source":["h_params={\n","    \"char_embd_dim\" : 128,  # Dimension of character embeddings\n","    \"hidden_layer_neurons\": 512,  # Number of neurons in hidden layers\n","    \"batch_size\": 32,  # Batch size\n","    \"number_of_layers\": 2,  # Number of layers in the encoder and decoder\n","    \"learning_rate\": 0.0001,  # Learning rate for optimization\n","    \"epochs\": 20,  # Number of epochs for training\n","    \"cell_type\": \"LSTM\",  # Type of RNN cell: RNN, LSTM, GRU\n","    \"dropout\": 0.3,  # Dropout probability\n","    \"optimizer\": \"adam\"  # Optimization algorithm: adam, nadam\n","}\n","\n","def prepare_dataloaders(train_source, train_target, val_source, val_target,test_source, test_target, h_params):\n","    # Preparing data loaders for training and validation\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","    \n","    # Training data\n","    training_data = [data[\"source_data_seq\"], data['target_data_seq']]\n","    train_dataset = MyDataset(training_data)\n","    train_dataloader = DataLoader(train_dataset, batch_size=h_params[\"batch_size\"], shuffle=False)\n","\n","    # Validation data\n","    val_padded_source_strings = add_padding(val_source, data[\"INPUT_MAX_LENGTH\"])\n","    val_padded_target_strings = add_padding(val_target, data[\"OUTPUT_MAX_LENGTH\"])\n","    val_source_sequences = generate_string_to_sequence(val_padded_source_strings, data['source_char_index'])\n","    val_target_sequences = generate_string_to_sequence(val_padded_target_strings, data['target_char_index'])\n","    validation_data = [val_source_sequences, val_target_sequences]\n","    val_dataset = MyDataset(validation_data)\n","    val_dataloader = DataLoader(val_dataset, batch_size=h_params[\"batch_size\"], shuffle=False)\n","    \n","    # test data\n","    test_padded_source_strings = add_padding(test_source, data[\"INPUT_MAX_LENGTH\"])\n","    test_padded_target_strings = add_padding(test_target, data[\"OUTPUT_MAX_LENGTH\"])\n","    test_source_sequences = generate_string_to_sequence(test_padded_source_strings, data['source_char_index'])\n","    test_target_sequences = generate_string_to_sequence(test_padded_target_strings, data['target_char_index'])\n","    test_data = [test_source_sequences, test_target_sequences]\n","    test_dataset = MyDataset(test_data)\n","    test_dataloader = DataLoader(test_dataset, batch_size=h_params[\"batch_size\"], shuffle=False)\n","    \n","    return train_dataloader, val_dataloader, test_dataloader, data\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T09:23:55.744021Z","iopub.status.busy":"2024-05-17T09:23:55.743717Z","iopub.status.idle":"2024-05-17T09:23:55.755072Z","shell.execute_reply":"2024-05-17T09:23:55.754228Z","shell.execute_reply.started":"2024-05-17T09:23:55.743998Z"},"trusted":true},"outputs":[],"source":["def train(h_params, data, device, data_loader, val_dataloader, use_teacher_forcing=True):\n","    encoder = Encoder(h_params, data, device).to(device)\n","    decoder = Decoder(h_params, data, device).to(device)\n","    loss_fn = train_loop(encoder, decoder,h_params, data, data_loader,device, val_dataloader, use_teacher_forcing)\n","    return encoder, decoder, loss_fn"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T09:23:55.756306Z","iopub.status.busy":"2024-05-17T09:23:55.756051Z","iopub.status.idle":"2024-05-17T10:08:43.505090Z","shell.execute_reply":"2024-05-17T10:08:43.504125Z","shell.execute_reply.started":"2024-05-17T09:23:55.756284Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjaswanth431\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240517_092355-wcpu2ayh</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention%201/runs/wcpu2ayh' target=\"_blank\">LSTM_adam_ep_20_lr_0.0001_embd_128_hid_lyr_neur_512_bs_32_enc_layers_2_dec_layers_2_dropout_0.3</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention%201' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention%201' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention%201</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention%201/runs/wcpu2ayh' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention%201/runs/wcpu2ayh</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ep:  0  train acc: 0.05705078125  train loss: 1.3567165669150976  val acc: 0.0  val loss: 1.2356311963952107\n","ep:  1  train acc: 0.35916015625  train loss: 0.7328936720931022  val acc: 0.0068359375  val loss: 0.7459257374639097\n","ep:  2  train acc: 0.4886328125  train loss: 0.38329930001343426  val acc: 0.13232421875  val loss: 0.47324748661207117\n","ep:  3  train acc: 0.567421875  train loss: 0.25723866042559584  val acc: 0.307373046875  val loss: 0.3494106790293818\n","ep:  4  train acc: 0.64888671875  train loss: 0.19122661505066532  val acc: 0.353759765625  val loss: 0.3150609057882558\n","ep:  5  train acc: 0.68744140625  train loss: 0.16484403335261558  val acc: 0.38818359375  val loss: 0.2963656342547873\n","ep:  6  train acc: 0.70115234375  train loss: 0.14890617834454717  val acc: 0.403076171875  val loss: 0.2789236358974291\n","ep:  7  train acc: 0.715703125  train loss: 0.13896221505174322  val acc: 0.408447265625  val loss: 0.2738995137421981\n","ep:  8  train acc: 0.73595703125  train loss: 0.12299607858712207  val acc: 0.43017578125  val loss: 0.27457954572594684\n","ep:  9  train acc: 0.74412109375  train loss: 0.12037767865519168  val acc: 0.449462890625  val loss: 0.26554543039073114\n","ep:  10  train acc: 0.7511328125  train loss: 0.11277949936900544  val acc: 0.448974609375  val loss: 0.2549426659293797\n","ep:  11  train acc: 0.7616796875  train loss: 0.10254337787306766  val acc: 0.4638671875  val loss: 0.25737246223117993\n","ep:  12  train acc: 0.76779296875  train loss: 0.10004147787742906  val acc: 0.475830078125  val loss: 0.2575191000233526\n","ep:  13  train acc: 0.778359375  train loss: 0.09201607343242185  val acc: 0.466064453125  val loss: 0.2568577683490256\n","ep:  14  train acc: 0.77900390625  train loss: 0.08957105845020594  val acc: 0.454833984375  val loss: 0.25938734800919244\n","ep:  15  train acc: 0.77806640625  train loss: 0.08956836237738523  val acc: 0.473876953125  val loss: 0.2565872772880223\n","ep:  16  train acc: 0.7964453125  train loss: 0.07919978042988547  val acc: 0.458740234375  val loss: 0.2585204787876295\n","ep:  17  train acc: 0.797109375  train loss: 0.0763582599002224  val acc: 0.4775390625  val loss: 0.255900590316109\n","ep:  18  train acc: 0.80662109375  train loss: 0.07065794031421074  val acc: 0.4814453125  val loss: 0.25241233991539996\n","ep:  19  train acc: 0.80123046875  train loss: 0.07048481421329396  val acc: 0.49267578125  val loss: 0.2584547167238982\n"]}],"source":["config = h_params\n","run = wandb.init(project=\"DL Assignment 3 With Attention 1\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config)\n","train_dataloader, val_dataloader, test_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target,test_source, test_target, h_params)\n","encoder, decoder, loss_fn = train(h_params, data, device, train_dataloader, val_dataloader, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#It will print the test accura\n","acc, loss = evaluate(encoder,decoder, data, test_dataloader, device, config, loss_fn)\n","print(acc, loss/data[\"OUTPUT_MAX_LENGTH\"])\n","def remove_padding(str):\n","    padding_removed_string = \"\"\n","    for ch in str:\n","        if ch ==\"<\" or ch == \">\" or ch ==\"_\":\n","            continue\n","        padding_removed_string+=ch\n","    return padding_removed_string\n","\n","\n","#It will generate the attention heatmap\n","def plot_attention_heatmap(attention_matrix, input_sequence, output_sequence , id):\n","\n","    plt.figure(figsize=(15, 10))\n","\n","    ax = sns.heatmap(attention_matrix, cmap='viridis', annot=False, xticklabels=input_sequence, yticklabels=output_sequence)\n","\n","    # Set font properties for Telugu characters\n","    font_path = '/kaggle/input/fonts-bro-1/NotoSansTelugu-VariableFont_wdth,wght.ttf'  # Replace with the path to a Telugu font file\n","    telugu_font = FontProperties(fname=font_path)\n","\n","    ax.set_xticklabels(input_sequence, fontproperties=telugu_font)\n","    ax.set_yticklabels(output_sequence, fontproperties=telugu_font)\n","\n","    ax.set_xlabel('Input Sequence')\n","    ax.set_ylabel('Output Sequence')\n","    plt.title('Attention Heatmap')\n","    wandb.log({\"Attention_Heatmap\"+str(id)+ \"temp\": wandb.Image(plt)})\n","\n","    plt.close()\n","#     plt.show()\n","\n","def generate_predictions_report(encoder, decoder, data, dataloader, device, config, loss_fn):\n","    encoder.eval()\n","    decoder.eval()\n","    \n","    zeroth_batch_attention = \"\"\n","    zeroth_source_batch = \"\"\n","    zeroth_target_batch = \"\"\n","    \n","    # The CSV file will be saved in the current working directory\n","    with open('predictions_report.csv', mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['Source String', 'Target String', 'Predicted String'])\n","        \n","        with torch.no_grad():\n","            for batch_num, (source_batch, target_batch) in enumerate(dataloader):\n","\n","                encoder_initial_state = encoder.getInitialState()\n","                if config[\"cell_type\"] == \"LSTM\":\n","                    encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n","                encoder_states, encoder_final_state = encoder(source_batch, encoder_initial_state)\n","\n","                decoder_current_state = encoder_final_state\n","                encoder_final_layer_states = encoder_states[:, -1, :, :]\n","\n","                # Generate decoder output\n","                decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, False)\n","                if batch_num == 0:\n","                    zeroth_batch_attention = attentions\n","                    zeroth_source_batch = source_batch\n","                    zeroth_target_batch = target_batch\n","\n","                # Generate the list of true and predicted words\n","                for j in range(config[\"batch_size\"]):\n","                    src_str, target_str, pred_str = make_strings(data, source_batch[j], target_batch[j], decoder_output[j])\n","                    writer.writerow([remove_padding(src_str), remove_padding(target_str), remove_padding(pred_str)])\n","\n","            \n","            processed_atten = torch.zeros(config[\"batch_size\"], data[\"OUTPUT_MAX_LENGTH\"], data[\"INPUT_MAX_LENGTH\"])\n","            \n","            for i in range(data[\"OUTPUT_MAX_LENGTH\"]):\n","                temp = zeroth_batch_attention[i][:, 0, :]\n","                for j in range(config[\"batch_size\"]):\n","                    processed_atten[j][i] = temp[j]\n","               \n","            for i in range(10):\n","                curr_src_seq = zeroth_source_batch[i]\n","                curr_trg_seq = zeroth_target_batch[i]\n","                curr_src_seq = [data[\"source_index_char\"][k.item()] for k in curr_src_seq]\n","                curr_trg_seq = [data[\"target_index_char\"][k.item()] for k in curr_trg_seq]\n","                plot_attention_heatmap(processed_atten[i], curr_src_seq, curr_trg_seq, i)\n","\n","\n","generate_predictions_report(encoder, decoder, data, test_dataloader, device, config, loss_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:08:58.787533Z","iopub.status.busy":"2024-05-17T10:08:58.786610Z","iopub.status.idle":"2024-05-17T10:08:58.794250Z","shell.execute_reply":"2024-05-17T10:08:58.793093Z","shell.execute_reply.started":"2024-05-17T10:08:58.787494Z"},"trusted":true},"outputs":[],"source":["#Run this cell to run a sweep with appropriate parameters\n","sweep_params = {\n","    'method' : 'bayes',\n","    'name'   : 'DL Assignment 3 With Attention',\n","    'metric' : {\n","        'goal' : 'maximize',\n","        'name' : 'val_accuracy',\n","    },\n","    'parameters' : {\n","        'epochs':{'values' : [15, 20]},\n","        'learning_rate':{'values' : [0.001, 0.0001]},\n","        'batch_size':{'values':[32,64, 128]},\n","        'char_embd_dim':{'values' : [64, 128, 256] } ,\n","        'number_of_layers':{'values' : [1,2,3,4]},\n","        'optimizer':{'values':['nadam','adam']},\n","        'cell_type':{'values' : [\"RNN\",\"LSTM\", \"GRU\"]},\n","        'hidden_layer_neurons':{'values': [ 128, 256, 512]},\n","        'dropout':{'values': [0,0.2, 0.3]}\n","    }\n","}\n","\n","sweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 3 With Attention\")\n","def main():\n","    wandb.init(project=\"DL Assignment 3\" )\n","    config = wandb.config\n","    with wandb.init(project=\"DL Assignment 3\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config):\n","        train_dataloader, val_dataloader,test_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target,test_source, test_target, config)\n","        train(config, data, device, train_dataloader, val_dataloader, True)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T10:08:58.795819Z","iopub.status.busy":"2024-05-17T10:08:58.795450Z","iopub.status.idle":"2024-05-17T10:08:58.810108Z","shell.execute_reply":"2024-05-17T10:08:58.808731Z","shell.execute_reply.started":"2024-05-17T10:08:58.795782Z"},"trusted":true},"outputs":[],"source":["wandb.agent(\"f4esgkqv\", function=main, count=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4891846,"sourceId":8245488,"sourceType":"datasetVersion"},{"datasetId":4899019,"sourceId":8255378,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
