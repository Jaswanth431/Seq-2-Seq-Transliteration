{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"f4ce1158-c562-476f-8f7d-0325f0b787c4","_uuid":"549ce73a-e98b-42bd-a1d8-2cab5faae7d5","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T13:18:40.267203Z","iopub.status.busy":"2024-05-16T13:18:40.266856Z","iopub.status.idle":"2024-05-16T13:18:40.272747Z","shell.execute_reply":"2024-05-16T13:18:40.271770Z","shell.execute_reply.started":"2024-05-16T13:18:40.267179Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","import copy\n","from torch.utils.data import Dataset, DataLoader\n","import gc\n","import random\n","import wandb"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T13:18:40.275367Z","iopub.status.busy":"2024-05-16T13:18:40.274782Z","iopub.status.idle":"2024-05-16T13:18:40.433828Z","shell.execute_reply":"2024-05-16T13:18:40.432963Z","shell.execute_reply.started":"2024-05-16T13:18:40.275320Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login(key=\"62cfafb7157dfba7fdd6132ac9d757ccd913aaaf\")"]},{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"c7dcb64a-f20a-4fa2-8b31-9801c3a762b8","_uuid":"b6363452-eb4f-4705-aa3f-fdec65f8e9db","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T13:18:40.435019Z","iopub.status.busy":"2024-05-16T13:18:40.434764Z","iopub.status.idle":"2024-05-16T13:18:40.555568Z","shell.execute_reply":"2024-05-16T13:18:40.554532Z","shell.execute_reply.started":"2024-05-16T13:18:40.434997Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Checking if CUDA is available, else use CPU\n","print(device)  # Printing the device being used (CUDA or CPU)\n","END_TOKEN = '>'  # Defining the end token for sequences\n","START_TOKEN = '<'  # Defining the start token for sequences\n","PAD_TOKEN = '_'  # Defining the padding token for sequences\n","TEACHER_FORCING_RATIO = 0.5  # Ratio of teacher forcing during training\n","\n","# Paths to the train, test, and validation CSV files\n","train_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_train.csv\"\n","test_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_test.csv\"\n","val_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_valid.csv\"\n","\n","# Reading the train, test, and validation CSV files into pandas dataframes\n","train_df = pd.read_csv(train_csv, header=None)\n","test_df = pd.read_csv(test_csv, header=None)\n","val_df = pd.read_csv(val_csv, header=None)\n","\n","# Extracting source and target sequences from train, test, and validation dataframes\n","train_source, train_target = train_df[0].to_numpy(), train_df[1].to_numpy()\n","val_source, val_target = val_df[0].to_numpy(), val_df[1].to_numpy()"]},{"cell_type":"code","execution_count":19,"metadata":{"_cell_guid":"fb22b602-e1fc-496b-93dc-f4d3178870d5","_uuid":"3708abf3-c2df-4e8d-8cfe-af3faab7bc53","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T13:18:40.558767Z","iopub.status.busy":"2024-05-16T13:18:40.558231Z","iopub.status.idle":"2024-05-16T13:18:40.574734Z","shell.execute_reply":"2024-05-16T13:18:40.573644Z","shell.execute_reply.started":"2024-05-16T13:18:40.558738Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Function to add padding to source sequences\n","def add_padding(source_data, MAX_LENGTH):\n","    \"\"\"\n","    Add padding to source sequences and truncate if necessary.\n","    \n","    Args:\n","    - source_data: List of source sequences\n","    - MAX_LENGTH: Maximum length of source sequences\n","    \n","    Returns:\n","    - padded_source_strings: List of padded source sequences\n","    \"\"\"\n","    padded_source_strings = []\n","    for i in range(len(source_data)):\n","        source_str = START_TOKEN + source_data[i] + END_TOKEN  # Add start and end tokens\n","        source_str = source_str[:MAX_LENGTH]  # Truncate if longer than MAX_LENGTH\n","        source_str += PAD_TOKEN * (MAX_LENGTH - len(source_str))  # Pad with PAD_TOKEN\n","\n","        padded_source_strings.append(source_str)\n","        \n","    return padded_source_strings\n","\n","\n","# Function to convert source strings to sequences of indices\n","def generate_string_to_sequence(source_data, source_char_index_dict):\n","    \"\"\"\n","    Convert source strings to sequences of indices using char_index_dict.\n","    \n","    Args:\n","    - source_data: List of padded source strings\n","    - source_char_index_dict: Dictionary mapping characters to their indices\n","    \n","    Returns:\n","    - source_sequences: Padded sequence of character indices\n","    \"\"\"\n","    source_sequences = []\n","    for i in range(len(source_data)):\n","        source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","    source_sequences = pad_sequence(source_sequences, batch_first=True, padding_value=2)\n","    return source_sequences\n","\n","\n","# Function to convert characters to their corresponding indices\n","def get_chars(string, char_index_dict):\n","    \"\"\"\n","    Convert characters in a string to their corresponding indices using char_index_dict.\n","    \n","    Args:\n","    - string: Input string\n","    - char_index_dict: Dictionary mapping characters to their indices\n","    \n","    Returns:\n","    - chars_indexes: List of character indices\n","    \"\"\"\n","    chars_indexes = []\n","    for char in string:\n","        chars_indexes.append(char_index_dict[char])\n","    return torch.tensor(chars_indexes, device=device)\n","\n","\n","# Preprocess the data, including adding padding, generating sequences, and updating dictionaries\n","def preprocess_data(source_data, target_data):\n","    \"\"\"\n","    Preprocess source and target data.\n","    \n","    Args:\n","    - source_data: List of source strings\n","    - target_data: List of target strings\n","    \n","    Returns:\n","    - data: Preprocessed data dictionary\n","    \"\"\"\n","    data = {\n","        \"source_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n","        \"target_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n","        \"source_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n","        \"source_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n","        \"target_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n","        \"target_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n","        \"source_len\": 3,\n","        \"target_len\": 3,\n","        \"source_data\": source_data,\n","        \"target_data\": target_data,\n","        \"source_data_seq\": [],\n","        \"target_data_seq\": []\n","    }\n","    \n","    # Calculate the maximum length of input and output sequences\n","    data[\"INPUT_MAX_LENGTH\"] = max(len(string) for string in source_data) + 2\n","    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n","\n","    # Pad the source and target sequences and update character dictionaries\n","    padded_source_strings = add_padding(source_data, data[\"INPUT_MAX_LENGTH\"])\n","    padded_target_strings = add_padding(target_data, data[\"OUTPUT_MAX_LENGTH\"])\n","    \n","    for i in range(len(padded_source_strings)):\n","        for char in padded_source_strings[i]:\n","            if data[\"source_char_index\"].get(char) is None:\n","                data[\"source_chars\"].append(char)\n","                idx = len(data[\"source_chars\"]) - 1\n","                data[\"source_char_index\"][char] = idx\n","                data[\"source_index_char\"][idx] = char\n","        for char in padded_target_strings[i]:\n","            if data[\"target_char_index\"].get(char) is None:\n","                data[\"target_chars\"].append(char)\n","                idx = len(data[\"target_chars\"]) - 1\n","                data[\"target_char_index\"][char] = idx\n","                data[\"target_index_char\"][idx] = char\n","\n","    # Generate sequences of indexes for source and target data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings, data['source_char_index'])\n","    data['target_data_seq'] = generate_string_to_sequence(padded_target_strings, data['target_char_index'])\n","    \n","    # Update lengths of source and target character lists\n","    data[\"source_len\"] = len(data[\"source_chars\"])\n","    data[\"target_len\"] = len(data[\"target_chars\"])\n","    \n","    return data\n"]},{"cell_type":"code","execution_count":20,"metadata":{"_cell_guid":"52035e98-753c-4cd0-8dec-2cec35aab863","_uuid":"58e2b696-e336-4cb3-b6e6-d14eac238c96","execution":{"iopub.execute_input":"2024-05-16T13:18:40.576377Z","iopub.status.busy":"2024-05-16T13:18:40.576061Z","iopub.status.idle":"2024-05-16T13:18:40.609150Z","shell.execute_reply":"2024-05-16T13:18:40.608252Z","shell.execute_reply.started":"2024-05-16T13:18:40.576329Z"},"trusted":true},"outputs":[],"source":["def get_cell_type(cell_type):\n","    # Function to return the appropriate RNN cell based on the specified type\n","    if(cell_type == \"RNN\"):\n","        return nn.RNN\n","    elif(cell_type == \"LSTM\"):\n","        return nn.LSTM\n","    elif(cell_type == \"GRU\"):\n","        return nn.GRU\n","    else:\n","        print(\"Specify correct cell type\")\n","\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        # Initialize the attention mechanism module\n","        super(Attention, self).__init__()\n","        self.Wa = nn.Linear(hidden_size, hidden_size)\n","        self.Ua = nn.Linear(hidden_size, hidden_size)\n","        self.Va = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, query, keys):\n","        # Forward pass of the attention mechanism\n","        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n","        scores = scores.squeeze().unsqueeze(1)\n","        weights = F.softmax(scores, dim=0)\n","        weights = weights.permute(2,1,0)\n","        keys = keys.permute(1,0,2)\n","        context = torch.bmm(weights, keys)\n","        return context, weights\n","\n","class Encoder(nn.Module):\n","    def __init__(self, h_params, data, device ):\n","        # Initialize the Encoder module\n","        super(Encoder, self).__init__()\n","        # Embedding layer for input characters\n","        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embd_dim\"])\n","        # RNN cell for encoding\n","        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],num_layers=h_params[\"number_of_layers\"], batch_first=True)\n","        self.device=device\n","        self.h_params = h_params\n","        self.data = data\n","        \n","    def forward(self, input , encoder_curr_state):\n","        # Forward pass of the Encoder module\n","        input_length = self.data[\"INPUT_MAX_LENGTH\"]\n","        batch_size = self.h_params[\"batch_size\"]\n","        hidden_neurons = self.h_params[\"hidden_layer_neurons\"]\n","        layers = self.h_params[\"number_of_layers\"]\n","        encoder_states  = torch.zeros(input_length, layers, batch_size, hidden_neurons, device=self.device )\n","        for i in range(input_length):\n","            current_input = input[:, i].view(batch_size,1)\n","            _, encoder_curr_state = self.forward_step(current_input, encoder_curr_state)\n","            if self.h_params[\"cell_type\"] == \"LSTM\":\n","                encoder_states[i] = encoder_curr_state[1]\n","            else:\n","                encoder_states[i] = encoder_curr_state\n","        return encoder_states, encoder_curr_state\n","    \n","    def forward_step(self, current_input, prev_state):\n","        # Perform forward pass for one time step\n","        embd_input = self.embedding(current_input)\n","        output, prev_state = self.cell(embd_input, prev_state)\n","        return output, prev_state\n","        \n","    def getInitialState(self):\n","        # Initialize initial hidden state for encoder\n","        return torch.zeros(self.h_params[\"number_of_layers\"],self.h_params[\"batch_size\"],self.h_params[\"hidden_layer_neurons\"], device=self.device)\n","\n","class Decoder(nn.Module):\n","    def __init__(self, h_params, data,device):\n","        # Initialize the Decoder module\n","        super(Decoder, self).__init__()\n","        # Attention mechanism\n","        self.attention = Attention(h_params[\"hidden_layer_neurons\"]).to(device)\n","        # Embedding layer for target characters\n","        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embd_dim\"])\n","        # RNN cell for decoding\n","        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"hidden_layer_neurons\"] +h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],num_layers=h_params[\"number_of_layers\"], batch_first=True)\n","        # Fully connected layer for output\n","        self.fc = nn.Linear(h_params[\"hidden_layer_neurons\"], data[\"target_len\"])\n","        # Softmax activation for output probabilities\n","        self.softmax = nn.LogSoftmax(dim=2)\n","        self.h_params = h_params\n","        self.data = data\n","        self.device = device\n","\n","    def forward(self, decoder_current_state, encoder_final_layers, target_batch, loss_fn, teacher_forcing_enabled=True):\n","        # Forward pass of the Decoder module\n","        batch_size = self.h_params[\"batch_size\"]\n","        decoder_current_input = torch.full((batch_size,1),self.data[\"target_char_index\"][START_TOKEN], device=self.device)\n","        embd_input = self.embedding(decoder_current_input)\n","        curr_embd = F.relu(embd_input)\n","        decoder_actual_output = []\n","        attentions = []\n","        loss = 0\n","        \n","        use_teacher_forcing = False\n","        if(teacher_forcing_enabled):\n","            use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n","        for i in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n","            # Perform one step of decoding\n","            decoder_output, decoder_current_state, attn_weights = self.forward_step(decoder_current_input, decoder_current_state, encoder_final_layers)\n","            attentions.append(attn_weights)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_current_input = topi.squeeze().detach()\n","            decoder_actual_output.append(decoder_current_input)\n","\n","            if(target_batch==None):\n","                decoder_current_input = decoder_current_input.view(self.h_params[\"batch_size\"], 1)\n","            else:\n","                curr_target_chars = target_batch[:, i]\n","                if(i<self.data[\"OUTPUT_MAX_LENGTH\"]-1):\n","                    if use_teacher_forcing:\n","                        decoder_current_input = target_batch[:, i+1].view(self.h_params[\"batch_size\"], 1)\n","                    else:\n","                        decoder_current_input = decoder_current_input.view(self.h_params[\"batch_size\"], 1)\n","                decoder_output = decoder_output[:, -1, :]\n","                loss+=(loss_fn(decoder_output, curr_target_chars))\n","\n","        decoder_actual_output = torch.cat(decoder_actual_output,dim=0).view(self.data[\"OUTPUT_MAX_LENGTH\"], self.h_params[\"batch_size\"]).transpose(0,1)\n","\n","        correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n","        return decoder_actual_output, attentions, loss, correct\n","    \n","    def forward_step(self, current_input, prev_state, encoder_final_layers):\n","        # Perform one step of decoding\n","        embd_input = self.embedding(current_input)\n","        if self.h_params[\"cell_type\"] == \"LSTM\":\n","            context , attn_weights = self.attention(prev_state[1][-1,:,:], encoder_final_layers)\n","        else:\n","            context , attn_weights = self.attention(prev_state[-1,:,:], encoder_final_layers)\n","        curr_embd = F.relu(embd_input)\n","        input_gru = torch.cat((curr_embd, context), dim=2)\n","        output, prev_state = self.cell(input_gru, prev_state)\n","        output = self.softmax(self.fc(output))\n","        return output, prev_state, attn_weights\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T13:18:40.610479Z","iopub.status.busy":"2024-05-16T13:18:40.610182Z","iopub.status.idle":"2024-05-16T13:18:40.621764Z","shell.execute_reply":"2024-05-16T13:18:40.620829Z","shell.execute_reply.started":"2024-05-16T13:18:40.610455Z"},"trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, data):\n","        self.source_data_seq = data[0]\n","        self.target_data_seq = data[1]\n","    \n","    def __len__(self):\n","        return len(self.source_data_seq)\n","    \n","    def __getitem__(self, idx):\n","        source_data = self.source_data_seq[idx]\n","        target_data = self.target_data_seq[idx]\n","        return source_data, target_data\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T13:18:40.623340Z","iopub.status.busy":"2024-05-16T13:18:40.623079Z","iopub.status.idle":"2024-05-16T13:18:40.635600Z","shell.execute_reply":"2024-05-16T13:18:40.634727Z","shell.execute_reply.started":"2024-05-16T13:18:40.623317Z"},"trusted":true},"outputs":[],"source":["def evaluate(encoder, decoder, data, dataloader, device, h_params, loss_fn, use_teacher_forcing = False):\n","    # Function to evaluate the performance of the model on a dataset\n","    correct_predictions = 0\n","    total_loss = 0\n","    total_predictions = len(dataloader.dataset)\n","    number_of_batches = len(dataloader)\n","    encoder.eval()\n","    decoder.eval()\n","    \n","    with torch.no_grad():\n","        for batch_num, (source_batch, target_batch) in enumerate(dataloader):\n","\n","            encoder_initial_state = encoder.getInitialState()\n","            if h_params[\"cell_type\"] == \"LSTM\":\n","                encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n","            encoder_states, encoder_final_state = encoder(source_batch,encoder_initial_state)\n","\n","            decoder_current_state = encoder_final_state\n","            encoder_final_layer_states = encoder_states[:, -1, :, :]\n","\n","            loss = 0\n","            correct = 0\n","\n","            decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n","\n","            correct_predictions+=correct\n","            total_loss +=loss\n","\n","        accuracy = correct_predictions / total_predictions\n","        total_loss /= number_of_batches\n","\n","        return accuracy, total_loss\n"]},{"cell_type":"code","execution_count":23,"metadata":{"_cell_guid":"0dc20b89-db7b-4230-a7ec-e76d1896adee","_uuid":"c743c5b1-1106-4388-a594-e4e301225bad","execution":{"iopub.execute_input":"2024-05-16T13:18:40.637402Z","iopub.status.busy":"2024-05-16T13:18:40.637039Z","iopub.status.idle":"2024-05-16T13:18:40.652566Z","shell.execute_reply":"2024-05-16T13:18:40.651570Z","shell.execute_reply.started":"2024-05-16T13:18:40.637372Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["def make_strings(data, source, target, output):\n","    # Function to convert indices to strings for source, target, and output sequences\n","    source_string = \"\"\n","    target_string = \"\"\n","    output_string = \"\"\n","    for i in source:\n","        source_string+=(data['source_index_char'][i.item()])\n","    for i in target:\n","        target_string+=(data['target_index_char'][i.item()])\n","    for i in output:\n","        output_string+=(data['target_index_char'][i.item()])\n","    return source_string, target_string, output_string\n","\n","\n","def train_loop(encoder, decoder,h_params, data, data_loader, device, val_dataloader, use_teacher_forcing=True):\n","    # Function to train the encoder-decoder model\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n","    \n","    loss_fn = nn.NLLLoss()\n","    \n","    total_predictions = len(data_loader.dataset)\n","    total_batches = len(data_loader)\n","    \n","    for ep in range(h_params[\"epochs\"]):\n","        total_correct = 0\n","        total_loss = 0\n","        encoder.train()\n","        decoder.train()\n","        for batch_num, (source_batch, target_batch) in enumerate(data_loader):\n","            encoder_initial_state = encoder.getInitialState()\n","            \n","            if h_params[\"cell_type\"] == \"LSTM\":\n","                encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n","            encoder_states, encoder_final_state = encoder(source_batch,encoder_initial_state)\n","            \n","            decoder_current_state = encoder_final_state\n","            encoder_final_layer_states = encoder_states[:, -1, :, :]\n","            \n","            \n","            loss = 0\n","            correct = 0\n","            \n","            decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n","            total_correct +=correct\n","            total_loss += loss.item()/data[\"OUTPUT_MAX_LENGTH\"]\n","            \n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            \n","            \n","        train_acc = total_correct/total_predictions\n","        train_loss = total_loss/total_batches\n","        val_acc, val_loss = evaluate(encoder, decoder, data, val_dataloader,device, h_params, loss_fn, False)\n","        print(\"ep: \", ep, \" train acc:\", train_acc, \" train loss:\", train_loss, \" val acc:\", val_acc, \" val loss:\", val_loss.item()/data[\"OUTPUT_MAX_LENGTH\"])\n","        wandb.log({\"train_accuracy\":train_acc, \"train_loss\":train_loss, \"val_accuracy\":val_acc, \"val_loss\":val_loss, \"epoch\":ep})\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T13:18:40.654516Z","iopub.status.busy":"2024-05-16T13:18:40.654219Z","iopub.status.idle":"2024-05-16T13:18:40.667303Z","shell.execute_reply":"2024-05-16T13:18:40.666283Z","shell.execute_reply.started":"2024-05-16T13:18:40.654493Z"},"trusted":true},"outputs":[],"source":["h_params={\n","    \"char_embd_dim\" : 64,  # Dimension of character embeddings\n","    \"hidden_layer_neurons\": 512,  # Number of neurons in hidden layers\n","    \"batch_size\": 32,  # Batch size\n","    \"number_of_layers\": 5,  # Number of layers in the encoder and decoder\n","    \"learning_rate\": 0.0001,  # Learning rate for optimization\n","    \"epochs\": 25,  # Number of epochs for training\n","    \"cell_type\": \"RNN\",  # Type of RNN cell: RNN, LSTM, GRU\n","    \"dropout\": 0.3,  # Dropout probability\n","    \"optimizer\": \"adam\"  # Optimization algorithm: adam, nadam\n","}\n","\n","def prepare_dataloaders(train_source, train_target, val_source, val_target, h_params):\n","    # Preparing data loaders for training and validation\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","    \n","    # Training data\n","    training_data = [data[\"source_data_seq\"], data['target_data_seq']]\n","    train_dataset = MyDataset(training_data)\n","    train_dataloader = DataLoader(train_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n","\n","    # Validation data\n","    val_padded_source_strings = add_padding(val_source, data[\"INPUT_MAX_LENGTH\"])\n","    val_padded_target_strings = add_padding(val_target, data[\"OUTPUT_MAX_LENGTH\"])\n","    val_source_sequences = generate_string_to_sequence(val_padded_source_strings, data['source_char_index'])\n","    val_target_sequences = generate_string_to_sequence(val_padded_target_strings, data['target_char_index'])\n","    validation_data = [val_source_sequences, val_target_sequences]\n","    val_dataset = MyDataset(validation_data)\n","    val_dataloader = DataLoader(val_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n","    \n","    return train_dataloader, val_dataloader, data\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T13:18:40.670930Z","iopub.status.busy":"2024-05-16T13:18:40.670564Z","iopub.status.idle":"2024-05-16T13:18:40.679398Z","shell.execute_reply":"2024-05-16T13:18:40.678547Z","shell.execute_reply.started":"2024-05-16T13:18:40.670907Z"},"trusted":true},"outputs":[],"source":["def train(h_params, data, device, data_loader, val_dataloader, use_teacher_forcing=True):\n","    encoder = Encoder(h_params, data, device).to(device)\n","    decoder = Decoder(h_params, data, device).to(device)\n","    train_loop(encoder, decoder,h_params, data, data_loader,device, val_dataloader, use_teacher_forcing)\n","    "]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T13:18:40.681330Z","iopub.status.busy":"2024-05-16T13:18:40.680630Z","iopub.status.idle":"2024-05-16T13:18:40.689683Z","shell.execute_reply":"2024-05-16T13:18:40.688718Z","shell.execute_reply.started":"2024-05-16T13:18:40.681300Z"},"trusted":true},"outputs":[],"source":["config = h_params\n","run = wandb.init(project=\"DL Assignment 3 With Attention\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config)\n","train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, h_params)\n","train(h_params, data, device, train_dataloader, val_dataloader, True)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T13:18:40.691695Z","iopub.status.busy":"2024-05-16T13:18:40.691035Z","iopub.status.idle":"2024-05-16T13:18:41.061231Z","shell.execute_reply":"2024-05-16T13:18:41.060318Z","shell.execute_reply.started":"2024-05-16T13:18:40.691663Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: qfu8kgax\n","Sweep URL: https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/qfu8kgax\n"]}],"source":["#Run this cell to run a sweep with appropriate parameters\n","sweep_params = {\n","    'method' : 'bayes',\n","    'name'   : 'DL Assignment 3 With Attention',\n","    'metric' : {\n","        'goal' : 'maximize',\n","        'name' : 'val_accuracy',\n","    },\n","    'parameters' : {\n","        'epochs':{'values' : [15, 20]},\n","        'learning_rate':{'values' : [0.001, 0.0001]},\n","        'batch_size':{'values':[32,64, 128]},\n","        'char_embd_dim':{'values' : [64, 128, 256] } ,\n","        'number_of_layers':{'values' : [1,2,3,4]},\n","        'optimizer':{'values':['nadam','adam']},\n","        'cell_type':{'values' : [\"RNN\",\"LSTM\", \"GRU\"]},\n","        'hidden_layer_neurons':{'values': [ 128, 256, 512]},\n","        'dropout':{'values': [0,0.2, 0.3]}\n","    }\n","}\n","\n","sweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 3 With Attention\")\n","def main():\n","    wandb.init(project=\"DL Assignment 3\" )\n","    config = wandb.config\n","    with wandb.init(project=\"DL Assignment 3\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config):\n","        train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","        train(config, data, device, train_dataloader, val_dataloader, True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T13:18:41.062624Z","iopub.status.busy":"2024-05-16T13:18:41.062314Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lsh7g72b with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n"]},{"name":"stdout","output_type":"stream","text":["VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))\n","<IPython.core.display.HTML object>\n","<IPython.core.display.HTML object>\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_131846-lsh7g72b</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b' target=\"_blank\">young-sweep-226</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["Finishing last run (ID:lsh7g72b) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">young-sweep-226</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240516_131846-lsh7g72b/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:lsh7g72b). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_131904-lsh7g72b</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b' target=\"_blank\">LSTM_adam_ep_20_lr_0.0001_embd_128_hid_lyr_neur_512_bs_32_enc_layers_2_dec_layers_2_dropout_0.3</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ep:  0  train acc: 0.060703125  train loss: 1.3598916569611277  val acc: 0.0  val loss: 1.2673139986784563\n","ep:  1  train acc: 0.385078125  train loss: 0.7451431657859815  val acc: 0.000244140625  val loss: 0.9511200448741084\n","ep:  2  train acc: 0.480078125  train loss: 0.4399974388406484  val acc: 0.056884765625  val loss: 0.52749604764192\n","ep:  3  train acc: 0.5425390625  train loss: 0.28035949707152724  val acc: 0.280029296875  val loss: 0.3564677031143852\n","ep:  4  train acc: 0.64916015625  train loss: 0.19978529172676168  val acc: 0.335205078125  val loss: 0.33012458552484925\n","ep:  5  train acc: 0.6590625  train loss: 0.1810680220670411  val acc: 0.35888671875  val loss: 0.30621188619862433\n","ep:  6  train acc: 0.6941015625  train loss: 0.15730468502440298  val acc: 0.379150390625  val loss: 0.30107657805733057\n","ep:  7  train acc: 0.69990234375  train loss: 0.14846995970130494  val acc: 0.422607421875  val loss: 0.2777045706044073\n","ep:  8  train acc: 0.7290234375  train loss: 0.12942769158836562  val acc: 0.44287109375  val loss: 0.27581171367479407\n","ep:  9  train acc: 0.73681640625  train loss: 0.12374315306744252  val acc: 0.4443359375  val loss: 0.27717136300128437\n","ep:  10  train acc: 0.73466796875  train loss: 0.12260970101294419  val acc: 0.453369140625  val loss: 0.2616979350214419\n","ep:  11  train acc: 0.7506640625  train loss: 0.11279589767865175  val acc: 0.460205078125  val loss: 0.2514152319534965\n","ep:  12  train acc: 0.7651953125  train loss: 0.10348084864180036  val acc: 0.434814453125  val loss: 0.2586122181104577\n","ep:  13  train acc: 0.77478515625  train loss: 0.09749001267195284  val acc: 0.46435546875  val loss: 0.2556588753410008\n","ep:  14  train acc: 0.7751953125  train loss: 0.09451656824480828  val acc: 0.470947265625  val loss: 0.24793100357055664\n","ep:  15  train acc: 0.77384765625  train loss: 0.09172641323642733  val acc: 0.478271484375  val loss: 0.2614920657614003\n","ep:  16  train acc: 0.7737890625  train loss: 0.09186966924849685  val acc: 0.46435546875  val loss: 0.2454061922819718\n","ep:  17  train acc: 0.79115234375  train loss: 0.08227550052648586  val acc: 0.49951171875  val loss: 0.2565223859704059\n","ep:  18  train acc: 0.8033203125  train loss: 0.07353969676086035  val acc: 0.48486328125  val loss: 0.25026085065758746\n","ep:  19  train acc: 0.80529296875  train loss: 0.07430736915319755  val acc: 0.5107421875  val loss: 0.25791698953379755\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.025 MB uploaded\\r'), FloatProgress(value=0.05228758169934641, max=1.…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▇▇▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▂▅▆▆▆▇▇▇▇▇▇▇▇█▇███</td></tr><tr><td>val_loss</td><td>█▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_accuracy</td><td>0.80529</td></tr><tr><td>train_loss</td><td>0.07431</td></tr><tr><td>val_accuracy</td><td>0.51074</td></tr><tr><td>val_loss</td><td>5.93209</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">LSTM_adam_ep_20_lr_0.0001_embd_128_hid_lyr_neur_512_bs_32_enc_layers_2_dec_layers_2_dropout_0.3</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lsh7g72b</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240516_131904-lsh7g72b/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b25zweri with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_140312-b25zweri</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri' target=\"_blank\">dutiful-sweep-227</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["Finishing last run (ID:b25zweri) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">dutiful-sweep-227</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240516_140312-b25zweri/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:b25zweri). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_140329-b25zweri</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri' target=\"_blank\">LSTM_adam_ep_20_lr_0.0001_embd_256_hid_lyr_neur_512_bs_64_enc_layers_3_dec_layers_3_dropout_0.2</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ep:  0  train acc: 0.0  train loss: 1.6767475973004886  val acc: 0.0  val loss: 1.2725757930589758\n","ep:  1  train acc: 0.001171875  train loss: 1.3597788157152113  val acc: 0.0  val loss: 1.2238353231678838\n","ep:  2  train acc: 0.1939453125  train loss: 0.9023545388683039  val acc: 0.0  val loss: 1.1987614838973335\n","ep:  3  train acc: 0.45251953125  train loss: 0.6803721010037093  val acc: 0.000244140625  val loss: 0.9439834097157354\n","ep:  4  train acc: 0.47955078125  train loss: 0.5796446153310983  val acc: 0.004150390625  val loss: 0.804785272349482\n","ep:  5  train acc: 0.47662109375  train loss: 0.47940716573239667  val acc: 0.067626953125  val loss: 0.5650688668955928\n","ep:  6  train acc: 0.5201171875  train loss: 0.33609296744203443  val acc: 0.139892578125  val loss: 0.515300875124724\n","ep:  7  train acc: 0.5991796875  train loss: 0.24387488683685687  val acc: 0.280029296875  val loss: 0.37906895513119904\n","ep:  8  train acc: 0.638828125  train loss: 0.21992339828456514  val acc: 0.2578125  val loss: 0.3805298183275306\n","ep:  9  train acc: 0.65705078125  train loss: 0.1925170741147242  val acc: 0.364501953125  val loss: 0.31502489421678626\n","ep:  10  train acc: 0.68083984375  train loss: 0.16734368777722533  val acc: 0.297119140625  val loss: 0.3290541068367336\n","ep:  11  train acc: 0.68470703125  train loss: 0.1596642522141339  val acc: 0.408203125  val loss: 0.29232255272243335\n","ep:  12  train acc: 0.71955078125  train loss: 0.13784654269490954  val acc: 0.416259765625  val loss: 0.2940492008043372\n","ep:  13  train acc: 0.71453125  train loss: 0.13798642263560482  val acc: 0.423583984375  val loss: 0.2925598517708156\n","ep:  14  train acc: 0.715859375  train loss: 0.1390956321034743  val acc: 0.443115234375  val loss: 0.2666041332742442\n","ep:  15  train acc: 0.7367578125  train loss: 0.12173453721865679  val acc: 0.45654296875  val loss: 0.28444766998291016\n","ep:  16  train acc: 0.75484375  train loss: 0.11271780574957478  val acc: 0.45458984375  val loss: 0.26982471217279846\n","ep:  17  train acc: 0.75033203125  train loss: 0.11127224058413923  val acc: 0.46728515625  val loss: 0.26561981698741083\n","ep:  18  train acc: 0.76912109375  train loss: 0.10283091750861463  val acc: 0.466796875  val loss: 0.2675611247187075\n","ep:  19  train acc: 0.77203125  train loss: 0.09891510670946448  val acc: 0.4609375  val loss: 0.2570555935735288\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▃▅▅▅▆▆▇▇▇▇█▇▇█████</td></tr><tr><td>train_loss</td><td>█▇▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▂▃▅▅▆▅▇▇▇██████</td></tr><tr><td>val_loss</td><td>██▇▆▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_accuracy</td><td>0.77203</td></tr><tr><td>train_loss</td><td>0.09892</td></tr><tr><td>val_accuracy</td><td>0.46094</td></tr><tr><td>val_loss</td><td>5.91228</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">LSTM_adam_ep_20_lr_0.0001_embd_256_hid_lyr_neur_512_bs_64_enc_layers_3_dec_layers_3_dropout_0.2</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/b25zweri</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240516_140329-b25zweri/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xrjxt3ya with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_143210-xrjxt3ya</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya' target=\"_blank\">light-sweep-228</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["Finishing last run (ID:xrjxt3ya) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">light-sweep-228</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240516_143210-xrjxt3ya/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:xrjxt3ya). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_143227-xrjxt3ya</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya' target=\"_blank\">GRU_adam_ep_20_lr_0.001_embd_64_hid_lyr_neur_128_bs_128_enc_layers_3_dec_layers_3_dropout_0.3</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ep:  0  train acc: 0.00029296875  train loss: 1.6407427055939399  val acc: 0.0  val loss: 1.3834874526314114\n","ep:  1  train acc: 0.28275390625  train loss: 0.9383157526669297  val acc: 0.0  val loss: 1.0804095060929009\n","ep:  2  train acc: 0.47716796875  train loss: 0.7108431286364792  val acc: 0.00048828125  val loss: 1.0713208240011465\n","ep:  3  train acc: 0.4951171875  train loss: 0.6165715354842984  val acc: 0.002685546875  val loss: 0.9365818189538043\n","ep:  4  train acc: 0.495390625  train loss: 0.5328888896982303  val acc: 0.0263671875  val loss: 0.7081696054209834\n","ep:  5  train acc: 0.488359375  train loss: 0.44866319839154284  val acc: 0.013427734375  val loss: 0.8386660866115404\n","ep:  6  train acc: 0.5640234375  train loss: 0.34817811860500475  val acc: 0.18310546875  val loss: 0.4393294790516729\n","ep:  7  train acc: 0.5698046875  train loss: 0.27100872862760145  val acc: 0.16357421875  val loss: 0.47560513537863025\n","ep:  8  train acc: 0.58654296875  train loss: 0.2657612212439593  val acc: 0.313232421875  val loss: 0.34848976135253906\n","ep:  9  train acc: 0.65138671875  train loss: 0.1914680612808013  val acc: 0.357666015625  val loss: 0.32391699500705884\n","ep:  10  train acc: 0.6544140625  train loss: 0.1927767163247841  val acc: 0.357666015625  val loss: 0.3157993399578592\n","ep:  11  train acc: 0.67640625  train loss: 0.18567846687044956  val acc: 0.39306640625  val loss: 0.2991704318834388\n","ep:  12  train acc: 0.69064453125  train loss: 0.15631925390648616  val acc: 0.33642578125  val loss: 0.33996445199717645\n","ep:  13  train acc: 0.70921875  train loss: 0.15455305329908653  val acc: 0.38671875  val loss: 0.3051409514054008\n","ep:  14  train acc: 0.71796875  train loss: 0.13575492959014  val acc: 0.40576171875  val loss: 0.28466871510381286\n","ep:  15  train acc: 0.7262890625  train loss: 0.13178390530221482  val acc: 0.43603515625  val loss: 0.28369594656902813\n","ep:  16  train acc: 0.71603515625  train loss: 0.1282307387116042  val acc: 0.418701171875  val loss: 0.28996287221493927\n","ep:  17  train acc: 0.72146484375  train loss: 0.1414389813238876  val acc: 0.437255859375  val loss: 0.28454913263735565\n","ep:  18  train acc: 0.74326171875  train loss: 0.1169779791344848  val acc: 0.4306640625  val loss: 0.2898751756419306\n","ep:  19  train acc: 0.7438671875  train loss: 0.11571639393162714  val acc: 0.438720703125  val loss: 0.28402121170707373\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.023 MB uploaded\\r'), FloatProgress(value=0.05730374296124544, max=1.…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▄▄▆▇▇▇▆▇▇█████</td></tr><tr><td>val_loss</td><td>█▆▆▅▄▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_accuracy</td><td>0.74387</td></tr><tr><td>train_loss</td><td>0.11572</td></tr><tr><td>val_accuracy</td><td>0.43872</td></tr><tr><td>val_loss</td><td>6.53249</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">GRU_adam_ep_20_lr_0.001_embd_64_hid_lyr_neur_128_bs_128_enc_layers_3_dec_layers_3_dropout_0.3</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xrjxt3ya</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240516_143227-xrjxt3ya/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hygxi89w with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_144517-hygxi89w</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w' target=\"_blank\">dauntless-sweep-230</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["Finishing last run (ID:hygxi89w) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">dauntless-sweep-230</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240516_144517-hygxi89w/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:hygxi89w). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_144534-hygxi89w</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w' target=\"_blank\">LSTM_adam_ep_20_lr_0.0001_embd_256_hid_lyr_neur_512_bs_64_enc_layers_2_dec_layers_2_dropout_0.2</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ep:  0  train acc: 0.0283203125  train loss: 1.4718731344782803  val acc: 0.0  val loss: 1.3344588901685632\n","ep:  1  train acc: 0.28478515625  train loss: 0.885497260430585  val acc: 0.0  val loss: 1.3487915370775305\n","ep:  2  train acc: 0.4500390625  train loss: 0.6453109680537307  val acc: 0.004150390625  val loss: 0.7819676606551461\n","ep:  3  train acc: 0.49642578125  train loss: 0.3929376092868978  val acc: 0.162841796875  val loss: 0.4447840400364088\n","ep:  4  train acc: 0.5783203125  train loss: 0.253743595585794  val acc: 0.271728515625  val loss: 0.3657737814861795\n","ep:  5  train acc: 0.6508203125  train loss: 0.20211279487237302  val acc: 0.346923828125  val loss: 0.3197191279867421\n","ep:  6  train acc: 0.67  train loss: 0.17789964214169784  val acc: 0.38037109375  val loss: 0.30554383733998175\n","ep:  7  train acc: 0.6691796875  train loss: 0.1736332487527524  val acc: 0.390869140625  val loss: 0.28624868392944336\n","ep:  8  train acc: 0.7132421875  train loss: 0.14435515786278189  val acc: 0.3671875  val loss: 0.29500345561815344\n","ep:  9  train acc: 0.7127734375  train loss: 0.1385316370470632  val acc: 0.40380859375  val loss: 0.2691631317138672\n","ep:  10  train acc: 0.7327734375  train loss: 0.12735692389370123  val acc: 0.4267578125  val loss: 0.26285370536472485\n","ep:  11  train acc: 0.7450390625  train loss: 0.12050705282619913  val acc: 0.426025390625  val loss: 0.26342479042384936\n","ep:  12  train acc: 0.7340234375  train loss: 0.12249433101296585  val acc: 0.447021484375  val loss: 0.2582272653994353\n","ep:  13  train acc: 0.76021484375  train loss: 0.10434623653026116  val acc: 0.439208984375  val loss: 0.2513520406640094\n","ep:  14  train acc: 0.75044921875  train loss: 0.10807699424844816  val acc: 0.46875  val loss: 0.2536571544149648\n","ep:  15  train acc: 0.77671875  train loss: 0.09789868700414205  val acc: 0.451904296875  val loss: 0.23940650276515796\n","ep:  16  train acc: 0.772734375  train loss: 0.09578355119753444  val acc: 0.468017578125  val loss: 0.2405305945354959\n","ep:  17  train acc: 0.783828125  train loss: 0.09014064948446542  val acc: 0.476318359375  val loss: 0.2585781346196714\n","ep:  18  train acc: 0.80193359375  train loss: 0.07970736011659019  val acc: 0.4638671875  val loss: 0.2484291118124257\n","ep:  19  train acc: 0.79490234375  train loss: 0.08358269161357476  val acc: 0.4892578125  val loss: 0.23875611761341925\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▃▅▅▆▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▃▅▆▆▇▆▇▇▇▇▇█▇████</td></tr><tr><td>val_loss</td><td>██▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_accuracy</td><td>0.7949</td></tr><tr><td>train_loss</td><td>0.08358</td></tr><tr><td>val_accuracy</td><td>0.48926</td></tr><tr><td>val_loss</td><td>5.49139</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">LSTM_adam_ep_20_lr_0.0001_embd_256_hid_lyr_neur_512_bs_64_enc_layers_2_dec_layers_2_dropout_0.2</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hygxi89w</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240516_144534-hygxi89w/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4te04t19 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_150734-4te04t19</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4te04t19' target=\"_blank\">vibrant-sweep-231</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4te04t19' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4te04t19</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"data":{"text/html":["Finishing last run (ID:4te04t19) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">vibrant-sweep-231</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4te04t19' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4te04t19</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240516_150734-4te04t19/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:4te04t19). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240516_150751-4te04t19</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4te04t19' target=\"_blank\">LSTM_adam_ep_20_lr_0.0001_embd_128_hid_lyr_neur_512_bs_32_enc_layers_2_dec_layers_2_dropout_0.3</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4te04t19' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4te04t19</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ep:  0  train acc: 0.0887890625  train loss: 1.2891621074339605  val acc: 0.0  val loss: 1.202598820561948\n","ep:  1  train acc: 0.42591796875  train loss: 0.6444516665286016  val acc: 0.01025390625  val loss: 0.6577523687611455\n","ep:  2  train acc: 0.50552734375  train loss: 0.3508361675926364  val acc: 0.200439453125  val loss: 0.40340983349344006\n","ep:  3  train acc: 0.62029296875  train loss: 0.21735633962342235  val acc: 0.224853515625  val loss: 0.39969257686449133\n","ep:  4  train acc: 0.64998046875  train loss: 0.1894767151834723  val acc: 0.36865234375  val loss: 0.31124197918435803\n","ep:  5  train acc: 0.68708984375  train loss: 0.16205762501385912  val acc: 0.34423828125  val loss: 0.3246242689049762\n","ep:  6  train acc: 0.705546875  train loss: 0.1461200351055215  val acc: 0.41650390625  val loss: 0.280131547347359\n","ep:  7  train acc: 0.731328125  train loss: 0.12995931103018246  val acc: 0.421875  val loss: 0.27200524703316065\n","ep:  8  train acc: 0.73044921875  train loss: 0.12566167701779785  val acc: 0.44873046875  val loss: 0.257046305614969\n","ep:  9  train acc: 0.75724609375  train loss: 0.11187240540267611  val acc: 0.416259765625  val loss: 0.2639322280883789\n","ep:  10  train acc: 0.74927734375  train loss: 0.11174110344826232  val acc: 0.451904296875  val loss: 0.2607018636620563\n","ep:  11  train acc: 0.7711328125  train loss: 0.0988236582451297  val acc: 0.46630859375  val loss: 0.25157789562059485\n"]}],"source":["wandb.agent(\"f4esgkqv\", function=main, count=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4891846,"sourceId":8245488,"sourceType":"datasetVersion"},{"datasetId":4899019,"sourceId":8255378,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
