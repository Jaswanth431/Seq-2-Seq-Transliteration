{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8245488,"sourceType":"datasetVersion","datasetId":4891846},{"sourceId":8255378,"sourceType":"datasetVersion","datasetId":4899019}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jaswanth431/dl-assignment-3-atten?scriptVersionId=177735535\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\nimport random\nimport wandb","metadata":{"_uuid":"549ce73a-e98b-42bd-a1d8-2cab5faae7d5","_cell_guid":"f4ce1158-c562-476f-8f7d-0325f0b787c4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-15T04:42:45.572608Z","iopub.execute_input":"2024-05-15T04:42:45.573046Z","iopub.status.idle":"2024-05-15T04:42:45.579812Z","shell.execute_reply.started":"2024-05-15T04:42:45.573012Z","shell.execute_reply":"2024-05-15T04:42:45.578556Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=\"62cfafb7157dfba7fdd6132ac9d757ccd913aaaf\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T04:42:45.58491Z","iopub.execute_input":"2024-05-15T04:42:45.585276Z","iopub.status.idle":"2024-05-15T04:42:45.751336Z","shell.execute_reply.started":"2024-05-15T04:42:45.585247Z","shell.execute_reply":"2024-05-15T04:42:45.750256Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nEND_TOKEN = '>'\nSTART_TOKEN = '<'\nPAD_TOKEN = '_'\nTEACHER_FORCING_RATIO = 0.5\n\ntrain_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_train.csv\"\ntest_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_test.csv\"\nval_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_valid.csv\"\n\ntrain_df = pd.read_csv(train_csv, header=None)\ntest_df = pd.read_csv(test_csv, header=None)\nval_df = pd.read_csv(val_csv, header=None)\ntrain_source, train_target = train_df[0].to_numpy(), train_df[1].to_numpy();\nval_source, val_target = val_df[0].to_numpy(), val_df[1].to_numpy();","metadata":{"_uuid":"b6363452-eb4f-4705-aa3f-fdec65f8e9db","_cell_guid":"c7dcb64a-f20a-4fa2-8b31-9801c3a762b8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-15T04:42:45.753602Z","iopub.execute_input":"2024-05-15T04:42:45.754344Z","iopub.status.idle":"2024-05-15T04:42:45.881542Z","shell.execute_reply.started":"2024-05-15T04:42:45.754302Z","shell.execute_reply":"2024-05-15T04:42:45.880502Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"def add_padding(source_data, MAX_LENGTH):\n    padded_source_strings = []\n    for i in range(len(source_data)):\n        source_str =START_TOKEN+ source_data[i] + END_TOKEN\n        # Truncate or pad source sequence\n        source_str = source_str[:MAX_LENGTH]\n        source_str += PAD_TOKEN * (MAX_LENGTH - len(source_str))\n\n        padded_source_strings.append(source_str)\n        \n    return padded_source_strings\n\n\ndef generate_string_to_sequence(source_data, source_char_index_dict):\n    source_sequences = []\n    for i in range(len(source_data)):\n        source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n    source_sequences = pad_sequence(source_sequences, batch_first=True, padding_value=2)\n    return source_sequences\n\n\ndef get_chars(str, char_index_dict):\n    chars_indexes = []\n    for ch in str:\n        chars_indexes.append(char_index_dict[ch])\n    return torch.tensor(chars_indexes, device=device)\n\n\ndef preprocess_data(source_data, target_data):\n    data = {\n        \"source_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n        \"target_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n        \"source_char_index\": {START_TOKEN: 0, END_TOKEN:1, PAD_TOKEN:2},\n        \"source_index_char\": {0:START_TOKEN, 1: END_TOKEN, 2:PAD_TOKEN},\n        \"target_char_index\": {START_TOKEN: 0, END_TOKEN:1, PAD_TOKEN:2},\n        \"target_index_char\": {0:START_TOKEN, 1: END_TOKEN, 2:PAD_TOKEN},\n        \"source_len\": 3,\n        \"target_len\": 3,\n        \"source_data\": source_data,\n        \"target_data\": target_data,\n        \"source_data_seq\": [],\n        \"target_data_seq\": []\n    }\n    \n    data[\"INPUT_MAX_LENGTH\"] = max(len(string) for string in source_data) +2\n    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data)+2\n\n    \n    padded_source_strings=add_padding(source_data, data[\"INPUT_MAX_LENGTH\"])\n    padded_target_strings = add_padding(target_data, data[\"OUTPUT_MAX_LENGTH\"])\n    \n    for i in range(len(padded_source_strings)):\n        for c in padded_source_strings[i]:\n            if data[\"source_char_index\"].get(c) is None:\n                data[\"source_chars\"].append(c)\n                idx = len(data[\"source_chars\"]) - 1\n                data[\"source_char_index\"][c] = idx\n                data[\"source_index_char\"][idx] = c\n        for c in padded_target_strings[i]:\n            if data[\"target_char_index\"].get(c) is None:\n                data[\"target_chars\"].append(c)\n                idx = len(data[\"target_chars\"]) - 1\n                data[\"target_char_index\"][c] = idx\n                data[\"target_index_char\"][idx] = c\n\n    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n    data['target_data_seq'] = generate_string_to_sequence(padded_target_strings,  data['target_char_index'])\n#     print(data[\"source_data\"][0])\n#     print(data[\"source_data_seq\"][0])\n#     print(data[\"target_data\"][0])\n#     print(data[\"target_data_seq\"][0])\n\n    \n    data[\"source_len\"] = len(data[\"source_chars\"])\n    data[\"target_len\"] = len(data[\"target_chars\"])\n    \n    return data\n\n# data = preprocess_data(copy.copy(train_source), copy.copy(train_target))","metadata":{"_uuid":"3708abf3-c2df-4e8d-8cfe-af3faab7bc53","_cell_guid":"fb22b602-e1fc-496b-93dc-f4d3178870d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-15T04:42:45.883142Z","iopub.execute_input":"2024-05-15T04:42:45.883424Z","iopub.status.idle":"2024-05-15T04:42:45.90179Z","shell.execute_reply.started":"2024-05-15T04:42:45.883401Z","shell.execute_reply":"2024-05-15T04:42:45.900591Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def get_cell_type(cell_type):\n    if(cell_type == \"RNN\"):\n        return nn.RNN\n    elif(cell_type == \"LSTM\"):\n        return nn.LSTM\n    elif(cell_type == \"GRU\"):\n        return nn.GRU\n    else:\n        print(\"Specify correct cell type\")\n        \nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size)\n        self.Ua = nn.Linear(hidden_size, hidden_size)\n        self.Va = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys):\n        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n        scores = scores.squeeze().unsqueeze(1)\n        weights = F.softmax(scores, dim=0)\n        weights = weights.permute(2,1,0)\n        keys = keys.permute(1,0,2)\n        context = torch.bmm(weights, keys)\n        return context, weights\n\n\nclass Encoder(nn.Module):\n    def __init__(self, h_params, data, device ):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embd_dim\"])\n        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],num_layers=h_params[\"number_of_layers\"], batch_first=True)\n        self.device=device\n        self.h_params = h_params\n        self.data = data\n    def forward(self, input , encoder_curr_state):\n        input_length = self.data[\"INPUT_MAX_LENGTH\"]\n        batch_size = self.h_params[\"batch_size\"]\n        hidden_neurons = self.h_params[\"hidden_layer_neurons\"]\n        layers = self.h_params[\"number_of_layers\"]\n        encoder_states  = torch.zeros(input_length, layers, batch_size, hidden_neurons, device=device )\n        for i in range(input_length):\n            current_input = input[:, i].view(batch_size,1)\n            _, encoder_curr_state = self.forward_step(current_input, encoder_curr_state)\n            if self.h_params[\"cell_type\"] == \"LSTM\":\n                encoder_states[i] = encoder_curr_state[1]\n            else:\n                encoder_states[i] = encoder_curr_state\n        return encoder_states, encoder_curr_state\n    \n    def forward_step(self, current_input, prev_state):\n        embd_input = self.embedding(current_input)\n#         print(embd_input.shape, prev_state.shape)\n        output, prev_state = self.cell(embd_input, prev_state)\n        return output, prev_state\n        \n    def getInitialState(self):\n        return torch.zeros(self.h_params[\"number_of_layers\"],self.h_params[\"batch_size\"],self.h_params[\"hidden_layer_neurons\"], device=self.device)\n\n    \nclass Decoder(nn.Module):\n    def __init__(self, h_params, data,device):\n        super(Decoder, self).__init__()\n        self.attention = Attention(h_params[\"hidden_layer_neurons\"]).to(device)\n        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embd_dim\"])\n        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"hidden_layer_neurons\"] +h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],num_layers=h_params[\"number_of_layers\"], batch_first=True)\n        self.fc = nn.Linear(h_params[\"hidden_layer_neurons\"], data[\"target_len\"])\n        self.softmax = nn.LogSoftmax(dim=2)\n        self.h_params = h_params\n        self.data = data\n        self.device = device\n\n    def forward(self, decoder_current_state, encoder_final_layers, target_batch, loss_fn, teacher_forcing_enabled=True):\n#         print(\"Teacher forcing:\", teacher_forcing_enabled)\n        batch_size = self.h_params[\"batch_size\"]\n        decoder_current_input = torch.full((batch_size,1),self.data[\"target_char_index\"][START_TOKEN], device=self.device)\n        embd_input = self.embedding(decoder_current_input)\n        curr_embd = F.relu(embd_input)\n        decoder_actual_output = []\n        attentions = []\n        loss = 0\n        \n        use_teacher_forcing = False\n        if(teacher_forcing_enabled):\n            use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n        for i in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n            decoder_output, decoder_current_state, attn_weights = self.forward_step(decoder_current_input, decoder_current_state, encoder_final_layers)\n            attentions.append(attn_weights)\n            topv, topi = decoder_output.topk(1)\n            decoder_current_input = topi.squeeze().detach()\n            decoder_actual_output.append(decoder_current_input)\n\n            if(target_batch==None):\n                decoder_current_input = decoder_current_input.view(self.h_params[\"batch_size\"], 1)\n            else:\n                curr_target_chars = target_batch[:, i]\n                if(i<self.data[\"OUTPUT_MAX_LENGTH\"]-1):\n                    if use_teacher_forcing:\n                        decoder_current_input = target_batch[:, i+1].view(self.h_params[\"batch_size\"], 1)\n                    else:\n                        decoder_current_input = decoder_current_input.view(self.h_params[\"batch_size\"], 1)\n                decoder_output = decoder_output[:, -1, :]\n                loss+=(loss_fn(decoder_output, curr_target_chars))\n\n        decoder_actual_output = torch.cat(decoder_actual_output,dim=0).view(self.data[\"OUTPUT_MAX_LENGTH\"], self.h_params[\"batch_size\"]).transpose(0,1)\n\n        correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n        return decoder_actual_output, attentions, loss, correct\n    \n    def forward_step(self, current_input, prev_state, encoder_final_layers):\n        embd_input = self.embedding(current_input)\n        if self.h_params[\"cell_type\"] == \"LSTM\":\n            context , attn_weights = self.attention(prev_state[1][-1,:,:], encoder_final_layers)\n        else:\n            context , attn_weights = self.attention(prev_state[-1,:,:], encoder_final_layers)\n        curr_embd = F.relu(embd_input)\n        input_gru = torch.cat((curr_embd, context), dim=2)\n        output, prev_state = self.cell(input_gru, prev_state)\n        output = self.softmax(self.fc(output))\n        return output, prev_state, attn_weights  ","metadata":{"_uuid":"58e2b696-e336-4cb3-b6e6-d14eac238c96","_cell_guid":"52035e98-753c-4cd0-8dec-2cec35aab863","execution":{"iopub.status.busy":"2024-05-15T04:42:45.904227Z","iopub.execute_input":"2024-05-15T04:42:45.904569Z","iopub.status.idle":"2024-05-15T04:42:45.940032Z","shell.execute_reply.started":"2024-05-15T04:42:45.904542Z","shell.execute_reply":"2024-05-15T04:42:45.938835Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, data):\n        self.source_data_seq = data[0]\n        self.target_data_seq = data[1]\n    \n    def __len__(self):\n        return len(self.source_data_seq)\n    \n    def __getitem__(self, idx):\n        source_data = self.source_data_seq[idx]\n        target_data = self.target_data_seq[idx]\n        return source_data, target_data\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T04:42:45.941255Z","iopub.execute_input":"2024-05-15T04:42:45.941603Z","iopub.status.idle":"2024-05-15T04:42:45.954952Z","shell.execute_reply.started":"2024-05-15T04:42:45.941567Z","shell.execute_reply":"2024-05-15T04:42:45.953774Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"   \ndef evaluate(encoder, decoder, data, dataloader, device, h_params, loss_fn, use_teacher_forcing = False):\n    correct_predictions = 0\n    total_loss = 0\n    total_predictions = len(dataloader.dataset)\n    number_of_batches = len(dataloader)\n    for batch_num, (source_batch, target_batch) in enumerate(dataloader):\n        \n        encoder_initial_state = encoder.getInitialState()\n        if h_params[\"cell_type\"] == \"LSTM\":\n            encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n        encoder_states, encoder_final_state = encoder(source_batch,encoder_initial_state)\n\n        decoder_current_state = encoder_final_state\n        encoder_final_layer_states = encoder_states[:, -1, :, :]\n\n        loss = 0\n        correct = 0\n\n        decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n        if(batch_num == 0):\n                for j in range(20):\n                    print(make_strings(data,source_batch[j],target_batch[j],decoder_output[j]))\n      \n        correct_predictions+=correct\n        total_loss +=loss\n    \n    accuracy = correct_predictions / total_predictions\n    total_loss /= number_of_batches\n    \n    return accuracy, total_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T04:42:45.956284Z","iopub.execute_input":"2024-05-15T04:42:45.956651Z","iopub.status.idle":"2024-05-15T04:42:45.971087Z","shell.execute_reply.started":"2024-05-15T04:42:45.956622Z","shell.execute_reply":"2024-05-15T04:42:45.969975Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def make_strings(data, source, target, output):\n    source_string = \"\"\n    target_string = \"\"\n    output_string = \"\"\n#     print(output)\n    for i in source:\n#         print(i.item())\n        source_string+=(data['source_index_char'][i.item()])\n    for i in target:\n        target_string+=(data['target_index_char'][i.item()])\n    for i in output:\n        output_string+=(data['target_index_char'][i.item()])\n    return source_string, target_string, output_string\n                        \n\ndef train_loop(encoder, decoder,h_params, data, data_loader, device, val_dataloader, use_teacher_forcing=True):\n    \n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n    \n    loss_fn = nn.NLLLoss()\n    \n    total_predictions = len(data_loader.dataset)\n    total_batches = len(data_loader)\n    \n    for ep in range(h_params[\"epochs\"]):\n        total_correct = 0\n        total_loss = 0\n        for batch_num, (source_batch, target_batch) in enumerate(data_loader):\n#             if(batch_num>0):\n#                 break\n            encoder_initial_state = encoder.getInitialState()\n            \n            if h_params[\"cell_type\"] == \"LSTM\":\n                encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n            encoder_states, encoder_final_state = encoder(source_batch,encoder_initial_state)\n            \n            decoder_current_state = encoder_final_state\n            encoder_final_layer_states = encoder_states[:, -1, :, :]\n            \n            \n            loss = 0\n            correct = 0\n            \n            decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n            total_correct +=correct\n            total_loss += loss.item()/data[\"OUTPUT_MAX_LENGTH\"]\n            if(batch_num == 0):\n                    for j in range(20):\n                        print(make_strings(data,source_batch[j],target_batch[j],decoder_output[j]))\n            if(batch_num%20 == 0):\n                print(\"ep:\", ep, \" bt:\", batch_num, \" loss:\", loss.item()/data[\"OUTPUT_MAX_LENGTH\"], \" acc: \", correct/h_params[\"batch_size\"])\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            loss.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            \n        train_acc = total_correct/total_predictions\n        train_loss = total_loss/total_batches\n        val_acc, val_loss = evaluate(encoder, decoder, data, val_dataloader,device, h_params, loss_fn, False)\n        print(\"ep: \", ep, \" train acc:\", train_acc, \" train loss:\", train_loss, \" val acc:\", val_acc, \" val loss:\", val_loss.item()/data[\"OUTPUT_MAX_LENGTH\"])\n        wandb.log({\"train_accuracy\":train_acc, \"train_loss\":train_loss, \"val_accuracy\":val_acc, \"val_loss\":val_loss, \"epoch\":ep})\n\n","metadata":{"_uuid":"c743c5b1-1106-4388-a594-e4e301225bad","_cell_guid":"0dc20b89-db7b-4230-a7ec-e76d1896adee","scrolled":true,"execution":{"iopub.status.busy":"2024-05-15T04:42:45.972523Z","iopub.execute_input":"2024-05-15T04:42:45.972887Z","iopub.status.idle":"2024-05-15T04:42:45.992287Z","shell.execute_reply.started":"2024-05-15T04:42:45.972846Z","shell.execute_reply":"2024-05-15T04:42:45.991217Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# h_params={\n#     \"char_embd_dim\" : 128, \n#     \"hidden_layer_neurons\":256,\n#     \"batch_size\":32,\n#     \"number_of_layers\":2,\n#     \"learning_rate\":0.0001,\n#     \"epochs\":20,\n#     \"cell_type\":\"GRU\",\n#     \"dropout\":0.3,\n#     \"optimizer\":\"nadam\"\n# }\n\ndef prepare_dataloaders(train_source, train_target, val_source, val_target, h_params):\n    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n    training_data = [data[\"source_data_seq\"], data['target_data_seq']]\n    train_dataset = MyDataset(training_data)\n    train_dataloader = DataLoader(train_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n\n    #prepare validation data\n    val_padded_source_strings=add_padding(val_source, data[\"INPUT_MAX_LENGTH\"])\n    val_padded_target_strings = add_padding(val_target, data[\"OUTPUT_MAX_LENGTH\"])\n    val_source_sequences = generate_string_to_sequence(val_padded_source_strings,  data['source_char_index'])\n    val_target_sequences = generate_string_to_sequence(val_padded_target_strings,  data['target_char_index'])\n    validation_data = [val_source_sequences, val_target_sequences]\n    val_dataset = MyDataset(validation_data)\n    val_dataloader = DataLoader(val_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n    return train_dataloader, val_dataloader, data\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T04:42:45.993583Z","iopub.execute_input":"2024-05-15T04:42:45.99432Z","iopub.status.idle":"2024-05-15T04:42:46.00726Z","shell.execute_reply.started":"2024-05-15T04:42:45.99429Z","shell.execute_reply":"2024-05-15T04:42:46.006341Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def train(h_params, data, device, data_loader, val_dataloader, use_teacher_forcing=True):\n    encoder = Encoder(h_params, data, device).to(device)\n    decoder = Decoder(h_params, data, device).to(device)\n    train_loop(encoder, decoder,h_params, data, data_loader,device, val_dataloader, use_teacher_forcing)\n    torch.cuda.empty_cache() \n    del encoder\n    del decoder\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T04:42:46.010379Z","iopub.execute_input":"2024-05-15T04:42:46.010738Z","iopub.status.idle":"2024-05-15T04:42:46.022805Z","shell.execute_reply.started":"2024-05-15T04:42:46.010685Z","shell.execute_reply":"2024-05-15T04:42:46.021759Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# config = h_params\n# # run = wandb.init(project=\"DL Assignment 3 With Attention\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config)\n# train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, h_params)\n# train(h_params, data, device, train_dataloader, val_dataloader, True)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T04:42:46.024129Z","iopub.execute_input":"2024-05-15T04:42:46.024473Z","iopub.status.idle":"2024-05-15T04:42:46.033651Z","shell.execute_reply.started":"2024-05-15T04:42:46.024444Z","shell.execute_reply":"2024-05-15T04:42:46.032752Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"#Run this cell to run a sweep with appropriate parameters\nsweep_params = {\n    'method' : 'bayes',\n    'name'   : 'DL Assignment 3 With Attention',\n    'metric' : {\n        'goal' : 'maximize',\n        'name' : 'val_accuracy',\n    },\n    'parameters' : {\n        'epochs':{'values' : [15, 20]},\n        'learning_rate':{'values' : [0.001, 0.0001]},\n        'batch_size':{'values':[32,64, 128]},\n        'char_embd_dim':{'values' : [64, 128, 256] } ,\n        'number_of_layers':{'values' : [1,2,3,4]},\n        'optimizer':{'values':['nadam','adam']},\n        'cell_type':{'values' : [\"RNN\",\"LSTM\", \"GRU\"]},\n        'hidden_layer_neurons':{'values': [ 128, 256, 512]},\n        'dropout':{'values': [0,0.2, 0.3]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 3 With Attention\")\ndef main():\n    torch.cuda.empty_cache()\n    gc.collect()\n    wandb.init(project=\"DL Assignment 3\" )\n    config = wandb.config\n    with wandb.init(project=\"DL Assignment 3\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config):\n        train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n        train(config, data, device, train_dataloader, val_dataloader, True)\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T04:42:46.034836Z","iopub.execute_input":"2024-05-15T04:42:46.035674Z","iopub.status.idle":"2024-05-15T04:42:46.305762Z","shell.execute_reply.started":"2024-05-15T04:42:46.035644Z","shell.execute_reply":"2024-05-15T04:42:46.304736Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Create sweep with ID: 5yt059tf\nSweep URL: https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/5yt059tf\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.agent(\"f4esgkqv\", function=main, count=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T04:42:46.307404Z","iopub.execute_input":"2024-05-15T04:42:46.307842Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wdru7z28 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240515_044249-wdru7z28</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wdru7z28' target=\"_blank\">valiant-sweep-145</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wdru7z28' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wdru7z28</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:wdru7z28) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">valiant-sweep-145</strong> at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wdru7z28' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wdru7z28</a><br/> View project at: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240515_044249-wdru7z28/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:wdru7z28). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240515_044306-wdru7z28</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wdru7z28' target=\"_blank\">LSTM_nadam_ep_20_lr_0.0001_embd_256_hid_lyr_neur_256_bs_32_enc_layers_1_dec_layers_1_dropout_0</a></strong> to <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wdru7z28' target=\"_blank\">https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wdru7z28</a>"},"metadata":{}},{"name":"stdout","text":"('<kinkartavya>_________________', '<కింకర్తవ్య>___________', '>>>>>ఐ>>>>>>చచచచచచచచచచచ')\n('<chejaripotaronani>___________', '<చేజారిపోతారోనని>______', '>>>>>ఐఐఋఋఖఖఈఖఖఖ>>చచచచచచ')\n('<nichesukuntunna>_____________', '<నిచేసుకుంటున్న>_______', '>>>>>>>>>>>>>>>>>చచచచచచ')\n('<veyalsinanta>________________', '<వేయాల్సినంత>__________', '>>>ోఈఋ>>>>ఛఖ>చచచచచచచచచచ')\n('<erpatluchesamani>____________', '<ఏర్పాట్లుచేశామని>_____', '>>>>>ఈఈ>ఋ>>ఐదఈఈ>>>చచచచచ')\n('<padyaran>____________________', '<పద్యరాన్>_____________', '>>>>>ఐఈఛ>>>చచచచచచచచచచచచ')\n('<chigurlanu>__________________', '<చిగుర్లను>____________', '>>>>>ఐ>ఋఖ>>>చచచచచచచచచచచ')\n('<abhipraayaanike>_____________', '<అభిప్రాయానికే>________', '>>ఖ>ఋ>ఐఈఈఈఛ>>>>>చచచచచచచ')\n('<ennikalostaayannaaru>________', '<ఎన్నికలొస్తాయన్నారు>__', '>>>>>>>ఋఋఋ>>>ఈఖ>>>ఈ>>చచ')\n('<papkapuram>__________________', '<పప్కాపురం>____________', '>>ఋ>>>ఓ>ఐఖ>చచచచచచచచచచచచ')\n('<kaamyamutho>_________________', '<కామ్యముతో>____________', '>>>>>>త>ఖఖ>>చచచచచచచచచచచ')\n('<repeaters>___________________', '<రిపీటర్లు>____________', '>ట>ఋ>టఐ>ఋ>>>చచచచచచచచచచచ')\n('<karvirostra>_________________', '<కర్విరోస్త్రా>________', '>>ట>>>ఐఖఖ>ఖ>ఐఐ>చచచచచచచచ')\n('<balapadavacchunu>____________', '<బలపడవచ్చును>__________', '>>ఋఋఋ>>>>>>>>>చచచచచచచచచ')\n('<salputondi>__________________', '<సల్పుతోంది>___________', '>>ఖ>ఋ>ఖఖఖఖ>>>చచచచచచచచచచ')\n('<tolaginchutalo>______________', '<తొలగించుటలో>__________', '>>>ఋఖ>ఖ>>ఈఋఖఋఖచచచచచచచచచ')\n('<braslanu>____________________', '<బ్రాస్లను>____________', '>>>టఐ>>ఋఛ>>>చచచచచచచచచచచ')\n('<yunna>_______________________', '<యున్నా>_______________', '>ో>>>>>>>చచచచచచచచచచచచచచ')\n('<gev>_________________________', '<గెవ్>_________________', '>>ఐ>>>>చచచచచచచచచచచచచచచచ')\n('<kallajusi>___________________', '<కళ్లజూసి>_____________', '>>>>>>ఖచ>>చచచచచచచచచచచచచ')\nep: 0  bt: 0  loss: 4.251927583113961  acc:  0.0\nep: 0  bt: 20  loss: 2.9283971371858017  acc:  0.0\nep: 0  bt: 40  loss: 2.4212666387143345  acc:  0.0\nep: 0  bt: 60  loss: 1.9954817398734714  acc:  0.0\nep: 0  bt: 80  loss: 1.8546747953995415  acc:  0.0\nep: 0  bt: 100  loss: 1.7515346692956013  acc:  0.0\nep: 0  bt: 120  loss: 1.6559960738472317  acc:  0.0\nep: 0  bt: 140  loss: 1.7848799332328464  acc:  0.0\nep: 0  bt: 160  loss: 1.8342447695524797  acc:  0.0\nep: 0  bt: 180  loss: 1.9681356678838315  acc:  0.0\nep: 0  bt: 200  loss: 1.6680394048276155  acc:  0.0\nep: 0  bt: 220  loss: 1.3608354485553245  acc:  0.0\nep: 0  bt: 240  loss: 1.3301302039104959  acc:  0.0\nep: 0  bt: 260  loss: 1.7454807447350544  acc:  0.0\nep: 0  bt: 280  loss: 1.785347316576087  acc:  0.0\nep: 0  bt: 300  loss: 1.6801641713018003  acc:  0.0\nep: 0  bt: 320  loss: 1.6677229508109714  acc:  0.0\nep: 0  bt: 340  loss: 1.5850549780804177  acc:  0.0\nep: 0  bt: 360  loss: 1.5394439697265625  acc:  0.0\nep: 0  bt: 380  loss: 1.7594914643660835  acc:  0.0\nep: 0  bt: 400  loss: 0.9410203850787618  acc:  0.03125\nep: 0  bt: 420  loss: 1.7520156528638757  acc:  0.0\nep: 0  bt: 440  loss: 1.6317433896272078  acc:  0.0\nep: 0  bt: 460  loss: 1.6733934153681216  acc:  0.0\nep: 0  bt: 480  loss: 0.7221760127855383  acc:  0.21875\nep: 0  bt: 500  loss: 0.7496331256368886  acc:  0.1875\nep: 0  bt: 520  loss: 1.6062945490298064  acc:  0.0\nep: 0  bt: 540  loss: 1.6778466597847317  acc:  0.0\nep: 0  bt: 560  loss: 0.5843880694845448  acc:  0.3125\nep: 0  bt: 580  loss: 1.8274957408075747  acc:  0.0\nep: 0  bt: 600  loss: 0.6384266563083815  acc:  0.34375\nep: 0  bt: 620  loss: 0.5232288941093113  acc:  0.46875\nep: 0  bt: 640  loss: 1.7459363522736921  acc:  0.0\nep: 0  bt: 660  loss: 1.5815872524095618  acc:  0.0\nep: 0  bt: 680  loss: 1.666963991911515  acc:  0.0\nep: 0  bt: 700  loss: 1.5833846382472827  acc:  0.0\nep: 0  bt: 720  loss: 0.4148826599121094  acc:  0.59375\nep: 0  bt: 740  loss: 0.5436677518098251  acc:  0.40625\nep: 0  bt: 760  loss: 0.4467032474020253  acc:  0.40625\nep: 0  bt: 780  loss: 1.43420177957286  acc:  0.0\nep: 0  bt: 800  loss: 1.6329430289890454  acc:  0.0\nep: 0  bt: 820  loss: 0.3847945254781972  acc:  0.53125\nep: 0  bt: 840  loss: 0.43938363116720447  acc:  0.25\nep: 0  bt: 860  loss: 0.3925162605617357  acc:  0.4375\nep: 0  bt: 880  loss: 1.7235941679581352  acc:  0.0\nep: 0  bt: 900  loss: 1.4252395629882812  acc:  0.0\nep: 0  bt: 920  loss: 0.307598238405974  acc:  0.625\nep: 0  bt: 940  loss: 0.3902532743371051  acc:  0.40625\nep: 0  bt: 960  loss: 0.32269788824993634  acc:  0.53125\nep: 0  bt: 980  loss: 1.5449165675951086  acc:  0.0\nep: 0  bt: 1000  loss: 0.3021399041880732  acc:  0.65625\nep: 0  bt: 1020  loss: 0.278918515080991  acc:  0.65625\nep: 0  bt: 1040  loss: 1.6247714498768682  acc:  0.0\nep: 0  bt: 1060  loss: 0.2809011832527492  acc:  0.625\nep: 0  bt: 1080  loss: 0.25729776465374493  acc:  0.59375\nep: 0  bt: 1100  loss: 1.5895065639329993  acc:  0.0\nep: 0  bt: 1120  loss: 0.2659663946732231  acc:  0.65625\nep: 0  bt: 1140  loss: 0.26821318916652515  acc:  0.65625\nep: 0  bt: 1160  loss: 1.6374201567276665  acc:  0.0\nep: 0  bt: 1180  loss: 0.22433880101079526  acc:  0.71875\nep: 0  bt: 1200  loss: 1.4881250132685122  acc:  0.0\nep: 0  bt: 1220  loss: 0.22087350098983102  acc:  0.75\nep: 0  bt: 1240  loss: 1.566238900889521  acc:  0.0\nep: 0  bt: 1260  loss: 0.19605945504230002  acc:  0.8125\nep: 0  bt: 1280  loss: 1.4956973531971807  acc:  0.0\nep: 0  bt: 1300  loss: 0.2011947424515434  acc:  0.75\nep: 0  bt: 1320  loss: 0.20559561770895254  acc:  0.78125\nep: 0  bt: 1340  loss: 1.5949156387992527  acc:  0.0\nep: 0  bt: 1360  loss: 1.355186296545941  acc:  0.0\nep: 0  bt: 1380  loss: 1.4057975437330164  acc:  0.0\nep: 0  bt: 1400  loss: 0.17886410588803497  acc:  0.8125\nep: 0  bt: 1420  loss: 0.20943216655565344  acc:  0.71875\nep: 0  bt: 1440  loss: 1.6169665792713994  acc:  0.0\nep: 0  bt: 1460  loss: 1.5296571151069973  acc:  0.0\nep: 0  bt: 1480  loss: 1.4268535116444463  acc:  0.0\nep: 0  bt: 1500  loss: 0.18286259278007175  acc:  0.8125\nep: 0  bt: 1520  loss: 0.1577724373858908  acc:  0.875\nep: 0  bt: 1540  loss: 1.5486558001974355  acc:  0.0\nep: 0  bt: 1560  loss: 1.429876244586447  acc:  0.0\nep: 0  bt: 1580  loss: 0.16552918890248175  acc:  0.90625\n('<maalatii>____________________', '<మాలతీ>________________', '<మాాాిిి_______________')\n('<nursery>_____________________', '<నర్సరీ>_______________', '<స్్్రరరర>>____________')\n('<bharatam>____________________', '<భరతం>_________________', '<మారరర>>_______________')\n('<chenduchunnappatiki>_________', '<చెందుచున్నప్పటికి>____', '<చచచచచచచచచచచచిిిి______')\n('<saasanasabhaku>______________', '<శాసనసభకు>_____________', '<సాాాాాాాా>____________')\n('<nalumuulala>_________________', '<నలుమూలల>______________', '<నననుుుుుు_____________')\n('<fovoy>_______________________', '<ఫోవోయ్>_______________', '<విిోోో________________')\n('<bahubhaashaavaetta>__________', '<బహుభాషావేత్త>_________', '<బబబబబబబబబబబ>>_________')\n('<mayuura>_____________________', '<మయూర>_________________', '<మారరరర________________')\n('<caruvu>______________________', '<కరువు>________________', '<చుుుుు________________')\n('<neelachuupee>________________', '<నేలచూపే>______________', '<నననననుుు>>____________')\n('<gottaalu>____________________', '<గొట్టాలు>_____________', '<గగగగగగ>>______________')\n('<kungi>_______________________', '<కుంగి>________________', '<కుుుు>>_______________')\n('<busyy>_______________________', '<బిజీ>_________________', '<స్్్్్్>______________')\n('<avaamtharam>_________________', '<అవాంతరం>______________', '<వాాాాాా>>_____________')\n('<vayasulone>__________________', '<వయసులోనే>_____________', '<వాలలలలలలల>____________')\n('<mukkallo>____________________', '<ముక్కల్లో>____________', '<కకకకకలలల>_____________')\n('<vandanaagupta>_______________', '<వందనాగుప్త>___________', '<వాాాాాాాా>>___________')\n('<bharatiyulaku>_______________', '<భారతీయులకు>___________', '<బారరరుుుుుు___________')\n('<tizer>_______________________', '<టైజర్>________________', '<పెెెె_________________')\nep:  0  train acc: 0.210625  train loss: 1.1698655967479157  val acc: 0.0  val loss: 1.1046431168265964\n('<manasikanganaina>____________', '<మానసికంగానైనా>________', '<మాాాాాాానననననన________')\n('<dinchudamanukunnamu>_________', '<దించుదమనుకున్నాము>____', '<దదదదదదదదదదదదదదదద______')\n('<gundapu>_____________________', '<గుండపు>_______________', '<గగగగగుు_______________')\n('<sodhakudiki>_________________', '<శోధకుడికి>____________', '<సాకకకకకకక_____________')\n('<karindo>_____________________', '<కరిందో>_______________', '<కారరడడడ_______________')\n('<lopayikariga>________________', '<లోపాయికారిగా>_________', '<పారరిిిిిి>>__________')\n('<saadistaado>_________________', '<సాదిస్తాడో>___________', '<సాాాాాాా>>____________')\n('<seophore>____________________', '<సియోఫోర్>_____________', '<పెెెెె________________')\n('<muligipoyadu>________________', '<ములిగిపోయాడు>_________', '<మాలలలలలలలల>>__________')\n('<narsimhapuram>_______________', '<నర్సింహాపురం>_________', '<ననననన్్్్రరర__________')\n('<vimarsinchadaanni>___________', '<విమర్శించడాన్ని>______', '<విిిిిిిిిననననన_______')\n('<dudoni>______________________', '<దుదోని>_______________', '<దదదదదద________________')\n('<manjulataku>_________________', '<మంజులతకు>_____________', '<మాలలలలలలుు____________')\n('<munakalai>___________________', '<మునకలై>_______________', '<మాాాాా>>______________')\n('<tiirpulistuu>________________', '<తీర్పులిస్తూ>_________', '<పిిిిిిిిుుుుు________')\n('<tecchipettadamlo>____________', '<తెచ్చిపెట్టడంలో>______', '<చచచచచచచచచచ్్్్>_______')\n('<manollu>_____________________', '<మనోళ్ళు>______________', '<మాలలలలల>>_____________')\n('<terakekkabotundadam>_________', '<తెరకెక్కబోతుండడం>_____', '<ప్రరకకకకకకకకకకక>>_____')\n('<amaluparachaalsindigaa>______', '<అమలుపరచాల్సిందిగా>____', '<పారరరరరరాాాాాిిి>>____')\n('<kapukunna>___________________', '<కపుకున్న>_____________', '<కకకకకుుుు>____________')\nep: 1  bt: 0  loss: 1.437859576681386  acc:  0.0\nep: 1  bt: 20  loss: 1.3530797543733015  acc:  0.0\nep: 1  bt: 40  loss: 0.18084011907162872  acc:  0.875\nep: 1  bt: 60  loss: 0.155964239783909  acc:  0.78125\nep: 1  bt: 80  loss: 0.18522928072058636  acc:  0.75\nep: 1  bt: 100  loss: 0.14560097196827765  acc:  0.875\nep: 1  bt: 120  loss: 0.1496658947156823  acc:  0.78125\nep: 1  bt: 140  loss: 1.3282938418181047  acc:  0.0\nep: 1  bt: 160  loss: 0.20383426417475162  acc:  0.71875\nep: 1  bt: 180  loss: 0.15228091115536896  acc:  0.9375\nep: 1  bt: 200  loss: 1.4673199860945991  acc:  0.0\nep: 1  bt: 220  loss: 0.1300300307895826  acc:  0.90625\nep: 1  bt: 240  loss: 0.1509367901345958  acc:  0.78125\nep: 1  bt: 260  loss: 1.2722991445790166  acc:  0.0\nep: 1  bt: 280  loss: 1.1845446047575579  acc:  0.0\nep: 1  bt: 300  loss: 0.11303432091422703  acc:  0.96875\nep: 1  bt: 320  loss: 0.1171070907426917  acc:  0.875\nep: 1  bt: 340  loss: 0.10537145448767621  acc:  0.90625\nep: 1  bt: 360  loss: 0.12706669517185376  acc:  0.9375\nep: 1  bt: 380  loss: 1.246636266293733  acc:  0.0\nep: 1  bt: 400  loss: 0.1344918790070907  acc:  0.75\nep: 1  bt: 420  loss: 1.1998819268268088  acc:  0.0\nep: 1  bt: 440  loss: 0.09149138823799464  acc:  0.84375\nep: 1  bt: 460  loss: 1.307934802511464  acc:  0.0\nep: 1  bt: 480  loss: 1.1722610307776409  acc:  0.0\nep: 1  bt: 500  loss: 0.1164624483689018  acc:  0.84375\nep: 1  bt: 520  loss: 0.08299220126608144  acc:  1.0\nep: 1  bt: 540  loss: 0.10195068691087805  acc:  0.90625\nep: 1  bt: 560  loss: 1.0189190740170686  acc:  0.0\nep: 1  bt: 580  loss: 0.09921047998511273  acc:  0.875\nep: 1  bt: 600  loss: 0.09160393217335576  acc:  0.90625\nep: 1  bt: 620  loss: 0.08269582624020784  acc:  0.96875\nep: 1  bt: 640  loss: 0.9666269551152769  acc:  0.0\nep: 1  bt: 660  loss: 0.08102867914282758  acc:  0.9375\nep: 1  bt: 680  loss: 0.07006071961444357  acc:  0.9375\nep: 1  bt: 700  loss: 1.1967712070630945  acc:  0.0\nep: 1  bt: 720  loss: 1.3009328427522078  acc:  0.0\nep: 1  bt: 740  loss: 1.0801223257313604  acc:  0.0\nep: 1  bt: 760  loss: 0.09097864316857379  acc:  0.8125\nep: 1  bt: 780  loss: 0.08709013980367909  acc:  0.8125\nep: 1  bt: 800  loss: 0.9826829329780911  acc:  0.0\nep: 1  bt: 820  loss: 0.07151238814644191  acc:  0.84375\nep: 1  bt: 840  loss: 0.055556748224341354  acc:  0.9375\nep: 1  bt: 860  loss: 0.9534510736880095  acc:  0.0\nep: 1  bt: 880  loss: 1.1101177879001782  acc:  0.0\nep: 1  bt: 900  loss: 0.07371326633121657  acc:  0.9375\nep: 1  bt: 920  loss: 0.9481508006220278  acc:  0.0\nep: 1  bt: 940  loss: 0.9820582348367443  acc:  0.0\nep: 1  bt: 960  loss: 0.06399069143378217  acc:  0.90625\nep: 1  bt: 980  loss: 0.949868160745372  acc:  0.0\nep: 1  bt: 1000  loss: 0.05317098161448603  acc:  0.96875\nep: 1  bt: 1020  loss: 0.9114913111147673  acc:  0.0\nep: 1  bt: 1040  loss: 0.050007654272991676  acc:  0.96875\nep: 1  bt: 1060  loss: 0.05249250453451405  acc:  0.9375\nep: 1  bt: 1080  loss: 0.07425394265548042  acc:  0.90625\nep: 1  bt: 1100  loss: 0.053187831588413406  acc:  0.9375\nep: 1  bt: 1120  loss: 0.044300893078679626  acc:  0.96875\nep: 1  bt: 1140  loss: 0.9071083068847656  acc:  0.0\nep: 1  bt: 1160  loss: 0.06439605484838071  acc:  0.90625\nep: 1  bt: 1180  loss: 0.05405163764953613  acc:  0.90625\nep: 1  bt: 1200  loss: 0.8997287750244141  acc:  0.0\nep: 1  bt: 1220  loss: 0.8607385054878567  acc:  0.0\nep: 1  bt: 1240  loss: 0.04190621427867724  acc:  0.9375\nep: 1  bt: 1260  loss: 0.894916700280231  acc:  0.0\nep: 1  bt: 1280  loss: 0.9128505043361498  acc:  0.0\nep: 1  bt: 1300  loss: 0.8621117135752803  acc:  0.0\nep: 1  bt: 1320  loss: 0.049037539440652596  acc:  0.90625\nep: 1  bt: 1340  loss: 0.8255247033160665  acc:  0.0\nep: 1  bt: 1360  loss: 0.044299037560172706  acc:  0.96875\nep: 1  bt: 1380  loss: 0.850409051646357  acc:  0.0\nep: 1  bt: 1400  loss: 0.8481271163277004  acc:  0.0\nep: 1  bt: 1420  loss: 0.0412323552629222  acc:  1.0\nep: 1  bt: 1440  loss: 0.04478224464084791  acc:  0.9375\nep: 1  bt: 1460  loss: 0.748930143273395  acc:  0.0625\nep: 1  bt: 1480  loss: 0.05545013883839483  acc:  0.9375\nep: 1  bt: 1500  loss: 0.03129987094713294  acc:  0.96875\nep: 1  bt: 1520  loss: 0.036157333332559334  acc:  0.96875\nep: 1  bt: 1540  loss: 0.048761419627977455  acc:  0.9375\nep: 1  bt: 1560  loss: 0.03896256115125573  acc:  0.96875\nep: 1  bt: 1580  loss: 0.04192439887834632  acc:  0.96875\n('<poojanu>_____________________', '<పూజను>________________', '<పోోనను>_______________')\n('<praamthamuna>________________', '<ప్రాంతమున>____________', '<ప్రామమమున>____________')\n('<appaginchevaaru>_____________', '<అప్పగించేవారు>________', '<అపపపగించేవరరు>________')\n('<praani>______________________', '<ప్రాణి>_______________', '<ప్రాని>_______________')\n('<ranuranu>____________________', '<రానురాను>_____________', '<రానుుు>_______________')\n('<amari>_______________________', '<అమరి>_________________', '<అమరరి>________________')\n('<shareeram>___________________', '<శరీరం>________________', '<సరరరమమ>_______________')\n('<varadalaku>__________________', '<వరదలకు>_______________', '<వరరలకు>_______________')\n('<crazi>_______________________', '<క్రేజీ>_______________', '<క్రిి>________________')\n('<utpattikavadam>______________', '<ఉత్పత్తికావడం>________', '<ఉత్్తతతికవడడ>_________')\n('<rachinchinapudu>_____________', '<రచించినపుడు>__________', '<రరిిచచననపుుు>_________')\n('<haridwar>____________________', '<హరీద్వార్>____________', '<హరిి్్రర>>____________')\n('<mukkalugaa>__________________', '<ముక్కలుగా>____________', '<ముక్కలుగా>____________')\n('<atyunnatamaina>______________', '<అత్యున్నతమైన>_________', '<అట్యున్్తమనన>_________')\n('<shuutter>____________________', '<షట్టర్>_______________', '<సుతతత్ర్>_____________')\n('<tadavannattu>________________', '<తడవన్నట్టు>___________', '<తడడనన్టటటట>___________')\n('<korikalu>____________________', '<కోరికలు>______________', '<కోరికాలు>_____________')\n('<yerpariche>__________________', '<ఏర్పరిచే>_____________', '<యేర్రరిచే>____________')\n('<tolinalla>___________________', '<తొలినాళ్ళ>____________', '<తోలినలలలల_____________')\n('<angeekarinchaadani>__________', '<అంగీకరించాడని>________', '<అంగగకరించానిి>________')\nep:  1  train acc: 0.44966796875  train loss: 0.5678672095274788  val acc: 0.042724609375  val loss: 0.5752475987309995\n('<talapettekaaryaalu>__________', '<తలపెట్టేకార్యాలు>_____', '<తలపెట్టేకార్యాలు>_____')\n('<vaarikundi>__________________', '<వారికుంది>____________', '<వారికుంది>____________')\n('<divaaraatrulu>_______________', '<దివారాత్రులు>_________', '<దివారాత్రులు>_________')\n('<chandanada>__________________', '<చందనాడ>_______________', '<చందనాడ>_______________')\n('<vidichipettaranna>___________', '<విడిచిపెట్టరన్న>______', '<విడిచిపెట్టరన్న>______')\n('<chakraakrutilo>______________', '<చక్రాకృతిలో>__________', '<చక్రాకరతిలో>__________')\n('<kasakudi>____________________', '<కాసాకుడి>_____________', '<కాసాకుడి>_____________')\n('<aakukooralante>______________', '<ఆకుకూరలంటే>___________', '<ఆకుకూరలంటే>___________')\n('<backhoe>_____________________', '<బ్యాక్హో>_____________', '<బ్యాక్హో>_____________')\n('<rasipedata>__________________', '<రాసిపెడతా>____________', '<రాసిపెడతా>____________')\n('<labhalunna>__________________', '<లాభాలున్నా>___________', '<లాభాలున్నా>___________')\n('<suutnu>______________________', '<సూట్ను>_______________', '<సూట్ను>_______________')\n('<bayatikostaanu>______________', '<బయటికొస్తాను>_________', '<బయటికొస్తాను>_________')\n('<nirvasitulanu>_______________', '<నిర్వసితులను>_________', '<నిర్వసితులను>_________')\n('<navvinchagalaru>_____________', '<నవ్వించగలరు>__________', '<నవ్వించగలరు>__________')\n('<balapadatamlo>_______________', '<బలపడటంలో>_____________', '<బలపడటంలో>_____________')\n('<mahaakaarana>________________', '<మహాకారణ>______________', '<మహాకారణ>______________')\n('<manasundi>___________________', '<మనసుంది>______________', '<మనసుంది>______________')\n('<kalamuntayi>_________________', '<కాలముంటాయి>___________', '<కాలముంటాయి>___________')\n('<swaagathinchi>_______________', '<స్వాగతించి>___________', '<స్వాగతించి>___________')\nep: 2  bt: 0  loss: 0.03324924344601839  acc:  0.96875\nep: 2  bt: 20  loss: 0.05582737922668457  acc:  0.9375\nep: 2  bt: 40  loss: 0.023237746694813603  acc:  1.0\nep: 2  bt: 60  loss: 0.5972152378248132  acc:  0.03125\nep: 2  bt: 80  loss: 0.028894307820693306  acc:  0.96875\nep: 2  bt: 100  loss: 0.03054654079934825  acc:  0.96875\nep: 2  bt: 120  loss: 0.02619110501330832  acc:  0.96875\nep: 2  bt: 140  loss: 0.0579544616782147  acc:  0.84375\nep: 2  bt: 160  loss: 0.6151370172915251  acc:  0.0625\nep: 2  bt: 180  loss: 0.034766842489657196  acc:  0.96875\nep: 2  bt: 200  loss: 0.026009795458420464  acc:  0.96875\nep: 2  bt: 220  loss: 0.5831533514935038  acc:  0.125\nep: 2  bt: 240  loss: 0.5911917893782906  acc:  0.15625\nep: 2  bt: 260  loss: 0.550742481065833  acc:  0.09375\nep: 2  bt: 280  loss: 0.050974208375682  acc:  0.90625\nep: 2  bt: 300  loss: 0.035240963749263596  acc:  0.9375\nep: 2  bt: 320  loss: 0.06741119985995085  acc:  0.90625\nep: 2  bt: 340  loss: 0.5789917655613112  acc:  0.0625\nep: 2  bt: 360  loss: 0.6926706562871519  acc:  0.0625\nep: 2  bt: 380  loss: 0.6718830025714376  acc:  0.09375\nep: 2  bt: 400  loss: 0.7795764259670092  acc:  0.0\nep: 2  bt: 420  loss: 0.6824742607448412  acc:  0.0625\nep: 2  bt: 440  loss: 0.024316240911898407  acc:  1.0\nep: 2  bt: 460  loss: 0.03912645837535029  acc:  0.9375\nep: 2  bt: 480  loss: 0.04598087331523066  acc:  0.90625\nep: 2  bt: 500  loss: 0.030971610027810802  acc:  0.90625\nep: 2  bt: 520  loss: 0.032136823820031205  acc:  0.96875\nep: 2  bt: 540  loss: 0.017385708249133568  acc:  1.0\nep: 2  bt: 560  loss: 0.021460962036381596  acc:  0.96875\nep: 2  bt: 580  loss: 0.026113784831503162  acc:  0.96875\nep: 2  bt: 600  loss: 0.021154332420100338  acc:  0.96875\nep: 2  bt: 620  loss: 0.77404669056768  acc:  0.03125\nep: 2  bt: 640  loss: 0.5236077101334281  acc:  0.125\nep: 2  bt: 660  loss: 0.03943882040355517  acc:  0.9375\nep: 2  bt: 680  loss: 0.6152391019074813  acc:  0.125\nep: 2  bt: 700  loss: 0.6464670430059019  acc:  0.0625\nep: 2  bt: 720  loss: 0.015054995599000351  acc:  1.0\nep: 2  bt: 740  loss: 0.6211314408675485  acc:  0.0625\nep: 2  bt: 760  loss: 0.5975572751915973  acc:  0.0625\nep: 2  bt: 780  loss: 0.030927339325780453  acc:  0.90625\nep: 2  bt: 800  loss: 0.6487464075503142  acc:  0.0625\nep: 2  bt: 820  loss: 0.025235189043957253  acc:  1.0\nep: 2  bt: 840  loss: 0.530734228051227  acc:  0.21875\nep: 2  bt: 860  loss: 0.5868920450625212  acc:  0.125\nep: 2  bt: 880  loss: 0.48791118290113367  acc:  0.125\nep: 2  bt: 900  loss: 0.03910790837329367  acc:  0.90625\nep: 2  bt: 920  loss: 0.026023750719816788  acc:  1.0\nep: 2  bt: 940  loss: 0.5264372618302054  acc:  0.0625\nep: 2  bt: 960  loss: 0.554367272750191  acc:  0.0625\nep: 2  bt: 980  loss: 0.029774007589920708  acc:  0.9375\nep: 2  bt: 1000  loss: 0.6490196145099142  acc:  0.1875\nep: 2  bt: 1020  loss: 0.014534858257874199  acc:  0.96875\nep: 2  bt: 1040  loss: 0.6298933443815812  acc:  0.125\nep: 2  bt: 1060  loss: 0.5472722675489343  acc:  0.125\nep: 2  bt: 1080  loss: 0.015443514222684114  acc:  1.0\nep: 2  bt: 1100  loss: 0.02297628185023432  acc:  0.90625\nep: 2  bt: 1120  loss: 0.01904267720554186  acc:  1.0\nep: 2  bt: 1140  loss: 0.026702204476232113  acc:  0.90625\nep: 2  bt: 1160  loss: 0.5552485922108525  acc:  0.0625\nep: 2  bt: 1180  loss: 0.019623547792434692  acc:  0.9375\nep: 2  bt: 1200  loss: 0.010305266017499178  acc:  1.0\nep: 2  bt: 1220  loss: 0.018331749283749123  acc:  0.96875\nep: 2  bt: 1240  loss: 0.019992885382279106  acc:  0.96875\nep: 2  bt: 1260  loss: 0.008911870096040808  acc:  1.0\nep: 2  bt: 1280  loss: 0.6024246215820312  acc:  0.09375\nep: 2  bt: 1300  loss: 0.6641642943672512  acc:  0.03125\nep: 2  bt: 1320  loss: 0.5461633101753567  acc:  0.0625\nep: 2  bt: 1340  loss: 0.49553116508152173  acc:  0.15625\nep: 2  bt: 1360  loss: 0.6185058925462805  acc:  0.09375\nep: 2  bt: 1380  loss: 0.5000804403553838  acc:  0.15625\nep: 2  bt: 1400  loss: 0.6217558072960895  acc:  0.03125\nep: 2  bt: 1420  loss: 0.018005802579548046  acc:  0.96875\nep: 2  bt: 1440  loss: 0.6316432538239852  acc:  0.03125\nep: 2  bt: 1460  loss: 0.017233595899913624  acc:  0.96875\nep: 2  bt: 1480  loss: 0.011710256338119507  acc:  1.0\nep: 2  bt: 1500  loss: 0.595260827437691  acc:  0.125\nep: 2  bt: 1520  loss: 0.02257523070210996  acc:  0.96875\nep: 2  bt: 1540  loss: 0.5996355803116508  acc:  0.1875\nep: 2  bt: 1560  loss: 0.008741608780363331  acc:  1.0\nep: 2  bt: 1580  loss: 0.008147110757620438  acc:  1.0\n('<is>__________________________', '<ఈస్>__________________', '<ఇస్స్>________________')\n('<phalakaalu>__________________', '<ఫలకాలు>_______________', '<పలకాలు>_______________')\n('<teeseskovali>________________', '<తీసేసుకోవాలి>_________', '<తీసేస్కోవలలి>_________')\n('<aasanaale>___________________', '<ఆసనాలే>_______________', '<ఆసనాలే>_______________')\n('<kaagaa>______________________', '<కాగా>_________________', '<కాగా>_________________')\n('<chinnavayasuloo>_____________', '<చిన్నవయసులో>__________', '<చిన్నవాయసులో>_________')\n('<darken>______________________', '<డార్కెన్>_____________', '<దరర్కన్>______________')\n('<mokari>______________________', '<మోకరి>________________', '<మొకరిి>_______________')\n('<prishuddhamgaa>______________', '<పరిశుద్ధంగా>__________', '<ప్రిషుడ్ధంగా>_________')\n('<palakarinpulu>_______________', '<పలకరింపులు>___________', '<పలకరిింపులు>__________')\n('<bhashyam>____________________', '<భాష్యమ్>______________', '<భాష్యం>_______________')\n('<theerani>____________________', '<తీరని>________________', '<తీరని>________________')\n('<trisuulamtoe>________________', '<త్రిశూలంతో>___________', '<త్రిసులంతో>___________')\n('<kaligistoo>__________________', '<కలిగిస్తూ>____________', '<కలిగిస్తోో>___________')\n('<verla>_______________________', '<వేర్ల>________________', '<వెర్లా>_______________')\n('<prodhbalamtho>_______________', '<ప్రోద్బలంతో>__________', '<ప్రోధ్బలంతో>__________')\n('<australiatho>________________', '<ఆస్ట్రేలియాతో>________', '<ఆస్త్రాలితో>__________')\n('<directoru>___________________', '<డైరెక్టరు>____________', '<దిరెక్టోరు>___________')\n('<aasistunnavaarunna>__________', '<ఆశిస్తున్నవారున్న>____', '<ఆసిస్తున్నవాారున్న>___')\n('<shareeram>___________________', '<శరీరం>________________', '<శరరరమమ>_______________')\nep:  2  train acc: 0.54162109375  train loss: 0.3046194211320708  val acc: 0.171142578125  val loss: 0.4386110305786133\n('<domainlapai>_________________', '<డొమైన్లపై>____________', '<దోమైన్లపైై____________')\n('<apaharistunnaranna>__________', '<అపహరిస్తున్నారన్న>____', '<ఆపారిస్తున్నారన్న>____')\n('<evvaderuguno>________________', '<ఎవ్వడెరుగునో>_________', '<ఎవ్వదేరుగునో>_________')\n('<siddhistaayi>________________', '<సిద్ధిస్తాయి>_________', '<సిద్ధిస్తాయి>_________')\n('<benny>_______________________', '<బెన్ని>_______________', '<బెన్న్య్______________')\n('<chedagotta>__________________', '<చెడగొట్ట>_____________', '<చేదగగత్టట>____________')\n('<truptiparachadu>_____________', '<తృప్తిపరచదు>__________', '<త్రుత్పపరచచడు>________')\n('<minnumuttinaayi>_____________', '<మిన్నుముట్టినాయి>_____', '<మిన్నుముత్తినాయి>_____')\n('<kalusukogaligindi>___________', '<కలుసుకోగలిగింది>______', '<కలుసుకోగలిగిింది>_____')\n('<punyamul>____________________', '<పుణ్యముల్>____________', '<పున్యముల్>____________')\n('<vidipoyinanta>_______________', '<విడిపోయినంత>__________', '<విదిపోయినంటట>_________')\n('<malaysiallonu>_______________', '<మలేషియాల్లోనూ>________', '<మలల్సిలల్లోను>________')\n('<bheemaneniki>________________', '<భీమనేనికి>____________', '<భీమనననికి>____________')\n('<jyeshtaalakshmini>___________', '<జ్యేష్ఠాలక్ష్మిని>____', '<జ్యేష్తాలక్్్ిిి>_____')\n('<malek>_______________________', '<మలెక్>________________', '<మలలకక్>_______________')\n('<avutuku>_____________________', '<అవుటుకు>______________', '<అవుతుకు>______________')\n('<tornodolu>___________________', '<టోర్నోడోలు>___________', '<తోర్నోడోలు>___________')\n('<perugutunnaya>_______________', '<పెరుగుతున్నాయా>_______', '<పెరుగుతున్నాయ>________')\n('<bhagnapremikulaina>__________', '<భగ్నప్రేమికులైన>______', '<భగ్నప్రరమమికులనన>_____')\n('<thomukunentha>_______________', '<తోముకునేంత>___________', '<తోముకునేంట>___________')\nep: 3  bt: 0  loss: 0.5831779811693274  acc:  0.0625\nep: 3  bt: 20  loss: 0.010402159198470737  acc:  1.0\nep: 3  bt: 40  loss: 0.4009931813115659  acc:  0.3125\nep: 3  bt: 60  loss: 0.010070114032081936  acc:  1.0\nep: 3  bt: 80  loss: 0.010450530959212261  acc:  1.0\nep: 3  bt: 100  loss: 0.49868455140487006  acc:  0.1875\nep: 3  bt: 120  loss: 0.44504298334536346  acc:  0.09375\nep: 3  bt: 140  loss: 0.013266392376111902  acc:  1.0\nep: 3  bt: 160  loss: 0.011283306971840237  acc:  1.0\nep: 3  bt: 180  loss: 0.0193193256855011  acc:  0.9375\nep: 3  bt: 200  loss: 0.019674355569093124  acc:  0.9375\nep: 3  bt: 220  loss: 0.5558426069176715  acc:  0.25\nep: 3  bt: 240  loss: 0.6426113377446714  acc:  0.1875\nep: 3  bt: 260  loss: 0.011541637389556221  acc:  1.0\nep: 3  bt: 280  loss: 0.71905169279679  acc:  0.15625\nep: 3  bt: 300  loss: 0.4847638918005902  acc:  0.1875\nep: 3  bt: 320  loss: 0.5621835045192553  acc:  0.125\nep: 3  bt: 340  loss: 0.013431783603585285  acc:  1.0\nep: 3  bt: 360  loss: 0.8622046760890795  acc:  0.0\nep: 3  bt: 380  loss: 0.3520277272100034  acc:  0.09375\nep: 3  bt: 400  loss: 0.008025846403578053  acc:  1.0\nep: 3  bt: 420  loss: 0.014254862847535507  acc:  0.96875\nep: 3  bt: 440  loss: 0.012438067923421446  acc:  0.96875\nep: 3  bt: 460  loss: 0.4515157367872155  acc:  0.1875\nep: 3  bt: 480  loss: 0.4862408430679985  acc:  0.1875\nep: 3  bt: 500  loss: 0.0068906357754831725  acc:  1.0\nep: 3  bt: 520  loss: 0.41569407089896826  acc:  0.28125\nep: 3  bt: 540  loss: 0.3982335380885912  acc:  0.1875\nep: 3  bt: 560  loss: 0.010604084833808567  acc:  1.0\nep: 3  bt: 580  loss: 0.01996582487355108  acc:  0.9375\nep: 3  bt: 600  loss: 0.015277225038279657  acc:  0.9375\nep: 3  bt: 620  loss: 0.020969131718511166  acc:  0.96875\nep: 3  bt: 640  loss: 0.023348139679950218  acc:  0.90625\nep: 3  bt: 660  loss: 0.35496628802755603  acc:  0.1875\nep: 3  bt: 680  loss: 0.010843231626178907  acc:  0.96875\nep: 3  bt: 700  loss: 0.4440433253412661  acc:  0.1875\nep: 3  bt: 720  loss: 0.009490874150524969  acc:  1.0\nep: 3  bt: 740  loss: 0.009650737047195435  acc:  1.0\nep: 3  bt: 760  loss: 0.030261604682258938  acc:  0.9375\nep: 3  bt: 780  loss: 0.00632613638172979  acc:  1.0\nep: 3  bt: 800  loss: 0.009830590175545734  acc:  0.96875\nep: 3  bt: 820  loss: 0.010464228365732275  acc:  0.96875\nep: 3  bt: 840  loss: 0.41951216822085174  acc:  0.21875\nep: 3  bt: 860  loss: 0.012901131225668865  acc:  0.96875\nep: 3  bt: 880  loss: 0.005076307965361554  acc:  1.0\nep: 3  bt: 900  loss: 0.012627701396527498  acc:  0.96875\nep: 3  bt: 920  loss: 0.009771785658338795  acc:  1.0\nep: 3  bt: 940  loss: 0.00909689522307852  acc:  0.96875\nep: 3  bt: 960  loss: 0.006087745661320893  acc:  1.0\nep: 3  bt: 980  loss: 0.012304930583290432  acc:  0.96875\nep: 3  bt: 1000  loss: 0.011094602553740791  acc:  0.96875\nep: 3  bt: 1020  loss: 0.30283268638279126  acc:  0.375\nep: 3  bt: 1040  loss: 0.02412925336671912  acc:  0.9375\nep: 3  bt: 1060  loss: 0.39778713558031165  acc:  0.21875\nep: 3  bt: 1080  loss: 0.00883685246757839  acc:  1.0\nep: 3  bt: 1100  loss: 0.010935430941374405  acc:  0.96875\nep: 3  bt: 1120  loss: 0.00661262351533641  acc:  1.0\nep: 3  bt: 1140  loss: 0.009876260290975157  acc:  0.96875\nep: 3  bt: 1160  loss: 0.3832649562669837  acc:  0.25\nep: 3  bt: 1180  loss: 0.006950365460437277  acc:  1.0\nep: 3  bt: 1200  loss: 0.006433713695277338  acc:  0.96875\nep: 3  bt: 1220  loss: 0.6602563858032227  acc:  0.25\nep: 3  bt: 1240  loss: 0.007574330205502717  acc:  1.0\nep: 3  bt: 1260  loss: 0.3070952995963719  acc:  0.375\nep: 3  bt: 1280  loss: 0.4634471976238748  acc:  0.25\nep: 3  bt: 1300  loss: 0.008712341604025467  acc:  0.96875\nep: 3  bt: 1320  loss: 0.0065711568231168  acc:  1.0\nep: 3  bt: 1340  loss: 0.011034822982290516  acc:  1.0\nep: 3  bt: 1360  loss: 0.45574466041896655  acc:  0.1875\nep: 3  bt: 1380  loss: 0.013736074385435684  acc:  1.0\nep: 3  bt: 1400  loss: 0.011641577534053637  acc:  1.0\nep: 3  bt: 1420  loss: 0.6811466217041016  acc:  0.0625\nep: 3  bt: 1440  loss: 0.4226395150889521  acc:  0.21875\nep: 3  bt: 1460  loss: 0.005959330693535183  acc:  1.0\nep: 3  bt: 1480  loss: 0.009360564143761345  acc:  0.96875\nep: 3  bt: 1500  loss: 0.44898128509521484  acc:  0.28125\nep: 3  bt: 1520  loss: 0.008208603314731432  acc:  0.96875\n","output_type":"stream"}]},{"cell_type":"code","source":"# torch.cuda.memory_summary(device=None, abbreviated=False)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}