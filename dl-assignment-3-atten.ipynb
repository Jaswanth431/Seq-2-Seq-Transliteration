{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jaswanth431/dl-assignment-3-atten?scriptVersionId=177668108\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"fb3147be","metadata":{"_cell_guid":"f4ce1158-c562-476f-8f7d-0325f0b787c4","_uuid":"549ce73a-e98b-42bd-a1d8-2cab5faae7d5","collapsed":false,"execution":{"iopub.execute_input":"2024-05-14T18:35:06.785594Z","iopub.status.busy":"2024-05-14T18:35:06.785211Z","iopub.status.idle":"2024-05-14T18:35:12.070144Z","shell.execute_reply":"2024-05-14T18:35:12.069359Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":5.294239,"end_time":"2024-05-14T18:35:12.072437","exception":false,"start_time":"2024-05-14T18:35:06.778198","status":"completed"},"tags":[]},"outputs":[],"source":["import torch\n","from torch import nn\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","import copy\n","from torch.utils.data import Dataset, DataLoader\n","import gc\n","import random\n","import wandb"]},{"cell_type":"code","execution_count":2,"id":"2b6655f6","metadata":{"execution":{"iopub.execute_input":"2024-05-14T18:35:12.084709Z","iopub.status.busy":"2024-05-14T18:35:12.084429Z","iopub.status.idle":"2024-05-14T18:35:13.342795Z","shell.execute_reply":"2024-05-14T18:35:13.341867Z"},"papermill":{"duration":1.266683,"end_time":"2024-05-14T18:35:13.344889","exception":false,"start_time":"2024-05-14T18:35:12.078206","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login(key=\"62cfafb7157dfba7fdd6132ac9d757ccd913aaaf\")"]},{"cell_type":"code","execution_count":3,"id":"056fd0fe","metadata":{"_cell_guid":"c7dcb64a-f20a-4fa2-8b31-9801c3a762b8","_uuid":"b6363452-eb4f-4705-aa3f-fdec65f8e9db","collapsed":false,"execution":{"iopub.execute_input":"2024-05-14T18:35:13.357267Z","iopub.status.busy":"2024-05-14T18:35:13.356919Z","iopub.status.idle":"2024-05-14T18:35:13.586022Z","shell.execute_reply":"2024-05-14T18:35:13.585123Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.23787,"end_time":"2024-05-14T18:35:13.588242","exception":false,"start_time":"2024-05-14T18:35:13.350372","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","END_TOKEN = '>'\n","START_TOKEN = '<'\n","PAD_TOKEN = '_'\n","TEACHER_FORCING_RATIO = 0.5\n","\n","train_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_train.csv\"\n","test_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_test.csv\"\n","val_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_valid.csv\"\n","\n","train_df = pd.read_csv(train_csv, header=None)\n","test_df = pd.read_csv(test_csv, header=None)\n","val_df = pd.read_csv(val_csv, header=None)\n","train_source, train_target = train_df[0].to_numpy(), train_df[1].to_numpy();\n","val_source, val_target = val_df[0].to_numpy(), val_df[1].to_numpy();"]},{"cell_type":"code","execution_count":4,"id":"5c3b3d63","metadata":{"_cell_guid":"fb22b602-e1fc-496b-93dc-f4d3178870d5","_uuid":"3708abf3-c2df-4e8d-8cfe-af3faab7bc53","collapsed":false,"execution":{"iopub.execute_input":"2024-05-14T18:35:13.600716Z","iopub.status.busy":"2024-05-14T18:35:13.600408Z","iopub.status.idle":"2024-05-14T18:35:13.616109Z","shell.execute_reply":"2024-05-14T18:35:13.615256Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.024282,"end_time":"2024-05-14T18:35:13.618045","exception":false,"start_time":"2024-05-14T18:35:13.593763","status":"completed"},"tags":[]},"outputs":[],"source":["def add_padding(source_data, MAX_LENGTH):\n","    padded_source_strings = []\n","    for i in range(len(source_data)):\n","        source_str =START_TOKEN+ source_data[i] + END_TOKEN\n","        # Truncate or pad source sequence\n","        source_str = source_str[:MAX_LENGTH]\n","        source_str += PAD_TOKEN * (MAX_LENGTH - len(source_str))\n","\n","        padded_source_strings.append(source_str)\n","        \n","    return padded_source_strings\n","\n","\n","def generate_string_to_sequence(source_data, source_char_index_dict):\n","    source_sequences = []\n","    for i in range(len(source_data)):\n","        source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","    source_sequences = pad_sequence(source_sequences, batch_first=True, padding_value=2)\n","    return source_sequences\n","\n","\n","def get_chars(str, char_index_dict):\n","    chars_indexes = []\n","    for ch in str:\n","        chars_indexes.append(char_index_dict[ch])\n","    return torch.tensor(chars_indexes, device=device)\n","\n","\n","def preprocess_data(source_data, target_data):\n","    data = {\n","        \"source_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n","        \"target_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n","        \"source_char_index\": {START_TOKEN: 0, END_TOKEN:1, PAD_TOKEN:2},\n","        \"source_index_char\": {0:START_TOKEN, 1: END_TOKEN, 2:PAD_TOKEN},\n","        \"target_char_index\": {START_TOKEN: 0, END_TOKEN:1, PAD_TOKEN:2},\n","        \"target_index_char\": {0:START_TOKEN, 1: END_TOKEN, 2:PAD_TOKEN},\n","        \"source_len\": 3,\n","        \"target_len\": 3,\n","        \"source_data\": source_data,\n","        \"target_data\": target_data,\n","        \"source_data_seq\": [],\n","        \"target_data_seq\": []\n","    }\n","    \n","    data[\"INPUT_MAX_LENGTH\"] = max(len(string) for string in source_data) +2\n","    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data)+2\n","\n","    \n","    padded_source_strings=add_padding(source_data, data[\"INPUT_MAX_LENGTH\"])\n","    padded_target_strings = add_padding(target_data, data[\"OUTPUT_MAX_LENGTH\"])\n","    \n","    for i in range(len(padded_source_strings)):\n","        for c in padded_source_strings[i]:\n","            if data[\"source_char_index\"].get(c) is None:\n","                data[\"source_chars\"].append(c)\n","                idx = len(data[\"source_chars\"]) - 1\n","                data[\"source_char_index\"][c] = idx\n","                data[\"source_index_char\"][idx] = c\n","        for c in padded_target_strings[i]:\n","            if data[\"target_char_index\"].get(c) is None:\n","                data[\"target_chars\"].append(c)\n","                idx = len(data[\"target_chars\"]) - 1\n","                data[\"target_char_index\"][c] = idx\n","                data[\"target_index_char\"][idx] = c\n","\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","    data['target_data_seq'] = generate_string_to_sequence(padded_target_strings,  data['target_char_index'])\n","#     print(data[\"source_data\"][0])\n","#     print(data[\"source_data_seq\"][0])\n","#     print(data[\"target_data\"][0])\n","#     print(data[\"target_data_seq\"][0])\n","\n","    \n","    data[\"source_len\"] = len(data[\"source_chars\"])\n","    data[\"target_len\"] = len(data[\"target_chars\"])\n","    \n","    return data\n","\n","# data = preprocess_data(copy.copy(train_source), copy.copy(train_target))"]},{"cell_type":"code","execution_count":5,"id":"60d1955d","metadata":{"_cell_guid":"52035e98-753c-4cd0-8dec-2cec35aab863","_uuid":"58e2b696-e336-4cb3-b6e6-d14eac238c96","execution":{"iopub.execute_input":"2024-05-14T18:35:13.630151Z","iopub.status.busy":"2024-05-14T18:35:13.629845Z","iopub.status.idle":"2024-05-14T18:35:13.658441Z","shell.execute_reply":"2024-05-14T18:35:13.657542Z"},"papermill":{"duration":0.037097,"end_time":"2024-05-14T18:35:13.660631","exception":false,"start_time":"2024-05-14T18:35:13.623534","status":"completed"},"tags":[]},"outputs":[],"source":["def get_cell_type(cell_type):\n","    if(cell_type == \"RNN\"):\n","        return nn.RNN\n","    elif(cell_type == \"LSTM\"):\n","        return nn.LSTM\n","    elif(cell_type == \"GRU\"):\n","        return nn.GRU\n","    else:\n","        print(\"Specify correct cell type\")\n","        \n","class Attention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(Attention, self).__init__()\n","        self.Wa = nn.Linear(hidden_size, hidden_size)\n","        self.Ua = nn.Linear(hidden_size, hidden_size)\n","        self.Va = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, query, keys):\n","        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n","        scores = scores.squeeze().unsqueeze(1)\n","        weights = F.softmax(scores, dim=0)\n","        weights = weights.permute(2,1,0)\n","        keys = keys.permute(1,0,2)\n","        context = torch.bmm(weights, keys)\n","        return context, weights\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, h_params, data, device ):\n","        super(Encoder, self).__init__()\n","        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embd_dim\"])\n","        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],num_layers=h_params[\"number_of_layers\"], batch_first=True)\n","        self.device=device\n","        self.h_params = h_params\n","        self.data = data\n","    def forward(self, input , encoder_curr_state):\n","        input_length = self.data[\"INPUT_MAX_LENGTH\"]\n","        batch_size = self.h_params[\"batch_size\"]\n","        hidden_neurons = self.h_params[\"hidden_layer_neurons\"]\n","        layers = self.h_params[\"number_of_layers\"]\n","        encoder_states  = torch.zeros(input_length, layers, batch_size, hidden_neurons, device=device )\n","        for i in range(input_length):\n","            current_input = input[:, i].view(batch_size,1)\n","            _, encoder_curr_state = self.forward_step(current_input, encoder_curr_state)\n","            encoder_states[i] = encoder_curr_state\n","        return encoder_states\n","    \n","    def forward_step(self, current_input, prev_state):\n","        embd_input = self.embedding(current_input)\n","        output, prev_state = self.cell(embd_input, prev_state)\n","        return output, prev_state\n","        \n","    def getInitialState(self):\n","        return torch.zeros(self.h_params[\"number_of_layers\"],self.h_params[\"batch_size\"],self.h_params[\"hidden_layer_neurons\"], device=self.device)\n","\n","    \n","class Decoder(nn.Module):\n","    def __init__(self, h_params, data,device):\n","        super(Decoder, self).__init__()\n","        self.attention = Attention(h_params[\"hidden_layer_neurons\"]).to(device)\n","        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embd_dim\"])\n","        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"hidden_layer_neurons\"] +h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],num_layers=h_params[\"number_of_layers\"], batch_first=True)\n","        self.fc = nn.Linear(h_params[\"hidden_layer_neurons\"], data[\"target_len\"])\n","        self.softmax = nn.LogSoftmax(dim=2)\n","        self.h_params = h_params\n","        self.data = data\n","        self.device = device\n","\n","    def forward(self, decoder_current_state, encoder_final_layers, target_batch, loss_fn, teacher_forcing_enabled=True):\n","#         print(\"Teacher forcing:\", teacher_forcing_enabled)\n","        batch_size = self.h_params[\"batch_size\"]\n","        decoder_current_input = torch.full((batch_size,1),self.data[\"target_char_index\"][START_TOKEN], device=self.device)\n","        embd_input = self.embedding(decoder_current_input)\n","        curr_embd = F.relu(embd_input)\n","        decoder_actual_output = []\n","        attentions = []\n","        loss = 0\n","        \n","        use_teacher_forcing = False\n","        if(teacher_forcing_enabled):\n","            use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n","        for i in range(self.data[\"OUTPUT_MAX_LENGTH\"]):\n","            decoder_output, decoder_current_state, attn_weights = self.forward_step(decoder_current_input, decoder_current_state, encoder_final_layers)\n","            attentions.append(attn_weights)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_current_input = topi.squeeze().detach()\n","            decoder_actual_output.append(decoder_current_input)\n","\n","            if(target_batch==None):\n","                decoder_current_input = decoder_current_input.view(self.h_params[\"batch_size\"], 1)\n","            else:\n","                curr_target_chars = target_batch[:, i]\n","                if(i<self.data[\"OUTPUT_MAX_LENGTH\"]-1):\n","                    if use_teacher_forcing:\n","                        decoder_current_input = target_batch[:, i+1].view(self.h_params[\"batch_size\"], 1)\n","                    else:\n","                        decoder_current_input = decoder_current_input.view(self.h_params[\"batch_size\"], 1)\n","                decoder_output = decoder_output[:, -1, :]\n","                loss+=(loss_fn(decoder_output, curr_target_chars))\n","\n","        decoder_actual_output = torch.cat(decoder_actual_output,dim=0).view(self.data[\"OUTPUT_MAX_LENGTH\"], self.h_params[\"batch_size\"]).transpose(0,1)\n","\n","        correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n","        return decoder_actual_output, attentions, loss, correct\n","    \n","    def forward_step(self, current_input, prev_state, encoder_final_layers):\n","        embd_input = self.embedding(current_input)\n","        context , attn_weights = self.attention(prev_state[-1,:,:], encoder_final_layers)\n","        curr_embd = F.relu(embd_input)\n","        input_gru = torch.cat((curr_embd, context), dim=2)\n","        output, prev_state = self.cell(input_gru, prev_state)\n","        output = self.softmax(self.fc(output))\n","        return output, prev_state, attn_weights  "]},{"cell_type":"code","execution_count":6,"id":"211011d4","metadata":{"execution":{"iopub.execute_input":"2024-05-14T18:35:13.672571Z","iopub.status.busy":"2024-05-14T18:35:13.672286Z","iopub.status.idle":"2024-05-14T18:35:13.677782Z","shell.execute_reply":"2024-05-14T18:35:13.676924Z"},"papermill":{"duration":0.013626,"end_time":"2024-05-14T18:35:13.679755","exception":false,"start_time":"2024-05-14T18:35:13.666129","status":"completed"},"tags":[]},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, data):\n","        self.source_data_seq = data[0]\n","        self.target_data_seq = data[1]\n","    \n","    def __len__(self):\n","        return len(self.source_data_seq)\n","    \n","    def __getitem__(self, idx):\n","        source_data = self.source_data_seq[idx]\n","        target_data = self.target_data_seq[idx]\n","        return source_data, target_data\n"]},{"cell_type":"code","execution_count":7,"id":"d95b96e0","metadata":{"execution":{"iopub.execute_input":"2024-05-14T18:35:13.691609Z","iopub.status.busy":"2024-05-14T18:35:13.691353Z","iopub.status.idle":"2024-05-14T18:35:13.699015Z","shell.execute_reply":"2024-05-14T18:35:13.698249Z"},"papermill":{"duration":0.015673,"end_time":"2024-05-14T18:35:13.700824","exception":false,"start_time":"2024-05-14T18:35:13.685151","status":"completed"},"tags":[]},"outputs":[],"source":["   \n","def evaluate(encoder, decoder, data, dataloader, device, h_params, loss_fn, use_teacher_forcing = False):\n","    correct_predictions = 0\n","    total_loss = 0\n","    total_predictions = len(dataloader.dataset)\n","    number_of_batches = len(dataloader)\n","    for batch_num, (source_batch, target_batch) in enumerate(dataloader):\n","        encoder_initial_state = encoder.getInitialState()\n","        encoder_states = encoder(source_batch,encoder_initial_state)\n","\n","        decoder_current_state = encoder_states[-1, :, :, :]\n","        encoder_final_layer_states = encoder_states[:, -1, :, :]\n","\n","        loss = 0\n","        correct = 0\n","\n","        decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n","        if(batch_num == 0):\n","                for j in range(20):\n","                    print(make_strings(data,source_batch[j],target_batch[j],decoder_output[j]))\n","      \n","        correct_predictions+=correct\n","        total_loss +=loss\n","    \n","    accuracy = correct_predictions / total_predictions\n","    total_loss /= number_of_batches\n","    \n","    return accuracy, total_loss\n"]},{"cell_type":"code","execution_count":8,"id":"9846964f","metadata":{"_cell_guid":"0dc20b89-db7b-4230-a7ec-e76d1896adee","_uuid":"c743c5b1-1106-4388-a594-e4e301225bad","execution":{"iopub.execute_input":"2024-05-14T18:35:13.712837Z","iopub.status.busy":"2024-05-14T18:35:13.712438Z","iopub.status.idle":"2024-05-14T18:35:13.726326Z","shell.execute_reply":"2024-05-14T18:35:13.725525Z"},"papermill":{"duration":0.022003,"end_time":"2024-05-14T18:35:13.728164","exception":false,"start_time":"2024-05-14T18:35:13.706161","status":"completed"},"scrolled":true,"tags":[]},"outputs":[],"source":["def make_strings(data, source, target, output):\n","    source_string = \"\"\n","    target_string = \"\"\n","    output_string = \"\"\n","#     print(output)\n","    for i in source:\n","#         print(i.item())\n","        source_string+=(data['source_index_char'][i.item()])\n","    for i in target:\n","        target_string+=(data['target_index_char'][i.item()])\n","    for i in output:\n","        output_string+=(data['target_index_char'][i.item()])\n","    return source_string, target_string, output_string\n","                        \n","\n","def train_loop(encoder, decoder,h_params, data, data_loader, device, val_dataloader, use_teacher_forcing=True):\n","    \n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n","    \n","    loss_fn = nn.NLLLoss()\n","    \n","    total_predictions = len(data_loader.dataset)\n","    total_batches = len(data_loader)\n","    \n","    for ep in range(h_params[\"epochs\"]):\n","        total_correct = 0\n","        total_loss = 0\n","        for batch_num, (source_batch, target_batch) in enumerate(data_loader):\n","#             if(batch_num>0):\n","#                 break\n","            encoder_initial_state = encoder.getInitialState()\n","            encoder_states = encoder(source_batch,encoder_initial_state)\n","            \n","            decoder_current_state = encoder_states[-1, :, :, :]\n","            encoder_final_layer_states = encoder_states[:, -1, :, :]\n","            \n","            \n","            loss = 0\n","            correct = 0\n","            \n","            decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n","            total_correct +=correct\n","            total_loss += loss.item()/data[\"OUTPUT_MAX_LENGTH\"]\n","            if(batch_num == 0):\n","                    for j in range(20):\n","                        print(make_strings(data,source_batch[j],target_batch[j],decoder_output[j]))\n","            if(batch_num%20 == 0):\n","                print(\"ep:\", ep, \" bt:\", batch_num, \" loss:\", loss.item()/data[\"OUTPUT_MAX_LENGTH\"], \" acc: \", correct/h_params[\"batch_size\"])\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            \n","        train_acc = total_correct/total_predictions\n","        train_loss = total_loss/total_batches\n","        val_acc, val_loss = evaluate(encoder, decoder, data, val_dataloader,device, h_params, loss_fn, False)\n","        print(\"ep: \", ep, \" train acc:\", train_acc, \" train loss:\", train_loss, \" val acc:\", val_acc, \" val loss:\", val_loss.item()/data[\"OUTPUT_MAX_LENGTH\"])\n","        wandb.log({\"train_accuracy\":train_acc, \"train_loss\":train_loss, \"val_accuracy\":val_acc, \"val_loss\":val_loss, \"epoch\":ep})\n","\n"]},{"cell_type":"code","execution_count":9,"id":"75b41ea2","metadata":{"execution":{"iopub.execute_input":"2024-05-14T18:35:13.739997Z","iopub.status.busy":"2024-05-14T18:35:13.739747Z","iopub.status.idle":"2024-05-14T18:35:13.746975Z","shell.execute_reply":"2024-05-14T18:35:13.746104Z"},"papermill":{"duration":0.015229,"end_time":"2024-05-14T18:35:13.748825","exception":false,"start_time":"2024-05-14T18:35:13.733596","status":"completed"},"tags":[]},"outputs":[],"source":["# h_params={\n","#     \"char_embd_dim\" : 256, \n","#     \"hidden_layer_neurons\":256,\n","#     \"batch_size\":32,\n","#     \"number_of_layers\":1,\n","#     \"learning_rate\":0.0001,\n","#     \"epochs\":20,\n","#     \"cell_type\":\"GRU\",\n","#     \"dropout\":0,\n","#     \"optimizer\":\"adam\"\n","# }\n","\n","def prepare_dataloaders(train_source, train_target, val_source, val_target, h_params):\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","    training_data = [data[\"source_data_seq\"], data['target_data_seq']]\n","    train_dataset = MyDataset(training_data)\n","    train_dataloader = DataLoader(train_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n","\n","    #prepare validation data\n","    val_padded_source_strings=add_padding(val_source, data[\"INPUT_MAX_LENGTH\"])\n","    val_padded_target_strings = add_padding(val_target, data[\"OUTPUT_MAX_LENGTH\"])\n","    val_source_sequences = generate_string_to_sequence(val_padded_source_strings,  data['source_char_index'])\n","    val_target_sequences = generate_string_to_sequence(val_padded_target_strings,  data['target_char_index'])\n","    validation_data = [val_source_sequences, val_target_sequences]\n","    val_dataset = MyDataset(validation_data)\n","    val_dataloader = DataLoader(val_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n","    return train_dataloader, val_dataloader, data\n"]},{"cell_type":"code","execution_count":10,"id":"b8edfc4d","metadata":{"execution":{"iopub.execute_input":"2024-05-14T18:35:13.760563Z","iopub.status.busy":"2024-05-14T18:35:13.760013Z","iopub.status.idle":"2024-05-14T18:35:13.765271Z","shell.execute_reply":"2024-05-14T18:35:13.764464Z"},"papermill":{"duration":0.01309,"end_time":"2024-05-14T18:35:13.767132","exception":false,"start_time":"2024-05-14T18:35:13.754042","status":"completed"},"tags":[]},"outputs":[],"source":["def train(h_params, data, device, data_loader, val_dataloader, use_teacher_forcing=True):\n","    encoder = Encoder(h_params, data, device).to(device)\n","    decoder = Decoder(h_params, data, device).to(device)\n","    train_loop(encoder, decoder,h_params, data, data_loader,device, val_dataloader, use_teacher_forcing)\n","    encoder=None\n","    decoder=None\n","    gc.collect()\n","    torch.cuda.empty_cache() \n"]},{"cell_type":"code","execution_count":11,"id":"aa4f983a","metadata":{"execution":{"iopub.execute_input":"2024-05-14T18:35:13.77883Z","iopub.status.busy":"2024-05-14T18:35:13.778568Z","iopub.status.idle":"2024-05-14T18:35:13.782393Z","shell.execute_reply":"2024-05-14T18:35:13.781549Z"},"papermill":{"duration":0.011892,"end_time":"2024-05-14T18:35:13.784262","exception":false,"start_time":"2024-05-14T18:35:13.77237","status":"completed"},"tags":[]},"outputs":[],"source":["# config = h_params\n","# # run = wandb.init(project=\"DL Assignment 3 With Attention\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config)\n","# train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, h_params)\n","# train(h_params, data, device, train_dataloader, val_dataloader, True)"]},{"cell_type":"code","execution_count":12,"id":"1b881afd","metadata":{"execution":{"iopub.execute_input":"2024-05-14T18:35:13.795788Z","iopub.status.busy":"2024-05-14T18:35:13.795521Z","iopub.status.idle":"2024-05-14T18:35:14.408889Z","shell.execute_reply":"2024-05-14T18:35:14.407782Z"},"papermill":{"duration":0.621692,"end_time":"2024-05-14T18:35:14.411215","exception":false,"start_time":"2024-05-14T18:35:13.789523","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: nvimt6qw\n","Sweep URL: https://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/nvimt6qw\n"]}],"source":["#Run this cell to run a sweep with appropriate parameters\n","sweep_params = {\n","    'method' : 'bayes',\n","    'name'   : 'DL Assignment 3 With Attention',\n","    'metric' : {\n","        'goal' : 'maximize',\n","        'name' : 'val_accuracy',\n","    },\n","    'parameters' : {\n","        'epochs':{'values' : [15, 20]},\n","        'learning_rate':{'values' : [0.001, 0.0001]},\n","        'batch_size':{'values':[32,64, 128]},\n","        'char_embd_dim':{'values' : [64, 128, 256] } ,\n","        'number_of_layers':{'values' : [1,2,3,4]},\n","        'optimizer':{'values':['nadam','adam']},\n","        'cell_type':{'values' : [\"RNN\",\"LSTM\", \"GRU\"]},\n","        'hidden_layer_neurons':{'values': [ 128, 256, 512]},\n","        'dropout':{'values': [0,0.2, 0.3]}\n","    }\n","}\n","\n","sweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 3 With Attention\")\n","def main():\n","    wandb.init(project=\"DL Assignment 3\" )\n","    config = wandb.config\n","    with wandb.init(project=\"DL Assignment 3\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config):\n","        train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","        train(config, data, device, train_dataloader, val_dataloader, True)\n"]},{"cell_type":"code","execution_count":13,"id":"f0fa4b2d","metadata":{"execution":{"iopub.execute_input":"2024-05-14T18:35:14.423886Z","iopub.status.busy":"2024-05-14T18:35:14.423613Z","iopub.status.idle":"2024-05-14T20:00:27.176368Z","shell.execute_reply":"2024-05-14T20:00:27.175518Z"},"papermill":{"duration":5112.761515,"end_time":"2024-05-14T20:00:27.178434","exception":false,"start_time":"2024-05-14T18:35:14.416919","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 94mujjqa with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjaswanth431\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_183515-94mujjqa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfine-sweep-31\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/94mujjqa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"name":"stdout","output_type":"stream","text":["('<doddiloo>____________________', '<దొడ్డిలో>_____________', 'ధధధధధధధధధధధధధధధధధధధధధధధ')\n","('<cranbroke>___________________', '<క్రాన్బ్రోక్>_________', 'ధధధధధధధధధధధధధధధధధధధధధధధ')\n","('<vaarinicchataku>_____________', '<వారినిచ్చటకు>_________', 'వధధధధధధధధధధధధధధధధధధధధధధ')\n","('<kungadame>___________________', '<కుంగడమే>______________', 'ధధధధధధధధధధధధధధధధధధధధధధధ')\n","('<kannayyagadu>________________', '<కన్నయ్యగాడు>__________', 'జగఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞ')\n","('<panichesthegaani>____________', '<పనిచేస్తేగాని>________', 'వధధధధధధధధధధధధధధధధధధధధధధ')\n","('<dukaanamku>__________________', '<దుకాణంకు>_____________', 'జగఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞ')\n","('<ismaily>_____________________', '<ఇస్మాయిలీ>____________', 'జీీీీీీీీీీీీీీీీీీీీీీ')\n","('<aritaakuluu>_________________', '<అరిటాకులూ>____________', 'జీీీహహచఱఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞ')\n","('<naalgusaarlu>________________', '<నాల్గుసార్లు>_________', 'ధధధధధధధధధధధధధధధధధధధధధధధ')\n","('<pyachlugaa>__________________', '<ప్యాచ్లుగా>___________', 'వధధధధధధధధధధధధధధధధధధధధధధ')\n","('<prastaavimstuu>______________', '<ప్రస్తావింస్తూ>_______', 'జీీీీీీీీీీీీీీీీీీీీీీ')\n","('<pinjarlu>____________________', '<పింజర్లు>_____________', 'వధధధధధధధధధధధధధధధధధధధధధధ')\n","('<velluchuu>___________________', '<వెళ్లుచూ>_____________', 'వధధధధధధధధధధధధధధధధధధధధధధ')\n","('<krupaavaraalu>_______________', '<కృపావరాలు>____________', 'వధధధధధధధధధధధధధధధధధధధధధధ')\n","('<paryaatakaramgaa>____________', '<పర్యాటకరంగా>__________', 'జఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞఞ')\n","('<hancha>______________________', '<హంచ>__________________', 'వధధధధధధధధధధధధధధధధధధధధధధ')\n","('<parrikar>____________________', '<పారికర్>______________', 'వధధధధధధధధధధధధధధధధధధధధధధ')\n","('<gulivindada>_________________', '<గులివిందాడ>___________', 'జీీీీీీీీీీీీీీీీీీీీీీ')\n","('<jorden>______________________', '<జోర్డెన్>_____________', 'వధధధధధధధధధధధధధధధధధధధధధధ')\n","ep: 0  bt: 0  loss: 4.244293876316236  acc:  0.0\n","ep: 0  bt: 20  loss: 2.0292713331139605  acc:  0.0\n","ep: 0  bt: 40  loss: 1.8975589586340862  acc:  0.0\n","ep: 0  bt: 60  loss: 1.951161757759426  acc:  0.0\n","ep: 0  bt: 80  loss: 1.1574556931205418  acc:  0.0\n","ep: 0  bt: 100  loss: 1.1788829305897588  acc:  0.0\n","ep: 0  bt: 120  loss: 1.842373225999915  acc:  0.0\n","ep: 0  bt: 140  loss: 1.844764046047045  acc:  0.0\n","ep: 0  bt: 160  loss: 0.6772474206012228  acc:  0.1875\n","ep: 0  bt: 180  loss: 0.5797573587168818  acc:  0.296875\n","ep: 0  bt: 200  loss: 0.40070168868355127  acc:  0.375\n","ep: 0  bt: 220  loss: 1.7966310252314028  acc:  0.0\n","ep: 0  bt: 240  loss: 0.2631262074346128  acc:  0.65625\n","ep: 0  bt: 260  loss: 2.2272048618482505  acc:  0.0\n","ep: 0  bt: 280  loss: 0.2412785239841627  acc:  0.734375\n","ep: 0  bt: 300  loss: 0.2206857100777004  acc:  0.75\n","ep: 0  bt: 320  loss: 0.18909124706102454  acc:  0.84375\n","ep: 0  bt: 340  loss: 0.1479957103729248  acc:  0.90625\n","ep: 0  bt: 360  loss: 1.663519817849864  acc:  0.0\n","ep: 0  bt: 380  loss: 0.13273841401805048  acc:  0.90625\n","ep: 0  bt: 400  loss: 1.732872838559358  acc:  0.0\n","ep: 0  bt: 420  loss: 1.713832191798998  acc:  0.0\n","ep: 0  bt: 440  loss: 0.14561039468516473  acc:  0.859375\n","ep: 0  bt: 460  loss: 0.13278442880381708  acc:  0.96875\n","ep: 0  bt: 480  loss: 1.6588625700577446  acc:  0.0\n","ep: 0  bt: 500  loss: 1.6326851222826086  acc:  0.0\n","ep: 0  bt: 520  loss: 1.5438093102496604  acc:  0.0\n","ep: 0  bt: 540  loss: 1.4739779596743376  acc:  0.0\n","ep: 0  bt: 560  loss: 1.5155288032863452  acc:  0.0\n","ep: 0  bt: 580  loss: 0.12435917232347571  acc:  0.9375\n","ep: 0  bt: 600  loss: 0.08357582403265912  acc:  0.9375\n","ep: 0  bt: 620  loss: 1.2536260356073794  acc:  0.0\n","ep: 0  bt: 640  loss: 0.05618383573449176  acc:  1.0\n","ep: 0  bt: 660  loss: 0.04751804600591245  acc:  0.96875\n","ep: 0  bt: 680  loss: 0.05635545046433159  acc:  0.96875\n","ep: 0  bt: 700  loss: 1.0476991404657778  acc:  0.0\n","ep: 0  bt: 720  loss: 0.04642210317694623  acc:  0.9375\n","ep: 0  bt: 740  loss: 1.0070581021516218  acc:  0.0\n","ep: 0  bt: 760  loss: 0.8306520710820737  acc:  0.0\n","ep: 0  bt: 780  loss: 1.2796579443890115  acc:  0.0\n","('<dharake>_____________________', '<ధరకే>_________________', '<దర్ే>>________________')\n","('<seethaaraamireddini>_________', '<సీతారామిరెడ్డిని>_____', '<సీతాాారరదదదిిిిి______')\n","('<desaniki>____________________', '<దేశానికి>_____________', '<దెసనిిి>______________')\n","('<endina>______________________', '<ఎండిన>________________', '<ఎనదినా>_______________')\n","('<khandincadu>_________________', '<ఖండించాడు>____________', '<కందింండడ>_____________')\n","('<viyogam>_____________________', '<వియోగం>_______________', '<వియయగమ>_______________')\n","('<teelchaaru>__________________', '<తేల్చారు>_____________', '<తీల్రరరు______________')\n","('<shuter>______________________', '<షట్టర్>_______________', '<శుత్ర్>_______________')\n","('<pakshilu>____________________', '<పక్షిలు>______________', '<పకషషషలల>______________')\n","('<digajaripotonda>_____________', '<దిగజారిపోతోందా>_______', '<దిగాాపపోోోోో>_________')\n","('<giltunagalu>_________________', '<గిల్టునగలు>___________', '<గిల్తుుుగగ>___________')\n","('<roopanlone>__________________', '<రూపంలోనో>_____________', '<రోప్లోనన>_____________')\n","('<aasamanjasam>________________', '<ఆసమంజసం>______________', '<అసమంససమ>______________')\n","('<indooneeshiyaa>______________', '<ఇండోనేషియా>___________', '<ఇనదోోేేియయ>___________')\n","('<meedhiki>____________________', '<మీదికి>_______________', '<మెధికి>_______________')\n","('<ekaadasine>__________________', '<ఏకాదశినే>_____________', '<ఎకాడిిి>______________')\n","('<nirviiryam>__________________', '<నిర్వీర్యం>___________', '<నిర్విర్మమ>___________')\n","('<sikshanhanu>_________________', '<శిక్షణను>_____________', '<సిక్షాను>_____________')\n","('<dweepamu>____________________', '<ద్వీపము>______________', '<ద్వేపము>______________')\n","('<sainikudu>___________________', '<సైనికుడు>_____________', '<సింికుుు>_____________')\n","ep:  0  train acc: 0.334140625  train loss: 1.0340591316281458  val acc: 0.02294921875  val loss: 0.6897680033808169\n","('<kannagadda>__________________', '<కన్నగడ్డ>_____________', '<కన్నగడ్డ>_____________')\n","('<vahinchadamaa>_______________', '<వహించడమా>_____________', '<వహించడమా>_____________')\n","('<rakeeyamgaa>_________________', '<రాకీయంగా>_____________', '<రాకీయంగా>_____________')\n","('<pampinan>____________________', '<పంపినన్>______________', '<పంపినన్>______________')\n","('<chaduvukoradani>_____________', '<చదువుకోరాదని>_________', '<చదువుకోరాదని>_________')\n","('<asatyaropanalu>______________', '<అసత్యారోపణలు>_________', '<అసత్యారోపణలు>_________')\n","('<akasmaatugaa>________________', '<అకస్మాతుగా>___________', '<అకస్మాతుగా>___________')\n","('<nickelodeon>_________________', '<నికెలోడియాన్>_________', '<నికెలోడియాన్>_________')\n","('<barnishing>__________________', '<బార్నిషింగ్>__________', '<బార్నిషింగ్>__________')\n","('<kantidoshaalu>_______________', '<కంటిదోషాలు>___________', '<కంటిదోషాలు>___________')\n","('<vacharem>____________________', '<వచ్చారేం>_____________', '<వచ్చారేం>_____________')\n","('<girruna>_____________________', '<గిర్రున>______________', '<గిర్రున>______________')\n","('<granthamaa>__________________', '<గ్రంథమా>______________', '<గ్రంథమా>______________')\n","('<durvargamto>_________________', '<దుర్వర్గంతో>__________', '<దుర్వర్గంతో>__________')\n","('<bhagotallo>__________________', '<భాగోతాల్లో>___________', '<భాగోతాల్లో>___________')\n","('<unility>_____________________', '<యునిలిటీ>_____________', '<యునిలిటీ>_____________')\n","('<siddeswarakona>______________', '<సిద్దేశ్వరకోన>________', '<సిద్దేశ్వరకోన>________')\n","('<abdadapu>____________________', '<అబ్దదపు>______________', '<అబ్దదపు>______________')\n","('<padukobedutunnadu>___________', '<పడుకోబెడుతున్నాడు>____', '<పడుకోబెడుతున్నాడు>____')\n","('<cosmerman>___________________', '<కాస్మెర్మాన్>_________', '<కాస్మెర్మాన్>_________')\n","ep: 1  bt: 0  loss: 0.038624190765878426  acc:  0.984375\n","ep: 1  bt: 20  loss: 0.03271669926850692  acc:  0.953125\n","ep: 1  bt: 40  loss: 1.3975397192913552  acc:  0.0\n","ep: 1  bt: 60  loss: 0.8725215248439623  acc:  0.0\n","ep: 1  bt: 80  loss: 0.02669841310252314  acc:  0.96875\n","ep: 1  bt: 100  loss: 0.921451071034307  acc:  0.0\n","ep: 1  bt: 120  loss: 0.750992899355681  acc:  0.03125\n","ep: 1  bt: 140  loss: 1.3437418730362602  acc:  0.0\n","ep: 1  bt: 160  loss: 0.027054346126058827  acc:  0.96875\n","ep: 1  bt: 180  loss: 0.021803964739260467  acc:  0.96875\n","ep: 1  bt: 200  loss: 0.014534132636111715  acc:  1.0\n","ep: 1  bt: 220  loss: 0.6674176506374193  acc:  0.046875\n","ep: 1  bt: 240  loss: 0.6196731899095618  acc:  0.03125\n","ep: 1  bt: 260  loss: 0.6872825207917587  acc:  0.0625\n","ep: 1  bt: 280  loss: 0.012660296066947605  acc:  1.0\n","ep: 1  bt: 300  loss: 0.01358020694359489  acc:  1.0\n","ep: 1  bt: 320  loss: 0.6680593075959579  acc:  0.046875\n","ep: 1  bt: 340  loss: 0.7155049365499745  acc:  0.046875\n","ep: 1  bt: 360  loss: 0.6428581320721171  acc:  0.078125\n","ep: 1  bt: 380  loss: 0.012361562770345936  acc:  0.984375\n","ep: 1  bt: 400  loss: 0.49241148907205334  acc:  0.15625\n","ep: 1  bt: 420  loss: 0.020949962346450142  acc:  0.984375\n","ep: 1  bt: 440  loss: 0.6328095145847487  acc:  0.15625\n","ep: 1  bt: 460  loss: 0.012473032526347948  acc:  0.984375\n","ep: 1  bt: 480  loss: 0.7130411396855894  acc:  0.109375\n","ep: 1  bt: 500  loss: 0.6255299112071162  acc:  0.171875\n","ep: 1  bt: 520  loss: 0.5023641586303711  acc:  0.15625\n","ep: 1  bt: 540  loss: 0.008519532887831978  acc:  1.0\n","ep: 1  bt: 560  loss: 0.6101406346196714  acc:  0.1875\n","ep: 1  bt: 580  loss: 0.5334755026775858  acc:  0.21875\n","ep: 1  bt: 600  loss: 1.57428210714589  acc:  0.0\n","ep: 1  bt: 620  loss: 0.9416428441586702  acc:  0.015625\n","ep: 1  bt: 640  loss: 0.01023456519064696  acc:  1.0\n","ep: 1  bt: 660  loss: 0.006767629929210829  acc:  1.0\n","ep: 1  bt: 680  loss: 0.5871885963108229  acc:  0.078125\n","ep: 1  bt: 700  loss: 0.5649200522381327  acc:  0.1875\n","ep: 1  bt: 720  loss: 0.5757631218951681  acc:  0.234375\n","ep: 1  bt: 740  loss: 0.4412371593972911  acc:  0.171875\n","ep: 1  bt: 760  loss: 0.6630914936894956  acc:  0.09375\n","ep: 1  bt: 780  loss: 0.004917400038760641  acc:  1.0\n","('<paali>_______________________', '<పాలి>_________________', '<పాలిిి>_______________')\n","('<varninchadaniki>_____________', '<వర్ణించడానికి>________', '<వర్నించడనికి>_________')\n","('<keegaon>_____________________', '<కీగాన్>_______________', '<కీగోనన్>______________')\n","('<egirekarla>__________________', '<ఎగిరేకార్ల>___________', '<ఎగిరేకర్ల>____________')\n","('<poojanu>_____________________', '<పూజను>________________', '<పూజను>________________')\n","('<fareekshalu>_________________', '<ఫరీక్షలు>_____________', '<ఫరీక్షలు>_____________')\n","('<chidamakudadu>_______________', '<చిదమకూడదు>____________', '<చిదమమకడడడు>___________')\n","('<intini>______________________', '<ఇంటిని>_______________', '<ఇంటిని>_______________')\n","('<adharalu>____________________', '<ఆధారాలు>______________', '<అధరలు>________________')\n","('<keeluki>_____________________', '<కీలుకి>_______________', '<కీలుకి>_______________')\n","('<shoorpanakha>________________', '<శూర్పణఖ>______________', '<శోర్పనకా>_____________')\n","('<nyaayangaa>__________________', '<న్యాయంగా>_____________', '<న్యాయంగా>_____________')\n","('<ambaripai>___________________', '<అంబారిపై>_____________', '<అమబరిపై>______________')\n","('<grahaanni>___________________', '<గ్రహాన్ని>____________', '<గ్రహాన్ని>____________')\n","('<satyalani>___________________', '<సత్యాలని>_____________', '<సత్యలలని>_____________')\n","('<writars>_____________________', '<రైటర్స్>______________', '<వ్రితర్్్_____________')\n","('<raayini>_____________________', '<రాయిని>_______________', '<రాయిని>_______________')\n","('<sathagoepam>_________________', '<శఠగోపం>_______________', '<సతగోపంమ>______________')\n","('<is>__________________________', '<ఈస్>__________________', '<ఇస్్్>________________')\n","('<vayasulone>__________________', '<వయసులోనే>_____________', '<వయసులోనే>_____________')\n","ep:  1  train acc: 0.5298046875  train loss: 0.3620533210857083  val acc: 0.227294921875  val loss: 0.416533843330715\n","('<punaade>_____________________', '<పునాదే>_______________', '<పునాడే>_______________')\n","('<chadivevadilo>_______________', '<చదివేవాడిలో>__________', '<చదివేవడిలో>___________')\n","('<swayampaakam>________________', '<స్వయంపాకం>____________', '<స్వయయంపాకం>___________')\n","('<malleswaraswamy>_____________', '<మల్లేశ్వరస్వామి>______', '<మల్లేస్వస్వాయయ>_______')\n","('<swargamayam>_________________', '<స్వర్గమయం>____________', '<స్వర్గంమయమ>___________')\n","('<yules>_______________________', '<యులెస్>_______________', '<యులేస్>_______________')\n","('<freakers>____________________', '<ఫ్రీకర్స్>____________', '<ఫ్రీకెర్్_____________')\n","('<mugalikar>___________________', '<ముగాలికర్>____________', '<ముగలికర్>_____________')\n","('<microsystem>_________________', '<మైక్రోసిస్టమ్>________', '<మిక్రోస్స్టేం>________')\n","('<valuvalni>___________________', '<వలువల్ని>_____________', '<వలువల్ని>_____________')\n","('<vaadakamnu>__________________', '<వాడకంను>______________', '<వాడకమను>______________')\n","('<adhyakshudanani>_____________', '<అధ్యక్షుడనని>_________', '<అధ్యక్షుడనని>_________')\n","('<guvulu>______________________', '<గువులు>_______________', '<గువులు>_______________')\n","('<venakkitaggela>______________', '<వెనక్కితగ్గేలా>_______', '<వెనక్కిటగగలల>_________')\n","('<tittukunnattu>_______________', '<తిట్టుకున్నట్టు>______', '<తిట్టుకున్నట్టు>______')\n","('<angeekarimpajesina>__________', '<అంగీకరింపజేసిన>_______', '<అంగీకరింపజేని>________')\n","('<niyojakavargampai>___________', '<నియోజకవర్గంపై>________', '<నియోజకవర్గంపై>________')\n","('<udvegaparamgaa>______________', '<ఉద్వేగపరంగా>__________', '<ఉద్వెగాపరంగా__________')\n","('<pradararipu>_________________', '<ప్రదరరిపు>____________', '<ప్రదరరిపు>____________')\n","('<nelalovacchaannenu>__________', '<నెలలోవచ్చాన్నేను>_____', '<నెలలోవవచ్చానననన>______')\n","ep: 2  bt: 0  loss: 0.4220955060875934  acc:  0.21875\n","ep: 2  bt: 20  loss: 0.4396673700083857  acc:  0.1875\n","ep: 2  bt: 40  loss: 0.005021807292233343  acc:  1.0\n","ep: 2  bt: 60  loss: 0.47084928595501446  acc:  0.234375\n","ep: 2  bt: 80  loss: 0.004507752864257149  acc:  1.0\n","ep: 2  bt: 100  loss: 0.006239617648332015  acc:  0.984375\n","ep: 2  bt: 120  loss: 0.005514367767002272  acc:  0.984375\n","ep: 2  bt: 140  loss: 0.006585443149442258  acc:  0.984375\n","ep: 2  bt: 160  loss: 0.46557542552118714  acc:  0.265625\n","ep: 2  bt: 180  loss: 0.4617823310520338  acc:  0.21875\n","ep: 2  bt: 200  loss: 0.004997473048127215  acc:  1.0\n","ep: 2  bt: 220  loss: 0.005539094624312028  acc:  1.0\n","ep: 2  bt: 240  loss: 0.004586972620176232  acc:  1.0\n","ep: 2  bt: 260  loss: 0.7418498163637908  acc:  0.078125\n","ep: 2  bt: 280  loss: 0.008246881158455559  acc:  0.984375\n","ep: 2  bt: 300  loss: 0.42411111748736835  acc:  0.265625\n","ep: 2  bt: 320  loss: 0.005889644441397294  acc:  0.984375\n","ep: 2  bt: 340  loss: 0.6161280922267748  acc:  0.21875\n","ep: 2  bt: 360  loss: 0.007576371664586274  acc:  0.984375\n","ep: 2  bt: 380  loss: 0.007961115111475405  acc:  0.984375\n","ep: 2  bt: 400  loss: 0.3676691884579866  acc:  0.296875\n","ep: 2  bt: 420  loss: 0.4193507899408755  acc:  0.3125\n","ep: 2  bt: 440  loss: 0.004854873794576396  acc:  0.984375\n","ep: 2  bt: 460  loss: 0.48632335662841797  acc:  0.25\n","ep: 2  bt: 480  loss: 0.4208056823067043  acc:  0.234375\n","ep: 2  bt: 500  loss: 0.003831381707087807  acc:  1.0\n","ep: 2  bt: 520  loss: 0.003651741730130237  acc:  1.0\n","ep: 2  bt: 540  loss: 0.004979368785153265  acc:  1.0\n","ep: 2  bt: 560  loss: 0.004655545496422312  acc:  1.0\n","ep: 2  bt: 580  loss: 0.6409113925436268  acc:  0.078125\n","ep: 2  bt: 600  loss: 0.004182489345902982  acc:  1.0\n","ep: 2  bt: 620  loss: 0.48182130896526837  acc:  0.140625\n","ep: 2  bt: 640  loss: 0.32144210649573285  acc:  0.28125\n","ep: 2  bt: 660  loss: 0.47690122023872705  acc:  0.140625\n","ep: 2  bt: 680  loss: 0.0039060776648314104  acc:  1.0\n","ep: 2  bt: 700  loss: 0.003967458787171737  acc:  1.0\n","ep: 2  bt: 720  loss: 0.0036846130438472915  acc:  1.0\n","ep: 2  bt: 740  loss: 0.4259910583496094  acc:  0.203125\n","ep: 2  bt: 760  loss: 0.479895425879437  acc:  0.265625\n","ep: 2  bt: 780  loss: 0.3902861968330715  acc:  0.234375\n","('<eedchina>____________________', '<ఈడ్చిన>_______________', '<ఎద్చిన>_______________')\n","('<pattanaabhivruddikosam>______', '<పట్టణాభివృద్దికోసం>___', '<పట్టనాభివ్రుడ్డంకో____')\n","('<soobhatoo>___________________', '<శోభతో>________________', '<సూభటో>________________')\n","('<nirodhakathanu>______________', '<నిరోధకతను>____________', '<నిరోధకతాు>____________')\n","('<sthaayulane>_________________', '<స్థాయులనే>____________', '<స్థాయులనే>____________')\n","('<ramayana>____________________', '<రామాయణ>_______________', '<రామయాా________________')\n","('<nirmoolinchadam>_____________', '<నిర్మూలించడం>_________', '<నిర్మూలించడం>_________')\n","('<screan>______________________', '<స్క్రీన్>_____________', '<స్క్రాన్>_____________')\n","('<prayanikula>_________________', '<ప్రయాణికుల>___________', '<ప్రయయిికుల>___________')\n","('<ushtakavasanni>______________', '<ఉష్టకవశాన్ని>_________', '<ఉష్టకవసన్ని>__________')\n","('<ayithe>______________________', '<అయితే>________________', '<అయితే>________________')\n","('<saakshyamgaa>________________', '<సాక్ష్యంగా>___________', '<సాక్ష్యంగా>___________')\n","('<arachetiloo>_________________', '<అరచేతిలో>_____________', '<అరచేటిలో>_____________')\n","('<bear>________________________', '<బీర్>_________________', '<బార్్>________________')\n","('<shyamala>____________________', '<శ్యామలా>______________', '<ష్యాలా________________')\n","('<denini>______________________', '<దేనిని>_______________', '<దెనిని>_______________')\n","('<cheeyadampai>________________', '<చేయడంపై>______________', '<చీయడంపై>______________')\n","('<muchatinchukovalanna>________', '<ముచ్చటించుకోవాలన్న>___', '<ముచటించుకోవాలన్న>_____')\n","('<kanada>______________________', '<కెనడా>________________', '<కనడా>_________________')\n","('<vimaanayaana>________________', '<విమానయాన>_____________', '<విమానాానా>____________')\n","ep:  2  train acc: 0.58615234375  train loss: 0.2628922137344742  val acc: 0.29296875  val loss: 0.3636358095251996\n","('<punchukunnaaru>______________', '<పుంచుకున్నారు>________', '<పుంచుకున్నారు>________')\n","('<samsthalavaipu>______________', '<సంస్థలవైపు>___________', '<సమస్థలవైపు>___________')\n","('<bhadraparachamani>___________', '<భద్రపరచామని>__________', '<భద్రపరచచమని>__________')\n","('<annaya>______________________', '<అన్నాయ>_______________', '<అన్నాయ>_______________')\n","('<vaerukaabadi>________________', '<వేరుకాబడి>____________', '<వేరుకాబడి>____________')\n","('<senile>______________________', '<సెనిలే>_______________', '<సెనిలే>_______________')\n","('<nikkabaduchukunela>__________', '<నిక్కబడుచుకునేలా>_____', '<నిక్కబడుచుకునేలేల>____')\n","('<ulupu>_______________________', '<ఉలుపు>________________', '<ఉలుపు>________________')\n","('<vinavu>______________________', '<వినవు>________________', '<వినవు>________________')\n","('<kumaarayyaki>________________', '<కుమారయ్యకి>___________', '<కుమారయయయయకి>__________')\n","('<gajulugondi>_________________', '<గాజులుగొంది>__________', '<గాుుుగొండి>___________')\n","('<samayaattamavutunnaayi>______', '<సమయాత్తమవుతున్నాయి>___', '<సమయాత్తమవుతున్నాయి>___')\n","('<shkaramvaipu>________________', '<ష్కారంవైపు>___________', '<శ్కరమవైపు>____________')\n","('<brancost>____________________', '<బ్రాంకోస్ట్>__________', '<బ్రాంకోస్్్>__________')\n","('<cheyyadu>____________________', '<చెయ్యదు>______________', '<చేయ్యడు>______________')\n","('<nadeepravaahamlo>____________', '<నదీప్రవాహంలో>_________', '<నడీప్రవాహంలో>_________')\n","('<pogadadanike>________________', '<పొగడడానికే>___________', '<పోగడడనికకే>___________')\n","('<vrishtin>____________________', '<వృష్టిన్>_____________', '<వ్రిష్టిన్>___________')\n","('<akaalamaranaalaku>___________', '<అకాలమరణాలకు>__________', '<అకాలమరణణలకు>__________')\n","('<dorakada>____________________', '<దొరకడా>_______________', '<దోరకడడ>_______________')\n","ep: 3  bt: 0  loss: 0.31508391836415167  acc:  0.328125\n","ep: 3  bt: 20  loss: 0.4237149694691534  acc:  0.3125\n","ep: 3  bt: 40  loss: 0.5283136367797852  acc:  0.1875\n","ep: 3  bt: 60  loss: 0.3921259589817213  acc:  0.328125\n","ep: 3  bt: 80  loss: 0.5480716124824856  acc:  0.15625\n","ep: 3  bt: 100  loss: 0.002911449126575304  acc:  1.0\n","ep: 3  bt: 120  loss: 0.36082777769669244  acc:  0.265625\n","ep: 3  bt: 140  loss: 0.37437766531239386  acc:  0.28125\n","ep: 3  bt: 160  loss: 0.3419182404227879  acc:  0.359375\n","ep: 3  bt: 180  loss: 0.0031514854534812594  acc:  1.0\n","ep: 3  bt: 200  loss: 0.46634429434071417  acc:  0.265625\n","ep: 3  bt: 220  loss: 0.0047375012351119  acc:  0.984375\n","ep: 3  bt: 240  loss: 0.0023645033007082734  acc:  1.0\n","ep: 3  bt: 260  loss: 0.3601372345634129  acc:  0.234375\n","ep: 3  bt: 280  loss: 0.0031990700441858044  acc:  1.0\n","ep: 3  bt: 300  loss: 0.47524452209472656  acc:  0.28125\n","ep: 3  bt: 320  loss: 0.8396484540856403  acc:  0.03125\n","ep: 3  bt: 340  loss: 0.004686863202115764  acc:  1.0\n","ep: 3  bt: 360  loss: 0.005571948445361593  acc:  0.984375\n","ep: 3  bt: 380  loss: 0.004876853331275608  acc:  0.984375\n","ep: 3  bt: 400  loss: 0.4819641942563264  acc:  0.3125\n","ep: 3  bt: 420  loss: 0.0024917880478112593  acc:  1.0\n","ep: 3  bt: 440  loss: 0.42873822087826935  acc:  0.34375\n","ep: 3  bt: 460  loss: 0.35783904531727667  acc:  0.328125\n","ep: 3  bt: 480  loss: 0.0027178490291471066  acc:  1.0\n","ep: 3  bt: 500  loss: 0.2996123148047406  acc:  0.328125\n","ep: 3  bt: 520  loss: 0.37779463892397674  acc:  0.375\n","ep: 3  bt: 540  loss: 0.38578191010848334  acc:  0.328125\n","ep: 3  bt: 560  loss: 0.004237959566323653  acc:  0.984375\n","ep: 3  bt: 580  loss: 0.3667520854784095  acc:  0.375\n","ep: 3  bt: 600  loss: 0.0029369080844132795  acc:  1.0\n","ep: 3  bt: 620  loss: 0.00224433580170507  acc:  1.0\n","ep: 3  bt: 640  loss: 0.33779326729152515  acc:  0.328125\n","ep: 3  bt: 660  loss: 0.004479267351005388  acc:  0.984375\n","ep: 3  bt: 680  loss: 0.4528053947117018  acc:  0.34375\n","ep: 3  bt: 700  loss: 0.30491161346435547  acc:  0.34375\n","ep: 3  bt: 720  loss: 0.002030453928138899  acc:  1.0\n","ep: 3  bt: 740  loss: 0.364520902219026  acc:  0.359375\n","ep: 3  bt: 760  loss: 0.2906802840854811  acc:  0.4375\n","ep: 3  bt: 780  loss: 0.318492433299189  acc:  0.375\n","('<vrellu>______________________', '<వ్రేళ్ళు>_____________', '<వ్రేల్లు>_____________')\n","('<aatasthalaalaku>_____________', '<ఆటస్థలాలకు>___________', '<ఆతస్థలాలకు>___________')\n","('<thimtaayi>___________________', '<తింటాయి>______________', '<తింతాయి>______________')\n","('<vyaktulani>__________________', '<వ్యక్తులని>___________', '<వ్యక్తులని>___________')\n","('<leen>________________________', '<లీన్>_________________', '<లీన్>_________________')\n","('<shaalaloo>___________________', '<శాలలో>________________', '<శాలలో>________________')\n","('<thaara>______________________', '<తార>__________________', '<తారా>_________________')\n","('<nilabadam>___________________', '<నిలబడం>_______________', '<నిలబడం>_______________')\n","('<kommaku>_____________________', '<కొమ్మకు>______________', '<కొమ్మకు>______________')\n","('<puruguloste>_________________', '<పురుగులొస్తే>_________', '<పురుగులోస్తే>_________')\n","('<paripakva>___________________', '<పరిపక్వ>______________', '<పరిపక్వ>______________')\n","('<draksha>_____________________', '<ద్రాక్ష>______________', '<ద్రక్ష>_______________')\n","('<kshetralu>___________________', '<క్షేత్రాలు>___________', '<క్షేత్రలు>____________')\n","('<mahanubhavulu>_______________', '<మహానుభావులు>__________', '<మహనుభవవులు>___________')\n","('<vachanam>____________________', '<వచనం>_________________', '<వచ్చనం>_______________')\n","('<jalaalanu>___________________', '<జలాలను>_______________', '<జలాలను>_______________')\n","('<thrisuulam>__________________', '<త్రిశూలం>_____________', '<త్రిసూలం>_____________')\n","('<turaga>______________________', '<తురగా>________________', '<తరరగా>________________')\n","('<marujanmaloo>________________', '<మరుజన్మలో>____________', '<మరుజన్మలో>____________')\n","('<terachaapa>__________________', '<తెరచాప>_______________', '<తెరచాప>_______________')\n","ep:  3  train acc: 0.648671875  train loss: 0.2012455770464454  val acc: 0.339599609375  val loss: 0.33121021934177564\n","('<narasimharaopalem>___________', '<నరసింహారావుపాలెం>_____', '<నరసింహారావుపాలెం>_____')\n","('<thokapalli>__________________', '<తోకపల్లి>_____________', '<తోకపల్లి>_____________')\n","('<merwaha>_____________________', '<మెర్వాహా>_____________', '<మెర్వాహా>_____________')\n","('<shaadeelo>___________________', '<షాదీలో>_______________', '<షాదీలో>_______________')\n","('<nekkupettaaru>_______________', '<నెక్కుపెట్టారు>_______', '<నెక్కుపెట్టారు>_______')\n","('<pallevaatavaranam>___________', '<పల్లెవాతవరణం>_________', '<పల్లెవాతవరణం>_________')\n","('<pakshamrojullo>______________', '<పక్షంరోజుల్లో>________', '<పక్షంరోజుల్లో>________')\n","('<tegabadutunnadani>___________', '<తెగబడుతున్నదని>_______', '<తెగబడుతున్నదని>_______')\n","('<cheraduku>___________________', '<చేరదుకు>______________', '<చేరదుకు>______________')\n","('<neetukumari>_________________', '<నీతుకుమారి>___________', '<నీతుకుమారి>___________')\n","('<ponnana>_____________________', '<పొన్నన>_______________', '<పొన్నన>_______________')\n","('<sphurinchetatlu>_____________', '<స్ఫురించేటట్లు>_______', '<స్ఫురించేటట్లు>_______')\n","('<tappinadi>___________________', '<తప్పినది>_____________', '<తప్పినది>_____________')\n","('<usigolpinattu>_______________', '<ఉసిగొల్పినట్టు>_______', '<ఉసిగొల్పినట్టు>_______')\n","('<upayogaastaaru>______________', '<ఉపయోగాస్తారు>_________', '<ఉపయోగాస్తారు>_________')\n","('<expendables>_________________', '<ఎక్స్పెన్డబుల్స్>_____', '<ఎక్స్పెన్డబుల్స్>_____')\n","('<deepavu>_____________________', '<దీపవు>________________', '<దీపవు>________________')\n","('<ishtapadutaanu>______________', '<ఇష్టపడుతాను>__________', '<ఇష్టపడుతాను>__________')\n","('<sweekarinchaaranii>__________', '<స్వీకరించారనీ>________', '<స్వీకరించారనీ>________')\n","('<chorabadevadu>_______________', '<చొరబడేవాడు>___________', '<చొరబడేవాడు>___________')\n","ep: 4  bt: 0  loss: 0.0016113524527653403  acc:  1.0\n","ep: 4  bt: 20  loss: 0.0031806178714918055  acc:  1.0\n","ep: 4  bt: 40  loss: 0.0019136869713016179  acc:  1.0\n","ep: 4  bt: 60  loss: 0.0017954252012397933  acc:  1.0\n","ep: 4  bt: 80  loss: 0.31812155765035877  acc:  0.328125\n","ep: 4  bt: 100  loss: 0.0032707216299098472  acc:  0.984375\n","ep: 4  bt: 120  loss: 0.27405452728271484  acc:  0.34375\n","ep: 4  bt: 140  loss: 0.002075338331253632  acc:  1.0\n","ep: 4  bt: 160  loss: 0.001423037246517513  acc:  1.0\n","ep: 4  bt: 180  loss: 0.0026189428956612296  acc:  1.0\n","ep: 4  bt: 200  loss: 0.0018479545479235442  acc:  1.0\n","ep: 4  bt: 220  loss: 0.314343493917714  acc:  0.34375\n","ep: 4  bt: 240  loss: 0.3669227517169455  acc:  0.234375\n","ep: 4  bt: 260  loss: 0.34730867717577063  acc:  0.484375\n","ep: 4  bt: 280  loss: 0.5826396527497665  acc:  0.171875\n","ep: 4  bt: 300  loss: 0.960102827652641  acc:  0.0625\n","ep: 4  bt: 320  loss: 0.014404132314350294  acc:  0.9375\n","ep: 4  bt: 340  loss: 0.5374951155289359  acc:  0.09375\n","ep: 4  bt: 360  loss: 0.5383887083634086  acc:  0.265625\n","ep: 4  bt: 380  loss: 0.36495673138162366  acc:  0.296875\n","ep: 4  bt: 400  loss: 0.32173092468925146  acc:  0.421875\n","ep: 4  bt: 420  loss: 0.0018682223947151847  acc:  1.0\n","ep: 4  bt: 440  loss: 0.37200537971828296  acc:  0.359375\n","ep: 4  bt: 460  loss: 0.26914980100548785  acc:  0.4375\n","ep: 4  bt: 480  loss: 0.00262980989140013  acc:  0.984375\n","ep: 4  bt: 500  loss: 0.0017492098976736484  acc:  1.0\n","ep: 4  bt: 520  loss: 0.400512405063795  acc:  0.375\n","ep: 4  bt: 540  loss: 0.3420900883881942  acc:  0.296875\n","ep: 4  bt: 560  loss: 0.42702397056247876  acc:  0.359375\n","ep: 4  bt: 580  loss: 0.002028110558572023  acc:  1.0\n","ep: 4  bt: 600  loss: 0.3653042005456012  acc:  0.328125\n","ep: 4  bt: 620  loss: 0.3308484865271527  acc:  0.46875\n","ep: 4  bt: 640  loss: 0.0016539291195247483  acc:  1.0\n","ep: 4  bt: 660  loss: 0.0020625931413277335  acc:  1.0\n","ep: 4  bt: 680  loss: 0.0034003354932950892  acc:  1.0\n","ep: 4  bt: 700  loss: 0.0014150283582832503  acc:  1.0\n","ep: 4  bt: 720  loss: 0.0017505974873252537  acc:  1.0\n","ep: 4  bt: 740  loss: 0.002537435811498891  acc:  1.0\n","ep: 4  bt: 760  loss: 0.3996438980102539  acc:  0.390625\n","ep: 4  bt: 780  loss: 0.0018447762274223826  acc:  1.0\n","('<sriramgaaniki>_______________', '<శ్రీరంగానికి>_________', '<స్రీరంగానికి>_________')\n","('<lonu>________________________', '<లోను>_________________', '<లోను>_________________')\n","('<karyaniki>___________________', '<కార్యానికి>___________', '<కరర్యనికి>____________')\n","('<angeekarinchadam>____________', '<అంగీకరించడం>__________', '<అంగీకరించడం>__________')\n","('<changnu>_____________________', '<చాంగ్ను>______________', '<చంగ్ను>_______________')\n","('<vishnubhaktudaina>___________', '<విష్ణుభక్తుడైన>_______', '<విష్ణుభక్తునైన>_______')\n","('<kalyaanamantapaanni>_________', '<కళ్యాణమంటపాన్ని>______', '<కల్యాణమంతపాన్ని>______')\n","('<niroodhinchi>________________', '<నిరోధించి>____________', '<నిరూధించి>____________')\n","('<sankshoebham>________________', '<సంక్షోభం>_____________', '<సంక్షోభం>_____________')\n","('<unnavaadu>___________________', '<ఉన్నవాడు>_____________', '<ఉన్నవాదు>_____________')\n","('<digalekapoyadu>______________', '<దిగలేకపోయాడు>_________', '<దిగలేకపోయాడు>_________')\n","('<sultana>_____________________', '<సుల్తానా>_____________', '<సుల్టన>_______________')\n","('<chaalaarojulavutundi>________', '<చాలారోజులవుతుంది>_____', '<చాలారోజులవుతుంది>_____')\n","('<gelavalekapoyanannaru>_______', '<గెలవలేకపోయానన్నారు>___', '<గెలవలేకపోయననననారు>____')\n","('<cheyanunnadantu>_____________', '<చేయనున్నాడంటూ>________', '<చేయనున్నాడంటూ>________')\n","('<canadalo>____________________', '<కెనడాలో>______________', '<కానాలో>_______________')\n","('<churchini>___________________', '<చర్చిని>______________', '<చూర్చిని>_____________')\n","('<garvakaranam>________________', '<గర్వకారణం>____________', '<గర్వకరణం>_____________')\n","('<keli>________________________', '<కేళి>_________________', '<కెలి>_________________')\n","('<yathatatha>__________________', '<యథాతథ>________________', '<యతతత>_________________')\n","ep:  4  train acc: 0.65490234375  train loss: 0.1971568889843056  val acc: 0.363525390625  val loss: 0.3220869561900263\n","('<samsaaraavarta>______________', '<సంసారావర్త>___________', '<సంసారావర్త>___________')\n","('<paintpai>____________________', '<పెయింట్పై>____________', '<పెయింట్పై>____________')\n","('<meppistunnaaru>______________', '<మెప్పిస్తున్నారు>_____', '<మెప్పిస్తున్నారు>_____')\n","('<upayoginchevallam>___________', '<ఉపయోగించేవాళ్ళం>______', '<ఉపయోగించేవాళ్ళం>______')\n","('<tippesukuntundani>___________', '<తిప్పెసుకుంటుందని>____', '<తిప్పెసుకుంటుందని>____')\n","('<kulnura>_____________________', '<కుల్నురా>_____________', '<కుల్నురా>_____________')\n","('<muramalla>___________________', '<మురమళ్ళ>______________', '<మురమళ్ళ>______________')\n","('<peethaadhipathulalo>_________', '<పీఠాధిపతులలో>_________', '<పీఠాధిపతులలో>_________')\n","('<marodaniloki>________________', '<మరోదానిలోకి>__________', '<మరోదానిలోకి>__________')\n","('<ramasamy>____________________', '<రామస్వామి>____________', '<రామస్వామి>____________')\n","('<pellikimundu>________________', '<పెళ్ళికిముందు>________', '<పెళ్ళికిముందు>________')\n","('<protography>_________________', '<ప్రొటోగ్రఫీ>__________', '<ప్రొటోగ్రఫీ>__________')\n","('<raledantene>_________________', '<రాలేదంటేనే>___________', '<రాలేదంటేనే>___________')\n","('<teekshanamayina>_____________', '<తీక్షణమయిన>___________', '<తీక్షణమయిన>___________')\n","('<roopondinchaali>_____________', '<రూపొందించాలి>_________', '<రూపొందించాలి>_________')\n","('<mudralivi>___________________', '<ముద్రలివి>____________', '<ముద్రలివి>____________')\n","('<tedinaatiki>_________________', '<తేదినాటికి>___________', '<తేదినాటికి>___________')\n","('<nadulaloogaani>______________', '<నదులలోగాని>___________', '<నదులలోగాని>___________')\n","('<manushulanundi>______________', '<మనుషులనుండి>__________', '<మనుషులనుండి>__________')\n","('<paryaaravarana>______________', '<పర్యారవరణ>____________', '<పర్యారవరణ>____________')\n","ep: 5  bt: 0  loss: 0.0015302021866259367  acc:  1.0\n","ep: 5  bt: 20  loss: 0.29919470911440643  acc:  0.3125\n","ep: 5  bt: 40  loss: 0.0010458000976106396  acc:  1.0\n","ep: 5  bt: 60  loss: 0.0013632672312466996  acc:  1.0\n","ep: 5  bt: 80  loss: 0.4251833791318147  acc:  0.390625\n","ep: 5  bt: 100  loss: 0.0012803491528915322  acc:  1.0\n","ep: 5  bt: 120  loss: 0.0017823866851951766  acc:  1.0\n","ep: 5  bt: 140  loss: 0.3003310535265052  acc:  0.3125\n","ep: 5  bt: 160  loss: 0.3833773239799168  acc:  0.328125\n","ep: 5  bt: 180  loss: 0.3152309915293818  acc:  0.484375\n","ep: 5  bt: 200  loss: 0.0019108978626520736  acc:  1.0\n","ep: 5  bt: 220  loss: 0.2900570993838103  acc:  0.4375\n","ep: 5  bt: 240  loss: 0.4776685548865277  acc:  0.203125\n","ep: 5  bt: 260  loss: 0.0014840970220773117  acc:  1.0\n","ep: 5  bt: 280  loss: 0.3976647335550059  acc:  0.15625\n","ep: 5  bt: 300  loss: 0.0021833142508631167  acc:  1.0\n","ep: 5  bt: 320  loss: 0.3474844020345937  acc:  0.421875\n","ep: 5  bt: 340  loss: 0.0021391381388125214  acc:  1.0\n","ep: 5  bt: 360  loss: 0.28616818137790845  acc:  0.390625\n","ep: 5  bt: 380  loss: 0.0016697112308896105  acc:  1.0\n","ep: 5  bt: 400  loss: 0.0014864429831504822  acc:  1.0\n","ep: 5  bt: 420  loss: 0.2543131579523501  acc:  0.4375\n","ep: 5  bt: 440  loss: 0.0011095061250354934  acc:  1.0\n","ep: 5  bt: 460  loss: 0.35287799005923065  acc:  0.421875\n","ep: 5  bt: 480  loss: 0.0013767079166744065  acc:  1.0\n","ep: 5  bt: 500  loss: 0.26702752320662787  acc:  0.40625\n","ep: 5  bt: 520  loss: 0.34064527179883874  acc:  0.375\n","ep: 5  bt: 540  loss: 0.284229734669561  acc:  0.4375\n","ep: 5  bt: 560  loss: 0.001111563619064248  acc:  1.0\n","ep: 5  bt: 580  loss: 0.3945716360340948  acc:  0.296875\n","ep: 5  bt: 600  loss: 0.0010916270801554556  acc:  1.0\n","ep: 5  bt: 620  loss: 0.001025155267637709  acc:  1.0\n","ep: 5  bt: 640  loss: 0.35725950158160663  acc:  0.28125\n","ep: 5  bt: 660  loss: 0.3007183074951172  acc:  0.421875\n","ep: 5  bt: 680  loss: 0.41976609437362006  acc:  0.28125\n","ep: 5  bt: 700  loss: 0.5024995803833008  acc:  0.375\n","ep: 5  bt: 720  loss: 0.2864337382109269  acc:  0.390625\n","ep: 5  bt: 740  loss: 0.001589806669432184  acc:  1.0\n","ep: 5  bt: 760  loss: 0.0009870656320582266  acc:  1.0\n","ep: 5  bt: 780  loss: 0.26675112351127295  acc:  0.46875\n","('<impyna>______________________', '<ఇంపైన>________________', '<ఇంప్న>________________')\n","('<helana>______________________', '<హేళన>_________________', '<హెలన>_________________')\n","('<sakhyathatho>________________', '<సఖ్యతతో>______________', '<సఖఖ్తతో>______________')\n","('<vishnuvuni>__________________', '<విష్ణువుని>___________', '<విష్ణువుని>___________')\n","('<falakaalu>___________________', '<ఫలకాలు>_______________', '<ఫలకాలు>_______________')\n","('<and>_________________________', '<ఆండ్>_________________', '<అండడ>_________________')\n","('<arachaetiloe>________________', '<అరచేతిలో>_____________', '<అరచేతిలో>_____________')\n","('<thupaakeelunnaayi>___________', '<తుపాకీలున్నాయి>_______', '<తుపాకీలున్నాయి>_______')\n","('<nirodhinche>_________________', '<నిరోధించే>____________', '<నిరోధించే>____________')\n","('<kagaladhu>___________________', '<కాగలదు>_______________', '<కగలదు>________________')\n","('<saastroktareetin>____________', '<శాస్త్రోక్తరీతిన్>____', '<సాస్త్రోక్టరీటిన్>____')\n","('<balaheenaparichindani>_______', '<బలహీనపరిచిందని>_______', '<బలహీనపరిచిందని>_______')\n","('<khaidilugaa>_________________', '<ఖైదీలుగా>_____________', '<ఖైదిలుగా>_____________')\n","('<teliduu>_____________________', '<తెలిదూ>_______________', '<తెలిడూ>_______________')\n","('<praantmulooni>_______________', '<ప్రాంతములోని>_________', '<ప్రాంత్ములోని>________')\n","('<sankshema>___________________', '<సంక్షేమ>______________', '<సంక్షేమమ______________')\n","('<vikrethalu>__________________', '<విక్రేతలు>____________', '<విక్రేతలు>____________')\n","('<kaligivunnadhi>______________', '<కలిగివున్నది>_________', '<కలిగివున్నది>_________')\n","('<nadirevu>____________________', '<నడిరేవు>______________', '<నడిరేవు>______________')\n","('<uchhaarana>__________________', '<ఉచ్చారణ>______________', '<ఉచాచాన>_______________')\n","ep:  5  train acc: 0.6771484375  train loss: 0.17081587887136274  val acc: 0.365966796875  val loss: 0.30448876256528107\n","('<premanagaramgaa>_____________', '<ప్రేమనగరంగా>__________', '<ప్రేమనగగంగా>__________')\n","('<kottadamena>_________________', '<కొట్టడమేనా>___________', '<కొట్టడమేన>____________')\n","('<upadhyayulante>______________', '<ఉపాధ్యాయులంటే>________', '<ఉపద్యయులంటే>__________')\n","('<srushtimstoondani>___________', '<సృష్టింస్తోందని>______', '<సృష్టింస్తోందని>______')\n","('<chitraalenni>________________', '<చిత్రాలెన్ని>_________', '<చిట్రాలేన్ని>_________')\n","('<gunjukuntunte>_______________', '<గుంజుకుంటుంటే>________', '<గుంజుకుంటుంటే>________')\n","('<aastaanamlooni>______________', '<ఆస్తానంలోని>__________', '<ఆస్తానంలోని>__________')\n","('<kannumuustundadam>___________', '<కన్నుమూస్తుండడం>______', '<కన్నుమూస్తుందడం>______')\n","('<pooladandanu>________________', '<పూలదండను>_____________', '<పూలదందను>_____________')\n","('<sandehinchavu>_______________', '<సందేహించవు>___________', '<సందేహించవు>___________')\n","('<morika>______________________', '<మోరిక>________________', '<మోరిక>________________')\n","('<vyaadhigrasturaalaina>_______', '<వ్యాధిగ్రస్తురాలైన>___', '<వ్యాధిగ్రస్తురాలైన>___')\n","('<pretaadulu>__________________', '<ప్రేతాదులు>___________', '<ప్రేతాడులు>___________')\n","('<kanulunnay>__________________', '<కనులున్నాయ్>__________', '<కనులున్నాయ్>__________')\n","('<heltical>____________________', '<హెల్టికల్>____________', '<హెల్టిక్ల్>___________')\n","('<doomaala>____________________', '<దూమాల>________________', '<దూమాల>________________')\n","('<tenno>_______________________', '<టెన్నో>_______________', '<టెన్నో>_______________')\n","('<cuminal>_____________________', '<క్యుమినల్>____________', '<క్యానిన్్_____________')\n","('<kekelu>______________________', '<కెకెలు>_______________', '<కేకెలు>_______________')\n","('<sabhasadulato>_______________', '<సభాసదులతో>____________', '<సభసాడులతో>____________')\n","ep: 6  bt: 0  loss: 0.2002208129219387  acc:  0.53125\n","ep: 6  bt: 20  loss: 0.0014361235434594362  acc:  1.0\n","ep: 6  bt: 40  loss: 0.24015256632929263  acc:  0.515625\n","ep: 6  bt: 60  loss: 0.3310316749241041  acc:  0.4375\n","ep: 6  bt: 80  loss: 0.2936893960703974  acc:  0.421875\n","ep: 6  bt: 100  loss: 0.0014276587120864701  acc:  1.0\n","ep: 6  bt: 120  loss: 0.2752693632374639  acc:  0.40625\n","ep: 6  bt: 140  loss: 0.29537526420924975  acc:  0.265625\n","ep: 6  bt: 160  loss: 0.002279280644396077  acc:  0.984375\n","ep: 6  bt: 180  loss: 0.2969878445500913  acc:  0.4375\n","ep: 6  bt: 200  loss: 0.00160134956240654  acc:  1.0\n","ep: 6  bt: 220  loss: 0.0015025069208248801  acc:  1.0\n","ep: 6  bt: 240  loss: 0.22904010440992273  acc:  0.5\n","ep: 6  bt: 260  loss: 0.20623037089472232  acc:  0.5\n","ep: 6  bt: 280  loss: 0.0010808537025814471  acc:  1.0\n","ep: 6  bt: 300  loss: 0.3867502627165421  acc:  0.328125\n","ep: 6  bt: 320  loss: 0.2975214667942213  acc:  0.46875\n","ep: 6  bt: 340  loss: 0.35881992008375085  acc:  0.421875\n","ep: 6  bt: 360  loss: 0.0013673193752765656  acc:  1.0\n","ep: 6  bt: 380  loss: 0.0008885565659274225  acc:  1.0\n","ep: 6  bt: 400  loss: 0.0009616770498130633  acc:  1.0\n","ep: 6  bt: 420  loss: 0.0008438826417145522  acc:  1.0\n","ep: 6  bt: 440  loss: 0.0012249749801729035  acc:  1.0\n","ep: 6  bt: 460  loss: 0.0011029523675856383  acc:  1.0\n","ep: 6  bt: 480  loss: 0.0008695732964121777  acc:  1.0\n","ep: 6  bt: 500  loss: 0.006991676014402638  acc:  0.984375\n","ep: 6  bt: 520  loss: 0.0013553534998841908  acc:  1.0\n","ep: 6  bt: 540  loss: 0.0007193890278754027  acc:  1.0\n","ep: 6  bt: 560  loss: 0.2577958106994629  acc:  0.390625\n","ep: 6  bt: 580  loss: 0.0009431936170743859  acc:  1.0\n","ep: 6  bt: 600  loss: 0.0010697510417388833  acc:  1.0\n","ep: 6  bt: 620  loss: 0.9956261178721553  acc:  0.03125\n","ep: 6  bt: 640  loss: 0.4745123904684316  acc:  0.328125\n","ep: 6  bt: 660  loss: 0.2901601791381836  acc:  0.375\n","ep: 6  bt: 680  loss: 0.0009807374976251435  acc:  1.0\n","ep: 6  bt: 700  loss: 0.0013615212038807247  acc:  1.0\n","ep: 6  bt: 720  loss: 0.0010831193269594855  acc:  1.0\n","ep: 6  bt: 740  loss: 0.0009283398318549861  acc:  1.0\n","ep: 6  bt: 760  loss: 0.43372100332508917  acc:  0.375\n","ep: 6  bt: 780  loss: 0.6249532284943954  acc:  0.328125\n","('<bipini>______________________', '<బిపిని>_______________', '<బిపిని>_______________')\n","('<agnana>______________________', '<అజ్ఞాన>_______________', '<అగ్నాన________________')\n","('<maharoudra>__________________', '<మహారౌద్ర>_____________', '<మహరూూ్ర>______________')\n","('<ashistunnaamu>_______________', '<ఆశిస్తున్నాము>________', '<అషిస్తున్నాము>________')\n","('<oppincharu>__________________', '<ఒప్పించారు>___________', '<ఒప్పించరు>____________')\n","('<pramaaneekarinchaali>________', '<ప్రమాణీకరించాలి>______', '<ప్రమానీకరించాలి>______')\n","('<sarva>_______________________', '<సర్వ>_________________', '<సర్వ>_________________')\n","('<sekaristonnatlu>_____________', '<సేకరిస్తోన్నట్లు>_____', '<సెకరిస్తోన్నట్>ు______')\n","('<aakraminchi>_________________', '<ఆక్రమించి>____________', '<ఆక్రమించి>____________')\n","('<aadesinchaadu>_______________', '<ఆదేశించాడు>___________', '<ఆదేశించాడు>___________')\n","('<anubhavagnaanamtho>__________', '<అనుభవజ్ఞానంతో>________', '<అనుభవగ్ఞానంతో>________')\n","('<moraco>______________________', '<మొరాకో>_______________', '<మొరాక>>_______________')\n","('<cheestunnaapudu>_____________', '<చేస్తున్నపుడు>________', '<చీస్తున్నాపుడు>_______')\n","('<pelamis>_____________________', '<పెలామిస్>_____________', '<పెలాిిస్______________')\n","('<marblehead>__________________', '<మార్బుల్హెడ్>_________', '<మార్లేహేయ్్___________')\n","('<ammananne>___________________', '<అమ్మానాన్నే>__________', '<అమ్మనన్నన>____________')\n","('<schooluloonuu>_______________', '<స్కూలులోనూ>___________', '<స్చూలులోనూ>___________')\n","('<anduloonu>___________________', '<అందులోను>_____________', '<అందులోను>_____________')\n","('<masambuna>___________________', '<మాసంబున>______________', '<మసంబున>_______________')\n","('<srimukhalingeshwaruni>_______', '<శ్రీముఖలింగేశ్వరుని>__', '<స్రీముఖలింగేశారుని>___')\n","ep:  6  train acc: 0.6734375  train loss: 0.17447747863161023  val acc: 0.388427734375  val loss: 0.30638576590496563\n","('<cheluvugan>__________________', '<చెలువుగన్>____________', '<చెలువుగన్>____________')\n","('<mming>_______________________', '<మ్మింగ్>______________', '<మ్మింగ్>______________')\n","('<shuklapakshamlo>_____________', '<శుక్లపక్షంలో>_________', '<శుక్లపక్షంలో>_________')\n","('<godaloniki>__________________', '<గోదాలోనికి>___________', '<గోదాలోనికి>___________')\n","('<praveshamaargampai>__________', '<ప్రవేశమార్గంపై>_______', '<ప్రవేశమార్గంపై>_______')\n","('<brilly>______________________', '<బ్రిలీ>_______________', '<బ్రిలీ>_______________')\n","('<vamshasthulamthaa>___________', '<వంశస్థులంతా>__________', '<వంశస్థులంతా>__________')\n","('<masaba>______________________', '<మసాబా>________________', '<మసాబా>________________')\n","('<jurerutho>___________________', '<జురేరుతో>_____________', '<జురేరుతో>_____________')\n","('<koolinatle>__________________', '<కూలినట్లే>____________', '<కూలినట్లే>____________')\n","('<korevaniki>__________________', '<కోరేవానికి>___________', '<కోరేవానికి>___________')\n","('<gasheshudini>________________', '<గశేషుడిని>____________', '<గశేషుడిని>____________')\n","('<subbannapetagaa>_____________', '<సుబ్బన్నపేటగా>________', '<సుబ్బన్నపేటగా>________')\n","('<uyghur>______________________', '<ఉయ్ఘుర్>______________', '<ఉయ్ఘుర్>______________')\n","('<thaathapettina>______________', '<తాతపెట్టిన>___________', '<తాతపెట్టిన>___________')\n","('<otamevaridi>_________________', '<ఓటమెవరిది>____________', '<ఓటమెవరిది>____________')\n","('<kaangresuvaaru>______________', '<కాంగ్రెసువారు>________', '<కాంగ్రెసువారు>________')\n","('<valva>_______________________', '<వాల్వా>_______________', '<వాల్వా>_______________')\n","('<pudicherla>__________________', '<పుదిచెర్ల>____________', '<పుదిచెర్ల>____________')\n","('<gurtuchestundatamto>_________', '<గుర్తుచేస్తుండటంతో>___', '<గుర్తుచేస్తుండటంతో>___')\n","ep: 7  bt: 0  loss: 0.001234847485371258  acc:  1.0\n","ep: 7  bt: 20  loss: 0.0009146555448355882  acc:  1.0\n","ep: 7  bt: 40  loss: 0.0008799048221629599  acc:  1.0\n","ep: 7  bt: 60  loss: 0.0010336934710326402  acc:  1.0\n","ep: 7  bt: 80  loss: 0.0007919481267099795  acc:  1.0\n","ep: 7  bt: 100  loss: 0.24455733921216882  acc:  0.453125\n","ep: 7  bt: 120  loss: 0.2570807830147121  acc:  0.359375\n","ep: 7  bt: 140  loss: 0.0015267386384632277  acc:  1.0\n","ep: 7  bt: 160  loss: 0.0014477004499539084  acc:  1.0\n","ep: 7  bt: 180  loss: 0.3743967802628227  acc:  0.4375\n","ep: 7  bt: 200  loss: 0.27899559684421704  acc:  0.375\n","ep: 7  bt: 220  loss: 0.2789983542069145  acc:  0.421875\n","ep: 7  bt: 240  loss: 0.0007471093179091164  acc:  1.0\n","ep: 7  bt: 260  loss: 0.0007479096884312837  acc:  1.0\n","ep: 7  bt: 280  loss: 0.0007175938428744026  acc:  1.0\n","ep: 7  bt: 300  loss: 0.0010540090987215872  acc:  1.0\n","ep: 7  bt: 320  loss: 0.38663760475490405  acc:  0.328125\n","ep: 7  bt: 340  loss: 0.20226101253343665  acc:  0.515625\n","ep: 7  bt: 360  loss: 0.2807480355967646  acc:  0.40625\n","ep: 7  bt: 380  loss: 0.23593353188556174  acc:  0.328125\n","ep: 7  bt: 400  loss: 0.3158625312473463  acc:  0.390625\n","ep: 7  bt: 420  loss: 0.19868230819702148  acc:  0.515625\n","ep: 7  bt: 440  loss: 0.0008126649519671564  acc:  1.0\n","ep: 7  bt: 460  loss: 0.0008046712078478025  acc:  1.0\n","ep: 7  bt: 480  loss: 0.0006889065160699513  acc:  1.0\n","ep: 7  bt: 500  loss: 0.0008552258267350819  acc:  1.0\n","ep: 7  bt: 520  loss: 0.001024813188806824  acc:  1.0\n","ep: 7  bt: 540  loss: 0.002564628972955372  acc:  0.984375\n","ep: 7  bt: 560  loss: 0.0008819300033476042  acc:  1.0\n","ep: 7  bt: 580  loss: 0.32860256277996563  acc:  0.359375\n","ep: 7  bt: 600  loss: 0.2629537997038468  acc:  0.4375\n","ep: 7  bt: 620  loss: 0.000980644689305969  acc:  1.0\n","ep: 7  bt: 640  loss: 0.0011381178122499714  acc:  1.0\n","ep: 7  bt: 660  loss: 0.24648998094641644  acc:  0.375\n","ep: 7  bt: 680  loss: 0.0008759351204270902  acc:  1.0\n","ep: 7  bt: 700  loss: 0.2976475383924401  acc:  0.359375\n","ep: 7  bt: 720  loss: 0.0009112677982319956  acc:  1.0\n","ep: 7  bt: 740  loss: 0.0007205223259718522  acc:  1.0\n","ep: 7  bt: 760  loss: 0.28996287221493927  acc:  0.4375\n","ep: 7  bt: 780  loss: 0.0006726099907056145  acc:  1.0\n","('<yavati>______________________', '<యవతి>_________________', '<యవవటి_________________')\n","('<padaviikaalam>_______________', '<పదవీకాలం>_____________', '<పదవీకాలం>_____________')\n","('<patrigaaru>__________________', '<పత్రిగారు>____________', '<పట్రిగారు>____________')\n","('<aaropanalatone>______________', '<ఆరోపణలతోనే>___________', '<ఆరోపనలతోనే>___________')\n","('<ushtakavasanni>______________', '<ఉష్టకవశాన్ని>_________', '<ఉష్టకవసన్నన>__________')\n","('<kashtamavutundi>_____________', '<కష్టమవుతుంది>_________', '<కష్టమవుతుంది>_________')\n","('<erparichina>_________________', '<ఏర్పరిచిన>____________', '<ఎర్పరిచిన>____________')\n","('<peddaellapuramlo>____________', '<పెద్దఎల్లాపురంలో>_____', '<పెద్దేళ్లపురంలో>______')\n","('<bhuuloekam>__________________', '<భూలోకం>_______________', '<భూలోకం>_______________')\n","('<vennamuddatoe>_______________', '<వెన్నముద్దతో>_________', '<వెన్నముడ్దతో>_________')\n","('<thyaagabuddhitho>____________', '<త్యాగబుద్ధితో>________', '<త్యాగబుద్తిత__________')\n","('<subharambhanne>______________', '<శుభారంభాన్నే>_________', '<సుభరంభన్నే>___________')\n","('<indonesian>__________________', '<ఇండోనేషియా>___________', '<ఇండోనేసియ్్___________')\n","('<chaalaarojulavutundi>________', '<చాలారోజులవుతుంది>_____', '<చాలారోజులవుతుంది>_____')\n","('<kesavaki>____________________', '<కేశవకి>_______________', '<కేసవకక>_______________')\n","('<mahabharatham>_______________', '<మహాభారతం>_____________', '<మహహభరతం>______________')\n","('<petteskunnattu>______________', '<పెట్టేసుకున్నట్టు>____', '<పెట్టేస్కున్నట్టు>____')\n","('<budokan>_____________________', '<బుడోకన్>______________', '<బుడోకా్్______________')\n","('<anubhavagnaanamtho>__________', '<అనుభవజ్ఞానంతో>________', '<అనుభవగ్ఞానంతో>________')\n","('<amgiikarimcha>_______________', '<అంగీకరించ>____________', '<అంగీకరించ>____________')\n","ep:  7  train acc: 0.69478515625  train loss: 0.15323408591636942  val acc: 0.394287109375  val loss: 0.2972309071084727\n","('<stylistku>___________________', '<స్టైలిస్ట్కు>_________', '<స్టీలిస్త్కు>_________')\n","('<namanam>_____________________', '<నమనం>_________________', '<నమనం>_________________')\n","('<kalparasakunna>______________', '<కల్పరసకున్న>__________', '<కల్పరసకున్న>__________')\n","('<chatukovadanikena>___________', '<చాటుకోవడానికేనా>______', '<చతుకోవాడననేే>_________')\n","('<ardhamayinatlu>______________', '<అర్ధమయినట్లు>_________', '<అర్థమయినట్లు>_________')\n","('<vaatimuulaana>_______________', '<వాటిమూలాన>____________', '<వాతిమూలాన>____________')\n","('<tokkem>______________________', '<తొక్కేం>______________', '<తొక్కేం_______________')\n","('<manamemaina>_________________', '<మనమేమైన>______________', '<మనమేమైన>______________')\n","('<wonderboard>_________________', '<వండర్బోర్డ్>__________', '<వాండరర్బోర్డ్_________')\n","('<talapostu>___________________', '<తలపోస్తూ>_____________', '<తలపోస్తు>_____________')\n","('<kammatanaanni>_______________', '<కమ్మతనాన్ని>__________', '<కమ్మతనాన్ని>__________')\n","('<panicheyakunna>______________', '<పనిచేయకున్నా>_________', '<పనిచేయకున్న>__________')\n","('<ennilloonuu>_________________', '<ఎన్నిల్లోనూ>__________', '<ఎన్నిల్లోనూ>__________')\n","('<draveebhootam>_______________', '<ద్రవీభూతం>____________', '<ద్రవీభోతం>____________')\n","('<bodipepai>___________________', '<బొడిపెపై>_____________', '<బోడిపెపై>_____________')\n","('<tamiletarulu>________________', '<తమిళేతరులు>___________', '<తమిలేటరులు>___________')\n","('<bongu>_______________________', '<బొంగు>________________', '<బొంగు>________________')\n","('<viyannaalo>__________________', '<వియన్నాలో>____________', '<వియన్నాలో>____________')\n","('<evokh>_______________________', '<ఎవోఖ్>________________', '<ఎవోఖ్>________________')\n","('<toorpudesa>__________________', '<తూర్పుదేశ>____________', '<తూర్పుడేస>____________')\n","ep: 8  bt: 0  loss: 0.28722746475883154  acc:  0.375\n","ep: 8  bt: 20  loss: 0.23157683662746265  acc:  0.53125\n","ep: 8  bt: 40  loss: 0.0008645959846351458  acc:  1.0\n","ep: 8  bt: 60  loss: 0.26386426842730976  acc:  0.359375\n","ep: 8  bt: 80  loss: 0.0010392347593670306  acc:  1.0\n","ep: 8  bt: 100  loss: 0.0006808022280102191  acc:  1.0\n","ep: 8  bt: 120  loss: 0.0011212454865808072  acc:  1.0\n","ep: 8  bt: 140  loss: 0.0009830159175655117  acc:  1.0\n","ep: 8  bt: 160  loss: 0.0008384285737638888  acc:  1.0\n","ep: 8  bt: 180  loss: 0.25843995550404425  acc:  0.4375\n","ep: 8  bt: 200  loss: 0.0010239324815895247  acc:  1.0\n","ep: 8  bt: 220  loss: 0.0008268089074155558  acc:  1.0\n","ep: 8  bt: 240  loss: 0.3460824385933254  acc:  0.390625\n","ep: 8  bt: 260  loss: 0.0007516013701324878  acc:  1.0\n","ep: 8  bt: 280  loss: 0.0006953170927970306  acc:  1.0\n","ep: 8  bt: 300  loss: 0.0005594677008364512  acc:  1.0\n","ep: 8  bt: 320  loss: 0.5649314548658289  acc:  0.296875\n","ep: 8  bt: 340  loss: 0.0010700512515461962  acc:  1.0\n","ep: 8  bt: 360  loss: 0.4078783366991126  acc:  0.28125\n","ep: 8  bt: 380  loss: 0.20471365555472995  acc:  0.53125\n","ep: 8  bt: 400  loss: 0.0009415203138538029  acc:  1.0\n","ep: 8  bt: 420  loss: 0.3050903444704802  acc:  0.46875\n","ep: 8  bt: 440  loss: 0.3236354330311651  acc:  0.375\n","ep: 8  bt: 460  loss: 0.29541548438694165  acc:  0.453125\n","ep: 8  bt: 480  loss: 0.2236552031143852  acc:  0.4375\n","ep: 8  bt: 500  loss: 0.26776502443396527  acc:  0.5\n","ep: 8  bt: 520  loss: 0.3610101782757303  acc:  0.453125\n","ep: 8  bt: 540  loss: 0.2266437696373981  acc:  0.46875\n","ep: 8  bt: 560  loss: 0.2394768466120181  acc:  0.46875\n","ep: 8  bt: 580  loss: 0.0010642702488795571  acc:  1.0\n","ep: 8  bt: 600  loss: 0.2381071007770041  acc:  0.5\n","ep: 8  bt: 620  loss: 0.2645376661549444  acc:  0.46875\n","ep: 8  bt: 640  loss: 0.24271502702132516  acc:  0.4375\n","ep: 8  bt: 660  loss: 0.0012932937268329704  acc:  1.0\n","ep: 8  bt: 680  loss: 0.0015691695329935653  acc:  1.0\n","ep: 8  bt: 700  loss: 0.2417189349298892  acc:  0.421875\n","ep: 8  bt: 720  loss: 0.368189231209133  acc:  0.328125\n","ep: 8  bt: 740  loss: 0.0008740821124418923  acc:  1.0\n","ep: 8  bt: 760  loss: 0.21151428637297257  acc:  0.546875\n","ep: 8  bt: 780  loss: 0.0008452713651501614  acc:  1.0\n","('<kommanu>_____________________', '<కొమ్మను>______________', '<కొమ్మను>______________')\n","('<satyalani>___________________', '<సత్యాలని>_____________', '<సత్యలలి>>_____________')\n","('<kramamulo>___________________', '<క్రమములో>_____________', '<క్రమములో>_____________')\n","('<veeroochitamgaa>_____________', '<వీరోచితంగా>___________', '<వీరూచితంగా>___________')\n","('<narasaiah>___________________', '<నరసయ్య>_______________', '<నరసాాహ>_______________')\n","('<thunakalu>___________________', '<తునకలు>_______________', '<తునకలు>_______________')\n","('<nilabedutunna>_______________', '<నిలబెడుతున్న>_________', '<నిలబెడుతున్న>_________')\n","('<nirvachimchaaru>_____________', '<నిర్వచించారు>_________', '<నిర్వచించారు>_________')\n","('<aha>_________________________', '<అహ>___________________', '<అహా>__________________')\n","('<himsanku>____________________', '<హింసంకు>______________', '<హింసంకు>______________')\n","('<hariharmahal>________________', '<హరిహరమహల్>____________', '<హరిహర్మహల్>___________')\n","('<daisee>______________________', '<డైసీ>_________________', '<డైసీ>_________________')\n","('<samudaaya>___________________', '<సముదాయ>_______________', '<సముదాయ>_______________')\n","('<svargamlo>___________________', '<స్వర్గంలో>____________', '<స్వర్గంలో>____________')\n","('<stratfordlo>_________________', '<స్ట్రాట్ఫోర్డ్లో>_____', '<స్ట్రత్ఫరర్్లో>_______')\n","('<aapaadistundi>_______________', '<ఆపాదిస్తుంది>_________', '<ఆపాడిస్తుంది>_________')\n","('<bodhimchataaniki>____________', '<బోధించటానికి>_________', '<బోధించటానికి>_________')\n","('<nachindhani>_________________', '<నచ్చిందని>____________', '<నచ్చింంనన>____________')\n","('<puraanhamulu>________________', '<పురాణములు>____________', '<పురాణములు>____________')\n","('<puusina>_____________________', '<పూసిన>________________', '<పూసిన>________________')\n","ep:  8  train acc: 0.69076171875  train loss: 0.15280676542270066  val acc: 0.416748046875  val loss: 0.28567312074744183\n","('<bhetini>_____________________', '<భేటిని>_______________', '<భేటిని>_______________')\n","('<chekura>_____________________', '<చేకూరా>_______________', '<చేకూరా>_______________')\n","('<vatiira>_____________________', '<వతీర>_________________', '<వతీర>_________________')\n","('<sambandhitamainave>__________', '<సంబంధితమైనవే>_________', '<సంబంధితమైనవే>_________')\n","('<penchanunnam>________________', '<పెంచనున్నాం>__________', '<పెంచనున్నాం>__________')\n","('<sponging>____________________', '<స్పాంగింగ్>___________', '<స్పాంగింగ్>___________')\n","('<eetivi>______________________', '<ఈటీవి>________________', '<ఈటీవి>________________')\n","('<eminences>___________________', '<ఎమినెన్స్>____________', '<ఎమినెన్స్>____________')\n","('<naralajeevitam>______________', '<నరాలజీవితం>___________', '<నరాలజీవితం>___________')\n","('<peekincheyali>_______________', '<పీకించేయాలి>__________', '<పీకించేయాలి>__________')\n","('<natinchagala>________________', '<నటించగల>______________', '<నటించగల>______________')\n","('<vellipoyindantu>_____________', '<వెళ్లిపోయిందంటూ>______', '<వెళ్లిపోయిందంటూ>______')\n","('<kallalayinatle>______________', '<కల్లలయినట్లే>_________', '<కల్లలయినట్లే>_________')\n","('<chejaripotaronani>___________', '<చేజారిపోతారోనని>______', '<చేజారిపోతారోనని>______')\n","('<vantivaalluu>________________', '<వంటివాళ్లూ>___________', '<వంటివాళ్లూ>___________')\n","('<avvanunnanu>_________________', '<అవ్వనున్నాను>_________', '<అవ్వనున్నాను>_________')\n","('<raayalaseemalanu>____________', '<రాయలసీమలను>___________', '<రాయలసీమలను>___________')\n","('<emvooyoolatoo>_______________', '<ఎంవోయూలతో>____________', '<ఎంవోయూలతో>____________')\n","('<bhraantulaloo>_______________', '<భ్రాంతులలో>___________', '<భ్రాంతులలో>___________')\n","('<chiruthaikkal>_______________', '<చిరుతైక్కల్>__________', '<చిరుతైక్కల్>__________')\n","ep: 9  bt: 0  loss: 0.0006999822900347087  acc:  1.0\n","ep: 9  bt: 20  loss: 0.0008014909437169199  acc:  1.0\n","ep: 9  bt: 40  loss: 0.00101912264590678  acc:  1.0\n","ep: 9  bt: 60  loss: 0.1944261426511018  acc:  0.46875\n","ep: 9  bt: 80  loss: 0.24744220401929773  acc:  0.328125\n","ep: 9  bt: 100  loss: 0.33985490384309186  acc:  0.421875\n","ep: 9  bt: 120  loss: 0.2733233700627866  acc:  0.390625\n","ep: 9  bt: 140  loss: 0.2672209532364555  acc:  0.328125\n","ep: 9  bt: 160  loss: 0.0009824939720008683  acc:  1.0\n","ep: 9  bt: 180  loss: 0.0007558652888173642  acc:  1.0\n","ep: 9  bt: 200  loss: 0.0009320799423300701  acc:  1.0\n","ep: 9  bt: 220  loss: 0.4168739318847656  acc:  0.453125\n","ep: 9  bt: 240  loss: 0.0004956787857024566  acc:  1.0\n","ep: 9  bt: 260  loss: 0.24534776936406674  acc:  0.453125\n","ep: 9  bt: 280  loss: 0.0008787242290766343  acc:  1.0\n","ep: 9  bt: 300  loss: 0.0006417036461441413  acc:  1.0\n","ep: 9  bt: 320  loss: 0.0007111739529215771  acc:  1.0\n","ep: 9  bt: 340  loss: 0.000892053317764531  acc:  1.0\n","ep: 9  bt: 360  loss: 0.22690913988196332  acc:  0.390625\n","ep: 9  bt: 380  loss: 0.0008009184637795324  acc:  1.0\n","ep: 9  bt: 400  loss: 0.2858817888342816  acc:  0.40625\n","ep: 9  bt: 420  loss: 0.33700246396272077  acc:  0.40625\n","ep: 9  bt: 440  loss: 0.33173726952594257  acc:  0.484375\n","ep: 9  bt: 460  loss: 0.20515450187351392  acc:  0.453125\n","ep: 9  bt: 480  loss: 0.0005559528489475665  acc:  1.0\n","ep: 9  bt: 500  loss: 0.261333880217179  acc:  0.390625\n","ep: 9  bt: 520  loss: 0.0010932705000690792  acc:  1.0\n","ep: 9  bt: 540  loss: 0.2898955137833305  acc:  0.34375\n","ep: 9  bt: 560  loss: 0.0006089877823124762  acc:  1.0\n","ep: 9  bt: 580  loss: 0.0006418970777936604  acc:  1.0\n","ep: 9  bt: 600  loss: 0.31236808196358057  acc:  0.328125\n","ep: 9  bt: 620  loss: 0.23728609085083008  acc:  0.421875\n","ep: 9  bt: 640  loss: 0.0011091881796069767  acc:  1.0\n","ep: 9  bt: 660  loss: 0.27264267465342645  acc:  0.40625\n","ep: 9  bt: 680  loss: 0.0006777046896193339  acc:  1.0\n","ep: 9  bt: 700  loss: 0.0008312419218861538  acc:  1.0\n","ep: 9  bt: 720  loss: 0.0007293809043324512  acc:  1.0\n","ep: 9  bt: 740  loss: 0.0006645480575768844  acc:  1.0\n","ep: 9  bt: 760  loss: 0.32150289286737854  acc:  0.421875\n","ep: 9  bt: 780  loss: 0.0006985556658195412  acc:  1.0\n","('<gauthama>____________________', '<గౌతమ>_________________', '<గౌతమ>_________________')\n","('<sthithulu>___________________', '<స్థితులు>_____________', '<స్థితులు>_____________')\n","('<kannadaloo>__________________', '<కన్నడలో>______________', '<కన్నదలో>______________')\n","('<writers>_____________________', '<రైటర్స్>______________', '<వ్రిటర్్్_____________')\n","('<todpadutundanatamlo>_________', '<తోడ్పడుతుందనటంలో>_____', '<తోడ్పడుతుందనటంలో>_____')\n","('<matladatamga>________________', '<మాట్లాడటంగా>__________', '<మాట్లాడతంగా>__________')\n","('<sikshana>____________________', '<శిక్షణ>_______________', '<సిక్షణ>_______________')\n","('<mahaadhevuni>________________', '<మహాదేవుని>____________', '<మహాధేవుని>____________')\n","('<saahiti>_____________________', '<సాహితి>_______________', '<సాహితి>_______________')\n","('<prananiki>___________________', '<ప్రాణానికి>___________', '<ప్రణనిిి>_____________')\n","('<kshetraalu>__________________', '<క్షేత్రాలు>___________', '<క్షేత్రాలు>___________')\n","('<nilustunnavaaru>_____________', '<నిలుస్తున్నవారు>______', '<నిలుస్తున్నవారు>______')\n","('<khtar>_______________________', '<ఖతార్>________________', '<ఖ్టార్>_______________')\n","('<cherache>____________________', '<చెరచే>________________', '<చేరచే>________________')\n","('<qongo>_______________________', '<కాంగో>________________', '<కొంగో>________________')\n","('<kalapadam>___________________', '<కలపడం>________________', '<కలపడం>________________')\n","('<eddu>________________________', '<ఎద్దు>________________', '<ఎడ్డు>________________')\n","('<pannunu>_____________________', '<పన్నును>______________', '<పన్నును>______________')\n","('<angeekarinchaadani>__________', '<అంగీకరించాడని>________', '<అంగీకరించాడని>________')\n","('<praveshinchagane>____________', '<ప్రవేశించగానే>________', '<ప్రవేషించగనే>_________')\n","ep:  9  train acc: 0.70951171875  train loss: 0.14193802078639203  val acc: 0.40185546875  val loss: 0.29707821555759595\n","('<guttoorulo>__________________', '<గుట్టూరులో>___________', '<గుట్టూరులో>___________')\n","('<sarusa>______________________', '<సరుస>_________________', '<సరుస>_________________')\n","('<anucharuraalaina>____________', '<అనుచరురాలైన>__________', '<అనుచరురాలైన>__________')\n","('<venukadagu>__________________', '<వెనుకడగు>_____________', '<వెనుకడగు>_____________')\n","('<naveekarinchabadinatlayite>__', '<నవీకరించబడినట్లయితే>__', '<నవీకరించబడినట్లయితే>__')\n","('<akkarlede>___________________', '<అక్కర్లేదే>___________', '<అక్కర్లేదే>___________')\n","('<namalinatlu>_________________', '<నమలినట్లు>____________', '<నమలినట్లు>____________')\n","('<sinimaalatoe>________________', '<సినిమాలతో>____________', '<సినిమాలతో>____________')\n","('<rudraveeryamu>_______________', '<రుద్రవీర్యము>_________', '<రుద్రవీర్యము>_________')\n","('<daarunamayindi>______________', '<దారుణమయింది>__________', '<దారుణమయింది>__________')\n","('<chadivanantu>________________', '<చదివానంటూ>____________', '<చదివానంటూ>____________')\n","('<smrutulalo>__________________', '<స్మృతులలో>____________', '<స్మృతులలో>____________')\n","('<siisaalatoe>_________________', '<సీసాలతో>______________', '<సీసాలతో>______________')\n","('<srityaagaraayagaanasabhalo>__', '<శ్రీత్యాగరాయగానసభలో>__', '<శ్రీత్యాగరాయగానసభలో>__')\n","('<muchatinchukuntam>___________', '<ముచ్చటించుకుంటాం>_____', '<ముచ్చటించుకుంటాం>_____')\n","('<chedipoyi>___________________', '<చెడిపోయి>_____________', '<చెడిపోయి>_____________')\n","('<chuttukuntundano>____________', '<చుట్టుకుంటుందనో>______', '<చుట్టుకుంటుందనో>______')\n","('<hemimelia>___________________', '<హెమిమెలియా>___________', '<హెమిమెలియా>___________')\n","('<niluvaristundane>____________', '<నిలువరిస్తుందనే>______', '<నిలువరిస్తుందనే>______')\n","('<bhaashaaparignaanamto>_______', '<భాషాపరిజ్ఞానంతో>______', '<భాషాపరిజ్ఞానంతో>______')\n","ep: 10  bt: 0  loss: 0.0021687392955241  acc:  1.0\n","ep: 10  bt: 20  loss: 0.1776602371879246  acc:  0.5625\n","ep: 10  bt: 40  loss: 0.0013204135162674863  acc:  0.984375\n","ep: 10  bt: 60  loss: 0.27316591014032776  acc:  0.40625\n","ep: 10  bt: 80  loss: 0.0008483349305132161  acc:  1.0\n","ep: 10  bt: 100  loss: 0.2618121686189071  acc:  0.421875\n","ep: 10  bt: 120  loss: 0.4001489722210428  acc:  0.28125\n","ep: 10  bt: 140  loss: 0.23226920418117358  acc:  0.46875\n","ep: 10  bt: 160  loss: 0.3196445548016092  acc:  0.421875\n","ep: 10  bt: 180  loss: 0.000528483206163282  acc:  1.0\n","ep: 10  bt: 200  loss: 0.0007120387871628222  acc:  1.0\n","ep: 10  bt: 220  loss: 0.3351743739584218  acc:  0.4375\n","ep: 10  bt: 240  loss: 0.0008370450333408687  acc:  1.0\n","ep: 10  bt: 260  loss: 0.0006413535903329434  acc:  1.0\n","ep: 10  bt: 280  loss: 0.21460667900417163  acc:  0.515625\n","ep: 10  bt: 300  loss: 0.0006109061043547547  acc:  1.0\n","ep: 10  bt: 320  loss: 0.2088353737540867  acc:  0.53125\n","ep: 10  bt: 340  loss: 0.24373740735261337  acc:  0.4375\n","ep: 10  bt: 360  loss: 0.2662712594737177  acc:  0.421875\n","ep: 10  bt: 380  loss: 0.0006934382507334585  acc:  1.0\n","ep: 10  bt: 400  loss: 0.35571517115053924  acc:  0.4375\n","ep: 10  bt: 420  loss: 0.0008647371407436287  acc:  1.0\n","ep: 10  bt: 440  loss: 0.24847198569256326  acc:  0.46875\n","ep: 10  bt: 460  loss: 0.000772881119147591  acc:  1.0\n","ep: 10  bt: 480  loss: 0.17076852010644  acc:  0.5625\n","ep: 10  bt: 500  loss: 0.22309954270072604  acc:  0.46875\n","ep: 10  bt: 520  loss: 0.001300461995212928  acc:  1.0\n","ep: 10  bt: 540  loss: 0.2456794614377229  acc:  0.453125\n","ep: 10  bt: 560  loss: 0.26149874148161517  acc:  0.40625\n","ep: 10  bt: 580  loss: 0.321151007776675  acc:  0.40625\n","ep: 10  bt: 600  loss: 0.0006424295513526253  acc:  1.0\n","ep: 10  bt: 620  loss: 0.26905634092248004  acc:  0.328125\n","ep: 10  bt: 640  loss: 0.2629994309466818  acc:  0.46875\n","ep: 10  bt: 660  loss: 0.0010296968014343925  acc:  1.0\n","ep: 10  bt: 680  loss: 0.002828253676062045  acc:  0.984375\n","ep: 10  bt: 700  loss: 0.0006416742892368981  acc:  1.0\n","ep: 10  bt: 720  loss: 0.0006596265036774719  acc:  1.0\n","ep: 10  bt: 740  loss: 0.0007265580251165059  acc:  1.0\n","ep: 10  bt: 760  loss: 0.0006796540287525758  acc:  1.0\n","ep: 10  bt: 780  loss: 0.2776700932046641  acc:  0.40625\n","('<vikrayinchaaranedi>__________', '<విక్రయించారనేది>______', '<విక్రయించారనేది>______')\n","('<chepattaalo>_________________', '<చేపట్టాలో>____________', '<చెపట్టాలో>____________')\n","('<nirvahisthoo>________________', '<నిర్వహిస్తూ>__________', '<నిర్వహిస్తో>__________')\n","('<varadhiga>___________________', '<వారధిగా>______________', '<వరధధి>>_______________')\n","('<sabdavegaanni>_______________', '<శబ్దవేగాన్ని>_________', '<సబ్దవేగాన్ని>_________')\n","('<rahasyalu>___________________', '<రహస్యాలు>_____________', '<రహస్యలు>______________')\n","('<kottave>_____________________', '<కొత్తవే>______________', '<కొట్టావ>>_____________')\n","('<rajyapu>_____________________', '<రాజ్యపు>______________', '<రాజ్యపు>______________')\n","('<amalulokivastundi>___________', '<అమలులోకివస్తుంది>_____', '<అమలులోకివస్తుంది>_____')\n","('<aadamtho>____________________', '<ఆడంతో>________________', '<ఆడంతో>________________')\n","('<thaamu>______________________', '<తాము>_________________', '<తాము>_________________')\n","('<yaantrikangaa>_______________', '<యాంత్రికంగా>__________', '<యాంత్రికంగా>__________')\n","('<indooneeshiyaa>______________', '<ఇండోనేషియా>___________', '<ఇందూనేషియా>___________')\n","('<nuuthi>______________________', '<నూతి>_________________', '<నూతి>_________________')\n","('<vidyudaavesham>______________', '<విద్యుదావేశం>_________', '<విడ్యుడావేశం>_________')\n","('<intilooni>___________________', '<ఇంటిలోని>_____________', '<ఇంటిలోని>_____________')\n","('<anugrahinchaadu>_____________', '<అనుగ్రహించాడు>________', '<అనుగ్రహించాడు>________')\n","('<disaga>______________________', '<దిశగా>________________', '<దిసాగా>_______________')\n","('<aandhrulu>___________________', '<ఆంధ్రులు>_____________', '<ఆంధ్రులు>_____________')\n","('<telanganalooni>______________', '<తెలంగాణలోని>__________', '<తెలంగనలోని>___________')\n","ep:  10  train acc: 0.6938671875  train loss: 0.1508536147000267  val acc: 0.38818359375  val loss: 0.2840192628943402\n","('<kikimore>____________________', '<కికిమోర్>_____________', '<కికిమోర్>_____________')\n","('<visirindante>________________', '<విసిరిందంటే>__________', '<విసిరిందంటే>__________')\n","('<assons>______________________', '<అస్సన్స్>_____________', '<అస్సన్స్>_____________')\n","('<paryatinchinappudu>__________', '<పర్యటించినప్పుడు>_____', '<పర్యటించినప్పుడు>_____')\n","('<veyali>______________________', '<వేయలి>________________', '<వేయలి>________________')\n","('<bodhapadadanna>______________', '<బోధపడదన్న>____________', '<బోధపడదన్న>____________')\n","('<beradumeeda>_________________', '<బెరడుమీద>_____________', '<బెరడుమీద>_____________')\n","('<manayandundagaa>_____________', '<మనయందుండగా>___________', '<మనయందుండగా>___________')\n","('<matladukovatamante>__________', '<మాట్లాడుకోవటమంటే>_____', '<మాట్లాడుకోవటమంటే>_____')\n","('<gupteekarinchani>____________', '<గుప్తీకరించని>________', '<గుప్తీకరించని>________')\n","('<maleedalu>___________________', '<మలీదలు>_______________', '<మలీదలు>_______________')\n","('<rangaraavuku>________________', '<రంగరావుకు>____________', '<రంగరావుకు>____________')\n","('<rooddupaigaanee>_____________', '<రోడ్డుపైగానీ>_________', '<రోడ్డుపైగానీ>_________')\n","('<raashtrapatikunna>___________', '<రాష్ట్రపతికున్న>______', '<రాష్ట్రపతికున్న>______')\n","('<srushtimpabade>______________', '<సృష్టింపబడే>__________', '<సృష్టింపబడే>__________')\n","('<teesukuraavaddanii>__________', '<తీసుకురావద్దనీ>_______', '<తీసుకురావద్దనీ>_______')\n","('<vellivachhe>_________________', '<వెళ్ళివచ్చే>__________', '<వెళ్ళివచ్చే>__________')\n","('<vijrumbhinchestaadu>_________', '<విజృంభించేస్తాడు>_____', '<విజృంభించేస్తాడు>_____')\n","('<aatmaabhimaanaalaku>_________', '<ఆత్మాభిమానాలకు>_______', '<ఆత్మాభిమానాలకు>_______')\n","('<manopai>_____________________', '<మనోపై>________________', '<మనోపై>________________')\n","ep: 11  bt: 0  loss: 0.0006204876319869705  acc:  1.0\n","ep: 11  bt: 20  loss: 0.0006975661963224411  acc:  1.0\n","ep: 11  bt: 40  loss: 0.3803386273591415  acc:  0.359375\n","ep: 11  bt: 60  loss: 0.0007216242020544798  acc:  1.0\n","ep: 11  bt: 80  loss: 0.25699491086213483  acc:  0.484375\n","ep: 11  bt: 100  loss: 0.0007107554246550021  acc:  1.0\n","ep: 11  bt: 120  loss: 0.2640840903572414  acc:  0.421875\n","ep: 11  bt: 140  loss: 0.25854322184687073  acc:  0.484375\n","ep: 11  bt: 160  loss: 0.0012098703857349312  acc:  1.0\n","ep: 11  bt: 180  loss: 0.000554620328804721  acc:  1.0\n","ep: 11  bt: 200  loss: 0.0007375626946273057  acc:  1.0\n","ep: 11  bt: 220  loss: 0.000691919709029405  acc:  1.0\n","ep: 11  bt: 240  loss: 0.26143766486126446  acc:  0.453125\n","ep: 11  bt: 260  loss: 0.22717326620350714  acc:  0.4375\n","ep: 11  bt: 280  loss: 0.24256503063699472  acc:  0.40625\n","ep: 11  bt: 300  loss: 0.269433187401813  acc:  0.484375\n","ep: 11  bt: 320  loss: 0.0009029293027908906  acc:  1.0\n","ep: 11  bt: 340  loss: 0.27679636167443317  acc:  0.375\n","ep: 11  bt: 360  loss: 0.000552027648233849  acc:  1.0\n","ep: 11  bt: 380  loss: 0.32752202904742694  acc:  0.328125\n","ep: 11  bt: 400  loss: 0.2821830873904021  acc:  0.515625\n","ep: 11  bt: 420  loss: 0.22191545237665591  acc:  0.484375\n","ep: 11  bt: 440  loss: 0.29282115853351093  acc:  0.46875\n","ep: 11  bt: 460  loss: 0.0007748032879570256  acc:  1.0\n","ep: 11  bt: 480  loss: 0.24334451426630435  acc:  0.484375\n","ep: 11  bt: 500  loss: 0.000564550414033558  acc:  1.0\n","ep: 11  bt: 520  loss: 0.23959512295930283  acc:  0.5\n","ep: 11  bt: 540  loss: 0.0008759087194567142  acc:  1.0\n","ep: 11  bt: 560  loss: 0.0005650980721997178  acc:  1.0\n","ep: 11  bt: 580  loss: 0.00076047007156455  acc:  1.0\n","ep: 11  bt: 600  loss: 0.0007233243111682975  acc:  1.0\n","ep: 11  bt: 620  loss: 0.355675863183063  acc:  0.375\n","ep: 11  bt: 640  loss: 0.1761471292246943  acc:  0.515625\n","ep: 11  bt: 660  loss: 0.0006106368306538333  acc:  1.0\n","ep: 11  bt: 680  loss: 0.0016867544340050738  acc:  0.984375\n","ep: 11  bt: 700  loss: 0.22112933449123218  acc:  0.53125\n","ep: 11  bt: 720  loss: 0.0009410778951385747  acc:  1.0\n","ep: 11  bt: 740  loss: 0.24316681986269745  acc:  0.515625\n","ep: 11  bt: 760  loss: 0.1630782873734184  acc:  0.53125\n","ep: 11  bt: 780  loss: 0.2579884736434273  acc:  0.46875\n","('<pondaelaa>___________________', '<పొందేలా>______________', '<పొందేలా>______________')\n","('<abhimaanulapai>______________', '<అభిమానులపై>___________', '<అభిమానులపై>___________')\n","('<sabram>______________________', '<సబ్రామ్>______________', '<సబ్రంం________________')\n","('<abadhdhaalu>_________________', '<అబద్ధాలు>_____________', '<అబధధధాలు>_____________')\n","('<nirviryam>___________________', '<నిర్వీర్యం>___________', '<నిర్విర్యం>___________')\n","('<afghanistan>_________________', '<ఆఫ్ఘనిస్తాన్>_________', '<అఫ్గనిస్తాన్్_________')\n","('<lekkalanu>___________________', '<లెక్కలను>_____________', '<లెక్కలను>_____________')\n","('<tibust>______________________', '<టిబస్ట్>______________', '<టిబుస్ట్>_____________')\n","('<trisuulamtoe>________________', '<త్రిశూలంతో>___________', '<త్రిసూలంతో>___________')\n","('<roopamulaku>_________________', '<రూపములకు>_____________', '<రూపములకు>_____________')\n","('<vijayaalaku>_________________', '<విజయాలకు>_____________', '<విజయాలకు>_____________')\n","('<kaada>_______________________', '<కాద>__________________', '<కాడ>__________________')\n","('<veettha>_____________________', '<వేత్త>________________', '<వీట్థ>________________')\n","('<abhimanulaku>________________', '<అభిమానులకు>___________', '<అభిమనుులకుు___________')\n","('<taramgaana>__________________', '<తరంగాన>_______________', '<తరంగాన>_______________')\n","('<anaitikamgaa>________________', '<అనైతికంగా>____________', '<అనైతికంగా>____________')\n","('<frameworkto>_________________', '<ఫ్రేమ్వర్క్తో>________', '<ఫ్రామవవ్్్ోోో_________')\n","('<bheeshmunni>_________________', '<భీష్మునికి>___________', '<భీష్మున్నిని__________')\n","('<noppentani>__________________', '<నొప్పేంటని>___________', '<నోప్పెంటాని>__________')\n","('<praanthamuloni>______________', '<ప్రాంతములోని>_________', '<ప్రాంతములోని>_________')\n","ep:  11  train acc: 0.7166796875  train loss: 0.13522191137404418  val acc: 0.34228515625  val loss: 0.34791150300399115\n","('<teesukuntunnavanti>__________', '<తీసుకుంటున్నవంటి>_____', '<తీసుకుంటున్నవంటి>_____')\n","('<pramaadamunaku>______________', '<ప్రమాదమునకు>__________', '<ప్రమాదమునకు>__________')\n","('<kanchenjunga>________________', '<కాంచనగంగా>____________', '<కాంచనగంగా>____________')\n","('<praarambhistaana>____________', '<ప్రారంభిస్తాన>________', '<ప్రారంభిస్తాన>________')\n","('<hatyaachaaraalaki>___________', '<హత్యాచారాలకి>_________', '<హత్యాచారాలకి>_________')\n","('<kurchobettukova>_____________', '<కూర్చోబెట్టుకోవ>______', '<కూర్చోబెట్టుకోవ>______')\n","('<saggurti>____________________', '<సగ్గుర్తి>____________', '<సగ్గుర్తి>____________')\n","('<dedorising>__________________', '<డీడొరైజింగ్>__________', '<డీడొరైజింగ్>__________')\n","('<cheyistaani>_________________', '<చేయిస్తాని>___________', '<చేయిస్తాని>___________')\n","('<paritapistuntam>_____________', '<పరితపిస్తుంటాం>_______', '<పరితపిస్తుంటాం>_______')\n","('<ghnaanaamsaalu>______________', '<జ్ఞానాంశాలు>__________', '<జ్ఞానాంశాలు>__________')\n","('<nadeenadamulanu>_____________', '<నదీనదములను>___________', '<నదీనదములను>___________')\n","('<malupullanti>________________', '<మలుపుల్లాంటి>_________', '<మలుపుల్లాంటి>_________')\n","('<samakuurcheevaariki>_________', '<సమకూర్చేవారికి>_______', '<సమకూర్చేవారికి>_______')\n","('<stakov>______________________', '<స్టకోవ్>______________', '<స్టకోవ్>______________')\n","('<ruupomdimchabadavachchu>_____', '<రూపొందించబడవచ్చు>_____', '<రూపొందించబడవచ్చు>_____')\n","('<niluvaristundane>____________', '<నిలువరిస్తుందనే>______', '<నిలువరిస్తుందనే>______')\n","('<egaresevaaru>________________', '<ఎగరేసేవారు>___________', '<ఎగరేసేవారు>___________')\n","('<ankella>_____________________', '<అంకెళ్ళ>______________', '<అంకెళ్ళ>______________')\n","('<pravesapettadamo>____________', '<ప్రవేశపెట్టడమో>_______', '<ప్రవేశపెట్టడమో>_______')\n","ep: 12  bt: 0  loss: 0.0046830047731814175  acc:  0.984375\n","ep: 12  bt: 20  loss: 0.2580597504325535  acc:  0.484375\n","ep: 12  bt: 40  loss: 0.3096896461818529  acc:  0.5\n","ep: 12  bt: 60  loss: 0.0006036856414183327  acc:  1.0\n","ep: 12  bt: 80  loss: 0.2748111227284307  acc:  0.46875\n","ep: 12  bt: 100  loss: 0.0016016999016637387  acc:  0.984375\n","ep: 12  bt: 120  loss: 0.0006609814160543939  acc:  1.0\n","ep: 12  bt: 140  loss: 0.2811184966045877  acc:  0.3125\n","ep: 12  bt: 160  loss: 0.0008365701398123865  acc:  1.0\n","ep: 12  bt: 180  loss: 0.0004817960459900939  acc:  1.0\n","ep: 12  bt: 200  loss: 0.000500676343622415  acc:  1.0\n","ep: 12  bt: 220  loss: 0.0006408682497947112  acc:  1.0\n","ep: 12  bt: 240  loss: 0.3607606887817383  acc:  0.328125\n","ep: 12  bt: 260  loss: 0.19888801160066025  acc:  0.40625\n","ep: 12  bt: 280  loss: 0.00042805840949649394  acc:  1.0\n","ep: 12  bt: 300  loss: 0.0005683505340762761  acc:  1.0\n","ep: 12  bt: 320  loss: 0.22709935644398566  acc:  0.515625\n","ep: 12  bt: 340  loss: 0.0005068202786471533  acc:  1.0\n","ep: 12  bt: 360  loss: 0.0003986156181148861  acc:  1.0\n","ep: 12  bt: 380  loss: 0.21785537056300952  acc:  0.453125\n","ep: 12  bt: 400  loss: 0.26302308621613874  acc:  0.421875\n","ep: 12  bt: 420  loss: 0.2827268476071565  acc:  0.453125\n","ep: 12  bt: 440  loss: 0.23651176950205927  acc:  0.453125\n","ep: 12  bt: 460  loss: 0.26237937678461487  acc:  0.390625\n","ep: 12  bt: 480  loss: 0.00048124211151962697  acc:  1.0\n","ep: 12  bt: 500  loss: 0.34627203319383704  acc:  0.453125\n","ep: 12  bt: 520  loss: 0.22001280991927438  acc:  0.453125\n","ep: 12  bt: 540  loss: 0.22233473736306894  acc:  0.453125\n","ep: 12  bt: 560  loss: 0.19432161165320355  acc:  0.5\n","ep: 12  bt: 580  loss: 0.20654491756273352  acc:  0.40625\n","ep: 12  bt: 600  loss: 0.31047872875047766  acc:  0.515625\n","ep: 12  bt: 620  loss: 0.5634508547575577  acc:  0.265625\n","ep: 12  bt: 640  loss: 0.2615748695705248  acc:  0.46875\n","ep: 12  bt: 660  loss: 0.19154078027476434  acc:  0.421875\n","ep: 12  bt: 680  loss: 0.0007935950289601865  acc:  1.0\n","ep: 12  bt: 700  loss: 0.0007438727695008982  acc:  1.0\n","ep: 12  bt: 720  loss: 0.0005057497031014899  acc:  1.0\n","ep: 12  bt: 740  loss: 0.004730409902075063  acc:  0.96875\n","ep: 12  bt: 760  loss: 0.6429640313853389  acc:  0.109375\n","ep: 12  bt: 780  loss: 0.25551136680271314  acc:  0.390625\n","('<criya>_______________________', '<క్రియ>________________', '<క్రియ>________________')\n","('<denish>______________________', '<డేనిష్>_______________', '<డినిష్>_______________')\n","('<odilo>_______________________', '<ఒడిలో>________________', '<ఓడిలో>________________')\n","('<aamoedimchadam>______________', '<ఆమోదించడం>____________', '<ఆమోదించడం>____________')\n","('<chaduvutunaapudu>____________', '<చదువుతున్నపుడు>_______', '<చదువుతునాపుడు>________')\n","('<veetthalu>___________________', '<వేత్తలు>______________', '<వీట్థలు>______________')\n","('<gautamuniki>_________________', '<గౌతమునికి>____________', '<గౌతమునికి>____________')\n","('<paripakvata>_________________', '<పరిపక్వత>_____________', '<పరిపప్వట>_____________')\n","('<dhooda>______________________', '<దూడ>__________________', '<ధూడ>__________________')\n","('<paatistunnaaranii>___________', '<పాటిస్తున్నారనీ>______', '<పాటిస్తున్నారనీ>______')\n","('<atmarakshana>________________', '<ఆత్మరక్షణ>____________', '<అత్మరక్షణ>____________')\n","('<sekarinchenduku>_____________', '<సేకరించేందుకు>________', '<సెకరించేందుకు>________')\n","('<migraneur>___________________', '<మైగ్రేన్యుర్>_________', '<మిగ్రానూర్>___________')\n","('<aalayamlooki>________________', '<ఆలయంలోకి>_____________', '<ఆలయంలోకి>_____________')\n","('<ugravaadulatoonuu>___________', '<ఉగ్రవాదులతోనూ>________', '<ఉగ్రవాదులతోనూ>________')\n","('<prajektullo>_________________', '<ప్రాజెక్టుల్లో>_______', '<ప్రజెక్టుల్లో>________')\n","('<sinnappatikelle>_____________', '<సిన్నప్పటికెల్లే>_____', '<సిన్నప్పటికేె్లేేే>___')\n","('<marathwada>__________________', '<మరాఠ్వాడ>_____________', '<మరథ్వాద>>_____________')\n","('<praeminchu>__________________', '<ప్రేమించు>____________', '<ప్రేమించు>____________')\n","('<bhaaratheeyulaku>____________', '<భారతీయులకు>___________', '<భారథీయులకు>___________')\n","ep:  12  train acc: 0.72197265625  train loss: 0.13667298156970537  val acc: 0.36376953125  val loss: 0.31121666535087256\n","('<challabadadu>________________', '<చల్లబడదు>_____________', '<చల్లబడడు>_____________')\n","('<chuudaalana>_________________', '<చూడాలన>_______________', '<చూడాలన>_______________')\n","('<jeppevaaru>__________________', '<జెప్పేవారు>___________', '<జెప్పేవారు>___________')\n","('<southapton>__________________', '<సౌతాప్టన్>____________', '<సౌతప్టోన్>____________')\n","('<nittoorsutunnaaru>___________', '<నిట్టూర్సుతున్నారు>___', '<నిత్టూర్సుతున్నారు>___')\n","('<duolo>_______________________', '<డుయోలో>_______________', '<దూయోలో>_______________')\n","('<nadavanivvakapovadam>________', '<నడవనివ్వకపోవడం>_______', '<నడవనివ్వకపోవడం>_______')\n","('<vrataalakannanu>_____________', '<వ్రతాలకన్నను>_________', '<వ్రతాలకన్నను>_________')\n","('<carlovna>____________________', '<కార్లోవ్నా>___________', '<కార్లోవ్న>____________')\n","('<nnukoni>_____________________', '<న్నుకొని>_____________', '<న్నుకొని>_____________')\n","('<pheyilavvatamtho>____________', '<ఫెయిలవ్వటంతో>_________', '<ఫెయిలవ్వటంతో>_________')\n","('<swasthalamloonuu>____________', '<స్వస్థలంలోనూ>_________', '<స్వస్థలంలోనూ>_________')\n","('<upadhikalpana>_______________', '<ఉపాధికల్పనా>__________', '<ఉపధధికక్పన>___________')\n","('<cherchem>____________________', '<చేర్చేం>______________', '<చెర్చేం>______________')\n","('<nalugudu>____________________', '<నలుగుడు>______________', '<నలుగుడు>______________')\n","('<andukaite>___________________', '<అందుకైతే>_____________', '<అందుకైతే>_____________')\n","('<tiragabedutunnaa>____________', '<తిరగబెడుతున్నా>_______', '<తిరగబెడుతున్నా>_______')\n","('<siluvanuguurchina>___________', '<సిలువనుగూర్చిన>_______', '<సిలువనుగూరిచిన>_______')\n","('<astavastamaindi>_____________', '<అస్తవస్తమైంది>________', '<అస్తవస్తమైంది>________')\n","('<gyape>_______________________', '<గ్యాపే>_______________', '<గ్యపప>>_______________')\n","ep: 13  bt: 0  loss: 0.30900055429209833  acc:  0.453125\n","ep: 13  bt: 20  loss: 0.27890103796254034  acc:  0.375\n","ep: 13  bt: 40  loss: 0.22364205899445908  acc:  0.46875\n","ep: 13  bt: 60  loss: 0.17815330754155698  acc:  0.53125\n","ep: 13  bt: 80  loss: 0.0009632620798504871  acc:  1.0\n","ep: 13  bt: 100  loss: 0.000759076327085495  acc:  1.0\n","ep: 13  bt: 120  loss: 0.1495647845060929  acc:  0.578125\n","ep: 13  bt: 140  loss: 0.000847910895295765  acc:  1.0\n","ep: 13  bt: 160  loss: 0.2691190761068593  acc:  0.421875\n","ep: 13  bt: 180  loss: 0.34158992767333984  acc:  0.4375\n","ep: 13  bt: 200  loss: 0.27092469256857166  acc:  0.4375\n","ep: 13  bt: 220  loss: 0.1503336118615192  acc:  0.53125\n","ep: 13  bt: 240  loss: 0.0007427113697580669  acc:  1.0\n","ep: 13  bt: 260  loss: 0.0008518963080385457  acc:  1.0\n","ep: 13  bt: 280  loss: 0.0008445284936739051  acc:  1.0\n","ep: 13  bt: 300  loss: 0.0007143730054730954  acc:  1.0\n","ep: 13  bt: 320  loss: 0.0006734544168347898  acc:  1.0\n","ep: 13  bt: 340  loss: 0.0009052302364421928  acc:  1.0\n","ep: 13  bt: 360  loss: 0.32387499187303626  acc:  0.4375\n","ep: 13  bt: 380  loss: 0.0008906319575465244  acc:  1.0\n","ep: 13  bt: 400  loss: 0.000496881163638571  acc:  1.0\n","ep: 13  bt: 420  loss: 0.23751920202504034  acc:  0.5\n","ep: 13  bt: 440  loss: 0.0006419005601302437  acc:  1.0\n","ep: 13  bt: 460  loss: 0.16294446198836618  acc:  0.546875\n","ep: 13  bt: 480  loss: 0.21684781364772632  acc:  0.515625\n","ep: 13  bt: 500  loss: 0.15856285717176355  acc:  0.546875\n","ep: 13  bt: 520  loss: 0.00048445983101492345  acc:  1.0\n","ep: 13  bt: 540  loss: 0.0015059589691784072  acc:  0.984375\n","ep: 13  bt: 560  loss: 0.20754820367564325  acc:  0.40625\n","ep: 13  bt: 580  loss: 0.00047404914284529894  acc:  1.0\n","ep: 13  bt: 600  loss: 0.0005593687781821127  acc:  1.0\n","ep: 13  bt: 620  loss: 0.2022902239923892  acc:  0.484375\n","ep: 13  bt: 640  loss: 0.2166222904039466  acc:  0.453125\n","ep: 13  bt: 660  loss: 0.1656536744988483  acc:  0.515625\n","ep: 13  bt: 680  loss: 0.27473182263581647  acc:  0.375\n","ep: 13  bt: 700  loss: 0.26105012064394745  acc:  0.4375\n","ep: 13  bt: 720  loss: 0.3747689620308254  acc:  0.3125\n","ep: 13  bt: 740  loss: 0.0007957200641217439  acc:  1.0\n","ep: 13  bt: 760  loss: 0.0015692777283813643  acc:  0.984375\n","ep: 13  bt: 780  loss: 0.0008673584493605987  acc:  1.0\n","('<chaeta>______________________', '<చేట>__________________', '<చేట>__________________')\n","('<chimpanje>___________________', '<చింపాంజీ>_____________', '<చింపనజజ>______________')\n","('<chesthunnadani>______________', '<చేస్తున్నాడని>________', '<చేస్తున్నదని>_________')\n","('<mugimputhone>________________', '<ముగింపుతోనే>__________', '<ముగింపుతోనే>__________')\n","('<anataaneke>__________________', '<అనటానికి>_____________', '<అనటానేకే>_____________')\n","('<vilinamayi>__________________', '<విలీనమయి>_____________', '<విలినమయి>_____________')\n","('<samrakshinchadam>____________', '<సంరక్షించడం>__________', '<సంరక్షించడం>__________')\n","('<kaada>_______________________', '<కాద>__________________', '<కాడ>__________________')\n","('<kanpistunnayante>____________', '<కన్పిస్తున్నాయంటే>____', '<కన్పిస్తున్నాయంటే>____')\n","('<telisii>_____________________', '<తెలిసీ>_______________', '<తెలిసీ>_______________')\n","('<nilabadam>___________________', '<నిలబడం>_______________', '<నిలబడం>_______________')\n","('<padadam>_____________________', '<పడడం>_________________', '<పదడం>_________________')\n","('<penku>_______________________', '<పెంకు>________________', '<పెంకు>________________')\n","('<palakarinchukuntam>__________', '<పలకరించుకుంటాం>_______', '<పలకరించుకుంటాం>_______')\n","('<canadaa>_____________________', '<కెనడా>________________', '<కానాా>________________')\n","('<cheeragaa>___________________', '<చేరగా>________________', '<చీరగా>________________')\n","('<parishiilisthuu>_____________', '<పరిశీలిస్తూ>__________', '<పరిషీలిస్తూ>__________')\n","('<vaadynaa>____________________', '<వాడైనా>_______________', '<వాడ్యా>_______________')\n","('<koppu>_______________________', '<కొప్పు>_______________', '<కొప్పు>_______________')\n","('<stambhamu>___________________', '<స్తంభము>______________', '<స్తంభము>______________')\n","ep:  13  train acc: 0.726015625  train loss: 0.1266475418849301  val acc: 0.4140625  val loss: 0.2901544363602348\n","('<tapophalitamgaa>_____________', '<తపోఫలితంగా>___________', '<తపోఫలితంగా>___________')\n","('<maddatuneeyaledu>____________', '<మద్దతునీయలేదు>________', '<మద్దతునీయలేదు>________')\n","('<elzar>_______________________', '<ఎల్జార్>______________', '<ఎల్జార్>______________')\n","('<troplaku>____________________', '<ట్రోప్లకు>____________', '<ట్రోప్లకు>____________')\n","('<saapavruttaantaanni>_________', '<శాపవృత్తాంతాన్ని>_____', '<శాపవృత్తాంతాన్ని>_____')\n","('<sudhaaraanilatoo>____________', '<సుధారాణిలతో>__________', '<సుధారాణిలతో>__________')\n","('<streeps>_____________________', '<స్ట్రీప్స్>___________', '<స్ట్రీప్స్>___________')\n","('<karmamulaneringinavadu>______', '<కర్మములనెరింగినవాడు>__', '<కర్మములనెరింగినవాడు>__')\n","('<ottukondi>___________________', '<ఒత్తుకొంది>___________', '<ఒత్తుకొంది>___________')\n","('<balaparustondi>______________', '<బలపరుస్తోంది>_________', '<బలపరుస్తోంది>_________')\n","('<ippudema>____________________', '<ఇప్పుడేమ>_____________', '<ఇప్పుడేమ>_____________')\n","('<dimpadamtho>_________________', '<దింపడంతో>_____________', '<దింపడంతో>_____________')\n","('<veyagaladu>__________________', '<వేయగలదు>______________', '<వేయగలదు>______________')\n","('<netaji>______________________', '<నేతాజి>_______________', '<నేతాజి>_______________')\n","('<piduguraallaloonu>___________', '<పిడుగురాళ్ళలోను>______', '<పిడుగురాళ్ళలోను>______')\n","('<vachesinattunnayi>___________', '<వచ్చేసినట్టున్నాయి>___', '<వచ్చేసినట్టున్నాయి>___')\n","('<oppukovadamvalla>____________', '<ఒప్పుకోవడంవల్ల>_______', '<ఒప్పుకోవడంవల్ల>_______')\n","('<veedhivaakiliki>_____________', '<వీధివాకిలికి>_________', '<వీధివాకిలికి>_________')\n","('<gurtuka>_____________________', '<గుర్తుక>______________', '<గుర్తుక>______________')\n","('<moosevesi>___________________', '<మూసేవేసి>_____________', '<మూసేవేసి>_____________')\n","ep: 14  bt: 0  loss: 0.0011216818314531574  acc:  1.0\n","ep: 14  bt: 20  loss: 0.0009313602324413216  acc:  1.0\n","ep: 14  bt: 40  loss: 0.22465944290161133  acc:  0.453125\n","ep: 14  bt: 60  loss: 0.23643423163372537  acc:  0.484375\n","ep: 14  bt: 80  loss: 0.0004649523888593135  acc:  1.0\n","ep: 14  bt: 100  loss: 0.21629260933917502  acc:  0.515625\n","ep: 14  bt: 120  loss: 0.0011055507575688155  acc:  1.0\n","ep: 14  bt: 140  loss: 0.0007835952969996825  acc:  1.0\n","ep: 14  bt: 160  loss: 0.0007216017693281174  acc:  1.0\n","ep: 14  bt: 180  loss: 0.0008487264909174131  acc:  1.0\n","ep: 14  bt: 200  loss: 0.0005676875943722932  acc:  1.0\n","ep: 14  bt: 220  loss: 0.000536489300429821  acc:  1.0\n","ep: 14  bt: 240  loss: 0.299365002176036  acc:  0.453125\n","ep: 14  bt: 260  loss: 0.2566976132600204  acc:  0.453125\n","ep: 14  bt: 280  loss: 0.32065795815509296  acc:  0.328125\n","ep: 14  bt: 300  loss: 0.0006996038491311281  acc:  1.0\n","ep: 14  bt: 320  loss: 0.15072039935899817  acc:  0.5625\n","ep: 14  bt: 340  loss: 0.19431452129198157  acc:  0.421875\n","ep: 14  bt: 360  loss: 0.0017957688997621121  acc:  0.984375\n","ep: 14  bt: 380  loss: 0.19447349465411642  acc:  0.546875\n","ep: 14  bt: 400  loss: 0.216113049051036  acc:  0.515625\n","ep: 14  bt: 420  loss: 0.0004689516094715699  acc:  1.0\n","ep: 14  bt: 440  loss: 0.0013825402635595074  acc:  1.0\n","ep: 14  bt: 460  loss: 0.24732293253359589  acc:  0.421875\n","ep: 14  bt: 480  loss: 0.001237631977900215  acc:  1.0\n","ep: 14  bt: 500  loss: 0.0006605745090738586  acc:  1.0\n","ep: 14  bt: 520  loss: 0.3557521156642748  acc:  0.40625\n","ep: 14  bt: 540  loss: 0.0006357477168026178  acc:  1.0\n","ep: 14  bt: 560  loss: 0.34298378488291864  acc:  0.4375\n","ep: 14  bt: 580  loss: 0.5210511166116466  acc:  0.140625\n","ep: 14  bt: 600  loss: 0.0009245039166315743  acc:  1.0\n","ep: 14  bt: 620  loss: 0.26122437352719513  acc:  0.40625\n","ep: 14  bt: 640  loss: 0.2728368925011676  acc:  0.546875\n","ep: 14  bt: 660  loss: 0.0010510285425445309  acc:  1.0\n","ep: 14  bt: 680  loss: 0.0007783375356508338  acc:  1.0\n","ep: 14  bt: 700  loss: 0.22091749440068784  acc:  0.578125\n","ep: 14  bt: 720  loss: 0.0012512896209955215  acc:  1.0\n","ep: 14  bt: 740  loss: 0.25993579366932745  acc:  0.421875\n","ep: 14  bt: 760  loss: 0.1862576318823773  acc:  0.53125\n","ep: 14  bt: 780  loss: 0.3043762082638948  acc:  0.390625\n","('<samakurchenduku>_____________', '<సమకూర్చేందుకు>________', '<సమకుర్చేందుకు>________')\n","('<tecchindani>_________________', '<తెచ్చిందని>___________', '<తెచ్చిందని>___________')\n","('<ritair>______________________', '<రిటైర్>_______________', '<రిటార్>_______________')\n","('<yavati>______________________', '<యవతి>_________________', '<యవవతి>________________')\n","('<bashan>______________________', '<బాషన్>________________', '<బాషన్>________________')\n","('<asthipanjaraalachae>_________', '<అస్థిపంజరాలచే>________', '<అస్థిపనజాాలచే>________')\n","('<oorugallulo>_________________', '<ఓరుగల్లులో>___________', '<ఊరుగల్లులో>___________')\n","('<raajyaanni>__________________', '<రాజ్యాన్ని>___________', '<రాజ్యాన్ని>___________')\n","('<asoemloe>____________________', '<అసోంలో>_______________', '<అసోంలో>_______________')\n","('<compleep>____________________', '<కంప్లీప్>_____________', '<కాంపప్ఫ్్_____________')\n","('<chacchaadu>__________________', '<చచ్చాడు>______________', '<చచ్చాడు>______________')\n","('<karol>_______________________', '<కారల్>________________', '<కాోోల>________________')\n","('<undipovadanike>______________', '<ఉండిపోవడానికే>________', '<ఉండిపోవడానికే>________')\n","('<sevikalandaru>_______________', '<సేవికలందరూ>___________', '<సేవికలందరు>___________')\n","('<vinyasaanni>_________________', '<విన్యాసాన్ని>_________', '<విన్యసాన్ని>__________')\n","('<mukta>_______________________', '<ముక్త>________________', '<ముక్త>________________')\n","('<scriining>___________________', '<స్క్రీనింగ్>__________', '<స్కిరీిింగ్>__________')\n","('<bellamkoda>__________________', '<బెల్లంకొడ>____________', '<బెల్లంకొడా____________')\n","('<mannana>_____________________', '<మన్నన>________________', '<మన్ననన________________')\n","('<sainyala>____________________', '<సైన్యాల>______________', '<సైన్యాల_______________')\n","ep:  14  train acc: 0.70935546875  train loss: 0.13589164904581946  val acc: 0.408447265625  val loss: 0.2881221978560738\n","('<susthiratala>________________', '<సుస్థిరతల>____________', '<సుస్థిరతల>____________')\n","('<maatakenta>__________________', '<మాటకెంత>______________', '<మాటకెంత>______________')\n","('<akattukovachannadi>__________', '<ఆకట్టుకోవచ్చన్నది>____', '<ఆకట్టుకోవచ్చన్నది>____')\n","('<chemorceptors>_______________', '<కెమోర్సెప్టర్స్>______', '<కెమోర్సెప్టర్స్>______')\n","('<deshaadhyakshulu>____________', '<దేశాధ్యక్షులు>________', '<దేశాధ్యక్షులు>________')\n","('<parabrahmaroopamaina>________', '<పరబ్రహ్మరూపమైన>_______', '<పరబ్రహ్మరూపమైన>_______')\n","('<deeskoni>____________________', '<దీస్కొని>_____________', '<దీస్కొని>_____________')\n","('<firaayincheshaadu>___________', '<ఫిరాయించేశాడు>________', '<ఫిరాయించేశాడు>________')\n","('<telchukovalsindi>____________', '<తేల్చుకోవాల్సింది>____', '<తేల్చుకోవాల్సింది>____')\n","('<nagarajuki>__________________', '<నాగరాజుకి>____________', '<నాగరాజుకి>____________')\n","('<konchempati>_________________', '<కొంచెంపాటి>___________', '<కొంచెంపాటి>___________')\n","('<cherupalli>__________________', '<చేరుపల్లి>____________', '<చేరుపల్లి>____________')\n","('<samaadhaanamicchinappudu>____', '<సమాధానమిచ్చినప్పుడు>__', '<సమాధానమిచ్చినప్పుడు>__')\n","('<orthropod>___________________', '<ఆర్థ్రోపోడ్>__________', '<ఆర్థ్రోపోడ్>__________')\n","('<laabhaalamota>_______________', '<లాభాలమోత>_____________', '<లాభాలమోత>_____________')\n","('<gummepa>_____________________', '<గుమ్మేప>______________', '<గుమ్మేప>______________')\n","('<muginchedi>__________________', '<ముగించేది>____________', '<ముగించేది>____________')\n","('<kottakodalni>________________', '<కొత్తకోడల్ని>_________', '<కొత్తకోడల్ని>_________')\n","('<dorakabuchukunnaru>__________', '<దొరకబుచ్చుకున్నారు>___', '<దొరకబుచ్చుకున్నారు>___')\n","('<porabadina>__________________', '<పొరబడిన>______________', '<పొరబడిన>______________')\n","ep: 15  bt: 0  loss: 0.0005956707603257636  acc:  1.0\n","ep: 15  bt: 20  loss: 0.0005315227596008259  acc:  1.0\n","ep: 15  bt: 40  loss: 0.22661254716956097  acc:  0.421875\n","ep: 15  bt: 60  loss: 0.0006204049062469732  acc:  1.0\n","ep: 15  bt: 80  loss: 0.0005254017027175945  acc:  1.0\n","ep: 15  bt: 100  loss: 0.0006679931817495304  acc:  1.0\n","ep: 15  bt: 120  loss: 0.0005319097848690075  acc:  1.0\n","ep: 15  bt: 140  loss: 0.0004404259278722431  acc:  1.0\n","ep: 15  bt: 160  loss: 0.31086670834085217  acc:  0.359375\n","ep: 15  bt: 180  loss: 0.25329042517620587  acc:  0.46875\n","ep: 15  bt: 200  loss: 0.1726915110712466  acc:  0.59375\n","ep: 15  bt: 220  loss: 0.0005716528014644333  acc:  1.0\n","ep: 15  bt: 240  loss: 0.24372382785962976  acc:  0.515625\n","ep: 15  bt: 260  loss: 0.17864832670792288  acc:  0.484375\n","ep: 15  bt: 280  loss: 0.23548795865929645  acc:  0.484375\n","ep: 15  bt: 300  loss: 0.0006538557455591533  acc:  1.0\n","ep: 15  bt: 320  loss: 0.2830598665320355  acc:  0.421875\n","ep: 15  bt: 340  loss: 0.00046214125240626544  acc:  1.0\n","ep: 15  bt: 360  loss: 0.0005930938717463742  acc:  1.0\n","ep: 15  bt: 380  loss: 0.00038743824900492376  acc:  1.0\n","ep: 15  bt: 400  loss: 0.0006254894011046575  acc:  1.0\n","ep: 15  bt: 420  loss: 0.0018242255825063457  acc:  0.984375\n","ep: 15  bt: 440  loss: 0.0005114110915557197  acc:  1.0\n","ep: 15  bt: 460  loss: 0.000991038978099823  acc:  1.0\n","ep: 15  bt: 480  loss: 0.2831322213877802  acc:  0.40625\n","ep: 15  bt: 500  loss: 0.24596429907757303  acc:  0.375\n","ep: 15  bt: 520  loss: 0.000654463413292947  acc:  1.0\n","ep: 15  bt: 540  loss: 0.30328485240106995  acc:  0.4375\n","ep: 15  bt: 560  loss: 0.33144065608148987  acc:  0.390625\n","ep: 15  bt: 580  loss: 0.0012203662291817043  acc:  1.0\n","ep: 15  bt: 600  loss: 0.0006356671371537706  acc:  1.0\n","ep: 15  bt: 620  loss: 0.20755135494729746  acc:  0.484375\n","ep: 15  bt: 640  loss: 0.20921806667162024  acc:  0.484375\n","ep: 15  bt: 660  loss: 0.000934912863632907  acc:  1.0\n","ep: 15  bt: 680  loss: 0.2324987909068232  acc:  0.5\n","ep: 15  bt: 700  loss: 0.0005169137907416924  acc:  1.0\n","ep: 15  bt: 720  loss: 0.25790355516516644  acc:  0.421875\n","ep: 15  bt: 740  loss: 0.000712934800464174  acc:  1.0\n","ep: 15  bt: 760  loss: 0.17716113380763843  acc:  0.5625\n","ep: 15  bt: 780  loss: 0.7340243795643682  acc:  0.1875\n","('<sastrachikitsaa>_____________', '<శస్త్రచికిత్సా>_______', '<సస్త్రచికిత్సా>_>_____')\n","('<admissionla>_________________', '<అడ్మిషన్ల>____________', '<అడ్మిస్సనన్>__________')\n","('<poojinchee>__________________', '<పూజించే>______________', '<పూజించే>______________')\n","('<monei>_______________________', '<మనీ>__________________', '<మొనీ>_________________')\n","('<aenugu>______________________', '<ఏనుగు>________________', '<ఈనుగు>________________')\n","('<arcadi>______________________', '<ఆర్కాడి>______________', '<అర్కాిి>______________')\n","('<tweaked>_____________________', '<ట్వీక్డ్>_____________', '<ట్వెకెడ్>_____________')\n","('<veeroochithangaa>____________', '<వీరోచితంగా>___________', '<వీరోచితంగా>___________')\n","('<kant>________________________', '<కాంట్>________________', '<కాట్్>________________')\n","('<ashistunnaamu>_______________', '<ఆశిస్తున్నాము>________', '<అషిస్తున్నాము>________')\n","('<niroodhinchi>________________', '<నిరోధించి>____________', '<నిరూధించి>____________')\n","('<nandharini>__________________', '<నందరిని>______________', '<నంధరిని>______________')\n","('<sunnii>______________________', '<సున్నీ>_______________', '<సున్నీ>_______________')\n","('<bhoolookam>__________________', '<భూలోకం>_______________', '<భూలోకం>_______________')\n","('<samithi>_____________________', '<సమితి>________________', '<సమితి>________________')\n","('<maharoudra>__________________', '<మహారౌద్ర>_____________', '<మహాోౌ్రర>>____________')\n","('<aalayamlooni>________________', '<ఆలయంలోని>_____________', '<ఆలయంలోని>_____________')\n","('<aalayamloki>_________________', '<ఆలయంలోకి>_____________', '<ఆలయంలోకి>_____________')\n","('<amaayakudu>__________________', '<అమాయకుడు>_____________', '<అమాయకుడు>_____________')\n","('<uureeginchaaru>______________', '<ఊరేగించారు>___________', '<ఊరీగించారు>___________')\n","ep:  15  train acc: 0.73861328125  train loss: 0.12050249502611965  val acc: 0.32373046875  val loss: 0.36116948335067084\n","('<marichitini>_________________', '<మరిచితిని>____________', '<మరిచితిని>____________')\n","('<narapureddy>_________________', '<నారపురెడ్డి>__________', '<నరాూరెడ్డడ>>>_________')\n","('<nerpinchagalam>______________', '<నేర్పించగలం>__________', '<నెర్పించగలం>__________')\n","('<varshamla>___________________', '<వర్షంలా>______________', '<వర్షంల>>______________')\n","('<compoverternu>_______________', '<కంపోవర్టర్ను>_________', '<కంపోవేర్టేర్ను>_______')\n","('<venakaduge>__________________', '<వెనకడుగే>_____________', '<వెనకడుగే>_____________')\n","('<chaitanyaparchanunnarani>____', '<చైతన్యపర్చనున్నారని>__', '<చైతన్యపర్చనునననారనననిన')\n","('<kaparuluga>__________________', '<కాపరులుగా>____________', '<కాాులుుగాా>___________')\n","('<gundepagilinanta>____________', '<గుండెపగిలినంత>________', '<గుండెపగిలినంతా>_______')\n","('<tirumalaina>_________________', '<తిరుమలైన>_____________', '<తిరుమలైననా>___________')\n","('<shabdbhava>__________________', '<శబ్ద్భావ>_____________', '<షబ్్ావా>______________')\n","('<pakhtoons>___________________', '<పఖ్తూన్స్>____________', '<పఖ్టోన్స్>____________')\n","('<vikramasamha>________________', '<విక్రమసంహ>____________', '<విక్రమసమహహా___________')\n","('<prajaasanta>_________________', '<ప్రజాసంత>_____________', '<ప్రజాసంత>_____________')\n","('<jilaayitii>__________________', '<జిలాయితీ>_____________', '<జిలాయితీ>_____________')\n","('<lissi>_______________________', '<లిస్సి>_______________', '<లిస్సి>_______________')\n","('<pondavachuu>_________________', '<పొందవచ్చూ>____________', '<పొందవచూచూ>____________')\n","('<daihikamainadi>______________', '<దైహికమైనది>___________', '<దైహికమైనది>___________')\n","('<naathayogi>__________________', '<నాథయోగి>______________', '<నాతయోగి>______________')\n","('<dwipatra>____________________', '<ద్విపాత్ర>____________', '<ద్విపట్రరా>___________')\n","ep: 16  bt: 0  loss: 0.32597487905751105  acc:  0.328125\n","ep: 16  bt: 20  loss: 0.0013348521745723226  acc:  1.0\n","ep: 16  bt: 40  loss: 0.24257728327875552  acc:  0.46875\n","ep: 16  bt: 60  loss: 0.28493709149567975  acc:  0.4375\n","ep: 16  bt: 80  loss: 0.20658217305722443  acc:  0.34375\n","ep: 16  bt: 100  loss: 0.2117421108743419  acc:  0.578125\n","ep: 16  bt: 120  loss: 0.0010684899499882822  acc:  1.0\n","ep: 16  bt: 140  loss: 0.000539311450784621  acc:  1.0\n","ep: 16  bt: 160  loss: 0.0006348542950075606  acc:  1.0\n","ep: 16  bt: 180  loss: 0.2654438433439835  acc:  0.375\n","ep: 16  bt: 200  loss: 0.0005338368532450303  acc:  1.0\n","ep: 16  bt: 220  loss: 0.0008865255538536155  acc:  1.0\n","ep: 16  bt: 240  loss: 0.26820614026940387  acc:  0.4375\n","ep: 16  bt: 260  loss: 0.0005035927195263946  acc:  1.0\n","ep: 16  bt: 280  loss: 0.21331478201824686  acc:  0.484375\n","ep: 16  bt: 300  loss: 0.18329549872356912  acc:  0.46875\n","ep: 16  bt: 320  loss: 0.000681513839441797  acc:  1.0\n","ep: 16  bt: 340  loss: 0.2596977689991827  acc:  0.46875\n","ep: 16  bt: 360  loss: 0.0005819639191031456  acc:  1.0\n","ep: 16  bt: 380  loss: 0.0005309499557251515  acc:  1.0\n","ep: 16  bt: 400  loss: 0.0004100294216819431  acc:  1.0\n","ep: 16  bt: 420  loss: 0.17235396219336468  acc:  0.4375\n","ep: 16  bt: 440  loss: 0.21615028381347656  acc:  0.5625\n","ep: 16  bt: 460  loss: 0.26619701800139056  acc:  0.5\n","ep: 16  bt: 480  loss: 0.20732894151107126  acc:  0.34375\n","ep: 16  bt: 500  loss: 0.24851179122924805  acc:  0.375\n","ep: 16  bt: 520  loss: 0.24481333856997284  acc:  0.546875\n","ep: 16  bt: 540  loss: 0.2069934554721998  acc:  0.4375\n","ep: 16  bt: 560  loss: 0.19337032152258832  acc:  0.515625\n","ep: 16  bt: 580  loss: 0.2625031263931938  acc:  0.421875\n","ep: 16  bt: 600  loss: 0.20595577488774838  acc:  0.53125\n","ep: 16  bt: 620  loss: 0.2830357966215714  acc:  0.390625\n","ep: 16  bt: 640  loss: 0.27543756236200745  acc:  0.421875\n","ep: 16  bt: 660  loss: 0.22211483250493588  acc:  0.53125\n","ep: 16  bt: 680  loss: 0.24106089965156888  acc:  0.375\n","ep: 16  bt: 700  loss: 0.3658233310865319  acc:  0.375\n","ep: 16  bt: 720  loss: 0.0010196581158948982  acc:  1.0\n","ep: 16  bt: 740  loss: 0.00046448296178942144  acc:  1.0\n","ep: 16  bt: 760  loss: 0.2752690315246582  acc:  0.453125\n","ep: 16  bt: 780  loss: 0.0008462868306947792  acc:  1.0\n","('<soedaanu>____________________', '<సోడాను>_______________', '<సోదాను>_______________')\n","('<dvaadashi>___________________', '<ద్వాదశి>______________', '<ద్వాదశి>______________')\n","('<nirvaahaka>__________________', '<నిర్వాహక>_____________', '<నిర్వాహక>_____________')\n","('<varasalu>____________________', '<వరసలు>________________', '<వరసలు>>_______________')\n","('<rojulu>______________________', '<రోజులు>_______________', '<రోజులు>_______________')\n","('<veyalantu>___________________', '<వేయాలంటూ>_____________', '<వేయలలంట>______________')\n","('<saphaari>____________________', '<సఫారి>________________', '<సాథారి>_______________')\n","('<chaerani>____________________', '<చేరని>________________', '<చేరని>________________')\n","('<thippani>____________________', '<తిప్పని>______________', '<తిప్పని>______________')\n","('<australiato>_________________', '<ఆస్ట్రేలియాతో>________', '<ఆస్త్రలిితో>__________')\n","('<sanpadinchi>_________________', '<సంపాదించి>____________', '<సన్పదించి>____________')\n","('<beverly>_____________________', '<బెవర్లీ>______________', '<బెవర్లీ>______________')\n","('<gurayyam>____________________', '<గురయ్యాం>_____________', '<గురయ్యం>______________')\n","('<namuunaa>____________________', '<నమూనా>________________', '<నాూనా>________________')\n","('<pratinidhulato>______________', '<ప్రతినిధులతో>_________', '<ప్రతినిధులతో>_________')\n","('<deebating>___________________', '<డిబేటింగ్>____________', '<డీబాిింగ్>____________')\n","('<viiramgam>___________________', '<వీరంగం>_______________', '<వీరంగం>_______________')\n","('<gaambheeryam>________________', '<గాంభీర్యం>____________', '<గాంభీర్యం>____________')\n","('<jodinchinatlugaa>____________', '<జోడించినట్లుగా>_______', '<జొదించినట్లుగా>_______')\n","('<mahaajan>____________________', '<మహాజన్>_______________', '<మహాజన్>_______________')\n","ep:  16  train acc: 0.72521484375  train loss: 0.12353879255925955  val acc: 0.4033203125  val loss: 0.2814339347507643\n","('<tushtuvaari>_________________', '<తుష్టువారి>___________', '<తుష్టువారి>___________')\n","('<ashwino>_____________________', '<అశ్వినో>______________', '<అష్వినో>______________')\n","('<tikri>_______________________', '<తిక్రీ>_______________', '<తిక్రీ>_______________')\n","('<podavadamtopaatu>____________', '<పొడవడంతోపాటు>_________', '<పొదవడంతోపాటు>_________')\n","('<aropincharaya>_______________', '<ఆరోపించారాయ>__________', '<అరోపించారయ>___________')\n","('<soosaitiki>__________________', '<సొసైటీకి>_____________', '<సూసైతికి>_____________')\n","('<prakshepaalayyaayo>__________', '<ప్రక్షేపాలయ్యాయో>_____', '<ప్రక్షేపాలయ్యాయో>_____')\n","('<saastavretta>________________', '<శాస్తవ్రేత్త>_________', '<సాస్తవ్రేట్త>_________')\n","('<fluoride>____________________', '<ఫ్లూరైడ్>_____________', '<ఫ్లూరిడ్>_____________')\n","('<tambiki>_____________________', '<తంబికి>_______________', '<తంబిిి>_______________')\n","('<paerkonnaadani>______________', '<పేర్కొన్నాడని>________', '<పేర్కొన్నాడని>________')\n","('<vratamaacharinchaali>________', '<వ్రతమాచరించాలి>_______', '<వ్రతమాచరించాలి>_______')\n","('<rendellakomaru>______________', '<రెండేళ్లకోమారు>_______', '<రెండెళ్లకోమారు>_______')\n","('<nilustaanandi>_______________', '<నిలుస్తానంది>_________', '<నిలుస్తానంది>_________')\n","('<shraminchakundaane>__________', '<శ్రమించకుండానే>_______', '<శ్రమించకుండానే>_______')\n","('<srenulani>___________________', '<శ్రేణులని>____________', '<శ్రెనులని>____________')\n","('<kaliginchanive>______________', '<కలిగించనివే>__________', '<కలిగించనివే>__________')\n","('<parasaaratnampai>____________', '<పరసారత్నంపై>__________', '<పరసారత్నంపై>__________')\n","('<kaarporetlaku>_______________', '<కార్పోరేట్లకు>________', '<కార్పోరెట్లకు>________')\n","('<vishayamloonu>_______________', '<విషయంలోను>____________', '<విశయంలోను>____________')\n","ep: 17  bt: 0  loss: 0.19475584444792374  acc:  0.453125\n","ep: 17  bt: 20  loss: 0.207778495291005  acc:  0.4375\n","ep: 17  bt: 40  loss: 0.0008565846668637317  acc:  1.0\n","ep: 17  bt: 60  loss: 0.23917670871900476  acc:  0.390625\n","ep: 17  bt: 80  loss: 0.0006918110277341759  acc:  1.0\n","ep: 17  bt: 100  loss: 0.23472454236901324  acc:  0.5\n","ep: 17  bt: 120  loss: 0.24130792203156845  acc:  0.421875\n","ep: 17  bt: 140  loss: 0.2931668862052586  acc:  0.421875\n","ep: 17  bt: 160  loss: 0.19149742955746857  acc:  0.484375\n","ep: 17  bt: 180  loss: 0.22602703260338824  acc:  0.53125\n","ep: 17  bt: 200  loss: 0.2504114275393279  acc:  0.53125\n","ep: 17  bt: 220  loss: 0.0004338034550132959  acc:  1.0\n","ep: 17  bt: 240  loss: 0.3363635436348293  acc:  0.421875\n","ep: 17  bt: 260  loss: 0.0005415724590420723  acc:  1.0\n","ep: 17  bt: 280  loss: 0.24758168925409732  acc:  0.421875\n","ep: 17  bt: 300  loss: 0.0006025130658046058  acc:  1.0\n","ep: 17  bt: 320  loss: 0.0004974843772209209  acc:  1.0\n","ep: 17  bt: 340  loss: 0.18316484534222147  acc:  0.453125\n","ep: 17  bt: 360  loss: 0.0008895745419937631  acc:  1.0\n","ep: 17  bt: 380  loss: 0.000499959225239961  acc:  1.0\n","ep: 17  bt: 400  loss: 0.00040186832294515943  acc:  1.0\n","ep: 17  bt: 420  loss: 0.00038453151026497716  acc:  1.0\n","ep: 17  bt: 440  loss: 0.33414059099943744  acc:  0.4375\n","ep: 17  bt: 460  loss: 0.0007328743358021197  acc:  1.0\n","ep: 17  bt: 480  loss: 0.001290556691263033  acc:  1.0\n","ep: 17  bt: 500  loss: 0.3292985377104386  acc:  0.40625\n","ep: 17  bt: 520  loss: 0.24676455622134003  acc:  0.484375\n","ep: 17  bt: 540  loss: 0.0005553195900891138  acc:  1.0\n","ep: 17  bt: 560  loss: 0.0006150632853741231  acc:  1.0\n","ep: 17  bt: 580  loss: 0.29371280255525006  acc:  0.4375\n","ep: 17  bt: 600  loss: 0.00040270493406316507  acc:  1.0\n","ep: 17  bt: 620  loss: 0.2444336932638417  acc:  0.4375\n","ep: 17  bt: 640  loss: 0.0005145343830404074  acc:  1.0\n","ep: 17  bt: 660  loss: 0.2570034317348314  acc:  0.390625\n","ep: 17  bt: 680  loss: 0.28088911719944165  acc:  0.484375\n","ep: 17  bt: 700  loss: 0.0006932885102603747  acc:  1.0\n","ep: 17  bt: 720  loss: 0.19609287510747495  acc:  0.40625\n","ep: 17  bt: 740  loss: 0.0004217048864001813  acc:  1.0\n","ep: 17  bt: 760  loss: 0.0005274038842838744  acc:  1.0\n","ep: 17  bt: 780  loss: 0.0005490434882433518  acc:  1.0\n","('<rakshinchina>________________', '<రక్షించిన>____________', '<రక్షించిన>____________')\n","('<churna>______________________', '<చూర్ణ>________________', '<చూర్నా________________')\n","('<pathyasya>___________________', '<పథ్యస్య>______________', '<పఠత్యా్>______________')\n","('<nadilo>______________________', '<నదిలో>________________', '<నడిలో>________________')\n","('<mahendhar>___________________', '<మహేందర్>______________', '<మహేందర్>______________')\n","('<panae>_______________________', '<పనే>__________________', '<పానే>_________________')\n","('<lakka>_______________________', '<లక్క>_________________', '<లక్క>_________________')\n","('<roge>________________________', '<రోగ్>_________________', '<రోగే>_________________')\n","('<okai>________________________', '<ఓకే>__________________', '<ఒకై>__________________')\n","('<vinimayam>___________________', '<వినిమయం>______________', '<వినిమయం>______________')\n","('<produktion>__________________', '<ప్రొడక్షన్>___________', '<ప్రోడ్క్టి్>__________')\n","('<peelustunna>_________________', '<పీలుస్తున్న>__________', '<పీలుస్తున్న>__________')\n","('<inatiki>_____________________', '<ఈనాటికీ>______________', '<ఇనటికి>_______________')\n","('<kenyaa>______________________', '<కెన్యా>_______________', '<కేన్యా>_______________')\n","('<mudurani>____________________', '<ముదురని>______________', '<ముడురని>______________')\n","('<kaamuni>_____________________', '<కాముని>_______________', '<కాముని>_______________')\n","('<sastravettala>_______________', '<శాస్త్రవేత్తల>________', '<సస్త్రవేత్తల>_________')\n","('<visurutuu>___________________', '<విసురుతూ>_____________', '<విసురుతూ>_____________')\n","('<sodanu>______________________', '<సోడాను>_______________', '<సోదను>________________')\n","('<shreemahalakshmi>____________', '<శ్రీమహాలక్ష్మి>_______', '<శ్రీమహలక్ష్మి>________')\n","ep:  17  train acc: 0.72986328125  train loss: 0.11877579585517471  val acc: 0.441650390625  val loss: 0.2698020313097083\n","('<abhimaanistaaru>_____________', '<అభిమానిస్తారు>________', '<అభిమానిస్తారు>________')\n","('<pagulutundi>_________________', '<పగులుతుంది>___________', '<పగులుతుంది>___________')\n","('<hikulaku>____________________', '<హికులకు>______________', '<హికులకు>______________')\n","('<poonukunnadanna>_____________', '<పూనుకున్నదన్న>________', '<పూనుకున్నదన్న>________')\n","('<rebensburg>__________________', '<రెబెన్స్బర్గ్>________', '<రెబెన్స్బర్గ్>________')\n","('<cheluvamula>_________________', '<చెలువముల>_____________', '<చెలువముల>_____________')\n","('<astramatics>_________________', '<ఆస్ట్రామాటిక్స్>______', '<ఆస్ట్రామాటిక్స్>______')\n","('<maanavataaviluvalni>_________', '<మానవతావిలువల్ని>______', '<మానవతావిలువల్ని>______')\n","('<hermer>______________________', '<హర్మర్>_______________', '<హర్మర్>_______________')\n","('<koluvalo>____________________', '<కొలువలో>______________', '<కొలువలో>______________')\n","('<janabhavunna>________________', '<జనాభావున్న>___________', '<జనాభావున్న>___________')\n","('<salomon>_____________________', '<సాలోమాన్>_____________', '<సాలోమాన్>_____________')\n","('<kulugutundani>_______________', '<కులుగుతుందని>_________', '<కులుగుతుందని>_________')\n","('<aasiirvadistunnaadu>_________', '<ఆశీర్వదిస్తున్నాడు>___', '<ఆశీర్వదిస్తున్నాడు>___')\n","('<kanajaalamuloni>_____________', '<కణజాలములోని>__________', '<కణజాలములోని>__________')\n","('<anuvadistayo>________________', '<అనువదిస్తాయో>_________', '<అనువదిస్తాయో>_________')\n","('<agrahamani>__________________', '<ఆగ్రహమని>_____________', '<ఆగ్రహమని>_____________')\n","('<ankammaraavunu>______________', '<అంకమ్మరావును>_________', '<అంకమ్మరావును>_________')\n","('<vigrahaateeta>_______________', '<విగ్రహాతీత>___________', '<విగ్రహాతీత>___________')\n","('<minginatlunnadu>_____________', '<మింగినట్లున్నాడు>_____', '<మింగినట్లున్నాడు>_____')\n","ep: 18  bt: 0  loss: 0.0004708741832038631  acc:  1.0\n","ep: 18  bt: 20  loss: 0.0006130165622934052  acc:  1.0\n","ep: 18  bt: 40  loss: 0.00047668891594461775  acc:  1.0\n","ep: 18  bt: 60  loss: 0.32453876992930536  acc:  0.421875\n","ep: 18  bt: 80  loss: 0.22543687405793564  acc:  0.484375\n","ep: 18  bt: 100  loss: 0.2288532257080078  acc:  0.4375\n","ep: 18  bt: 120  loss: 0.24789551030034604  acc:  0.515625\n","ep: 18  bt: 140  loss: 0.4278959606004798  acc:  0.34375\n","ep: 18  bt: 160  loss: 0.22681773227194083  acc:  0.4375\n","ep: 18  bt: 180  loss: 0.0005465825290783592  acc:  1.0\n","ep: 18  bt: 200  loss: 0.25342493471891986  acc:  0.453125\n","ep: 18  bt: 220  loss: 0.0006241429516154786  acc:  1.0\n","ep: 18  bt: 240  loss: 0.26401472091674805  acc:  0.46875\n","ep: 18  bt: 260  loss: 0.20398965089217477  acc:  0.5\n","ep: 18  bt: 280  loss: 0.19365748115207837  acc:  0.484375\n","ep: 18  bt: 300  loss: 0.0005214254817237024  acc:  1.0\n","ep: 18  bt: 320  loss: 0.000961865905834281  acc:  1.0\n","ep: 18  bt: 340  loss: 0.26198109336521314  acc:  0.515625\n","ep: 18  bt: 360  loss: 0.000497824714883514  acc:  1.0\n","ep: 18  bt: 380  loss: 0.0004779504531103632  acc:  1.0\n","ep: 18  bt: 400  loss: 0.1206456059994905  acc:  0.609375\n","ep: 18  bt: 420  loss: 0.000661207039071166  acc:  1.0\n","ep: 18  bt: 440  loss: 0.2679222977679709  acc:  0.421875\n","ep: 18  bt: 460  loss: 0.30977701104205585  acc:  0.484375\n","ep: 18  bt: 480  loss: 0.19618820107501486  acc:  0.515625\n","ep: 18  bt: 500  loss: 0.2595024523527726  acc:  0.4375\n","ep: 18  bt: 520  loss: 0.339201139367145  acc:  0.46875\n","ep: 18  bt: 540  loss: 0.31359564739724866  acc:  0.375\n","ep: 18  bt: 560  loss: 0.00066340787579184  acc:  1.0\n","ep: 18  bt: 580  loss: 0.0007096202639134034  acc:  1.0\n","ep: 18  bt: 600  loss: 0.0006704428435667702  acc:  1.0\n","ep: 18  bt: 620  loss: 0.0005099107308880142  acc:  1.0\n","ep: 18  bt: 640  loss: 0.23603474575540292  acc:  0.484375\n","ep: 18  bt: 660  loss: 0.2361562148384426  acc:  0.421875\n","ep: 18  bt: 680  loss: 0.18067693710327148  acc:  0.53125\n","ep: 18  bt: 700  loss: 0.0007155862353418185  acc:  1.0\n","ep: 18  bt: 720  loss: 0.21571816568789276  acc:  0.359375\n","ep: 18  bt: 740  loss: 0.22587593742038892  acc:  0.46875\n","ep: 18  bt: 760  loss: 0.0004910463872163192  acc:  1.0\n","ep: 18  bt: 780  loss: 0.20644293660702911  acc:  0.484375\n","('<kolambonu>___________________', '<కొలంబోను>_____________', '<కొలంబోను>_____________')\n","('<rabinia>_____________________', '<రాబినియా>_____________', '<రాబినియా>_____________')\n","('<timdi>_______________________', '<తిండి>________________', '<తింది>________________')\n","('<jayamalini>__________________', '<జయమాలిని>_____________', '<జయమలిిిి>_____________')\n","('<dhasalu>_____________________', '<దశలు>_________________', '<ధాలాల>________________')\n","('<broaad>______________________', '<బ్రాడ్>_______________', '<బ్రాడ్>_______________')\n","('<bhulokam>____________________', '<భూలోకం>_______________', '<భులోకం>_______________')\n","('<bhaagamtoe>__________________', '<భాగంతో>_______________', '<భాగంతో>_______________')\n","('<granthalayanni>______________', '<గ్రంథాలయాన్ని>________', '<గ్రాథథలలయననని>________')\n","('<mehataa>_____________________', '<మెహతా>________________', '<మెహతా>________________')\n","('<evvaruu>_____________________', '<ఎవ్వరూ>_______________', '<ఎవ్వరూ>_______________')\n","('<hydrophobicity>______________', '<హైడ్రోఫోబిసిటి>_______', '<హైడ్రోఫోబికిటీ>_______')\n","('<taaragaa>____________________', '<తారగా>________________', '<తారగా>________________')\n","('<apakhyati>___________________', '<అపఖ్యాతి>_____________', '<అపఖ్యతిి______________')\n","('<kadhilinchee>________________', '<కదిలించే>_____________', '<కధిలించే>_____________')\n","('<varadala>____________________', '<వరదల>_________________', '<వరదాల>________________')\n","('<chuusaaru>___________________', '<చూసారు>_______________', '<చూసారు>_______________')\n","('<veluvadu>____________________', '<వెలువడు>______________', '<వెలువడడు>_____________')\n","('<thrishuulamthoo>_____________', '<త్రిశూలంతో>___________', '<త్రీషూలంతో>___________')\n","('<tecchukuni>__________________', '<తెచ్చుకుని>___________', '<తెచ్చుకుని>___________')\n","ep:  18  train acc: 0.72759765625  train loss: 0.12321809610053283  val acc: 0.40478515625  val loss: 0.27842706182728644\n","('<didree>______________________', '<డిడ్రీ>_______________', '<డిద్రీ>_______________')\n","('<interior>____________________', '<ఇంటిరీయర్>____________', '<ఇంటరరరరర>_____________')\n","('<fiaco>_______________________', '<ఫియాకో>_______________', '<ఫియాకో>_______________')\n","('<mullugucchukunte>____________', '<ముల్లుగుచ్చుకుంటే>____', '<ముళ్లుగుచ్చుకుంటే>____')\n","('<dachipettedanni>_____________', '<దాచిపెట్టేదాన్ని>_____', '<దాచిపెట్టేదన్ని>______')\n","('<vinevaadinani>_______________', '<వినేవాడినని>__________', '<వినేవాదినని>__________')\n","('<nerpinave>___________________', '<నేర్పినవే>____________', '<నేర్పినవే>____________')\n","('<avasaramyayi>________________', '<అవసరమ్యాయి>___________', '<అవసరమమ్యాయి>__________')\n","('<bharyalakanna>_______________', '<భార్యలకన్నా>__________', '<భరర్యాలకన్న>__________')\n","('<yicchaaru>___________________', '<యిచ్చారు>_____________', '<యిచ్చారు>_____________')\n","('<srushtikartalam>_____________', '<సృష్టికర్తలం>_________', '<శృష్టికర్తలం>_________')\n","('<shuddhavishamani>____________', '<శుద్ధవిషమని>__________', '<శుద్ధాిషమనిి__________')\n","('<saadukune>___________________', '<సాదుకునే>_____________', '<సాదుకునే>_____________')\n","('<jaaratapai>__________________', '<జారతపై>_______________', '<జారతపై>_______________')\n","('<ippinchave>__________________', '<ఇప్పించవే>____________', '<ఇప్పించవే>____________')\n","('<teliyajeyithe>_______________', '<తెలియజేయితే>__________', '<తెలియజేయితే>__________')\n","('<naatyamunu>__________________', '<నాట్యమును>____________', '<నాట్యమును>____________')\n","('<kattukuntamanna>_____________', '<కట్టుకుంటామన్నా>______', '<కట్టుకుంటామన్న>_______')\n","('<aatmakala>___________________', '<ఆత్మకళ>_______________', '<ఆత్మకల>_______________')\n","('<tontla>______________________', '<తొంట్ల>_______________', '<టాంట్ల>>______________')\n","ep: 19  bt: 0  loss: 0.17864917672198752  acc:  0.453125\n","ep: 19  bt: 20  loss: 0.1809742554374363  acc:  0.5\n","ep: 19  bt: 40  loss: 0.000381745721982873  acc:  1.0\n","ep: 19  bt: 60  loss: 0.0005122652763257856  acc:  1.0\n","ep: 19  bt: 80  loss: 0.0004852016496917476  acc:  1.0\n","ep: 19  bt: 100  loss: 0.0005169796311984892  acc:  1.0\n","ep: 19  bt: 120  loss: 0.2938786174940026  acc:  0.40625\n","ep: 19  bt: 140  loss: 0.2299059992251189  acc:  0.46875\n","ep: 19  bt: 160  loss: 0.0006140189488296924  acc:  1.0\n","ep: 19  bt: 180  loss: 0.0006085562560221423  acc:  1.0\n","ep: 19  bt: 200  loss: 0.0009193419600310533  acc:  1.0\n","ep: 19  bt: 220  loss: 0.001589364169732384  acc:  1.0\n","ep: 19  bt: 240  loss: 0.0010975295596796534  acc:  1.0\n","ep: 19  bt: 260  loss: 0.38044353153394617  acc:  0.328125\n","ep: 19  bt: 280  loss: 0.28976160547007684  acc:  0.5\n","ep: 19  bt: 300  loss: 0.23162381545357083  acc:  0.453125\n","ep: 19  bt: 320  loss: 0.21960426413494608  acc:  0.484375\n","ep: 19  bt: 340  loss: 0.0008018854195656984  acc:  1.0\n","ep: 19  bt: 360  loss: 0.0009164069981678673  acc:  1.0\n","ep: 19  bt: 380  loss: 0.19217748227326767  acc:  0.46875\n","ep: 19  bt: 400  loss: 0.00048035852935003197  acc:  1.0\n","ep: 19  bt: 420  loss: 0.0004035585114489431  acc:  1.0\n","ep: 19  bt: 440  loss: 0.000741761744670246  acc:  1.0\n","ep: 19  bt: 460  loss: 0.2350440439970597  acc:  0.5\n","ep: 19  bt: 480  loss: 0.24026912191639777  acc:  0.484375\n","ep: 19  bt: 500  loss: 0.0004445309143351472  acc:  1.0\n","ep: 19  bt: 520  loss: 0.00033415613048102546  acc:  1.0\n","ep: 19  bt: 540  loss: 0.19112344410108484  acc:  0.421875\n","ep: 19  bt: 560  loss: 0.2516386819922406  acc:  0.5625\n","ep: 19  bt: 580  loss: 0.0007481584730355636  acc:  1.0\n","ep: 19  bt: 600  loss: 0.200768926869268  acc:  0.375\n","ep: 19  bt: 620  loss: 0.2527052423228388  acc:  0.4375\n","ep: 19  bt: 640  loss: 0.30379873773326044  acc:  0.453125\n","ep: 19  bt: 660  loss: 0.17574219081712805  acc:  0.609375\n","ep: 19  bt: 680  loss: 0.22207521355670432  acc:  0.3125\n","ep: 19  bt: 700  loss: 0.23112713772317636  acc:  0.4375\n","ep: 19  bt: 720  loss: 0.2569977926171344  acc:  0.53125\n","ep: 19  bt: 740  loss: 0.2062862437704335  acc:  0.453125\n","ep: 19  bt: 760  loss: 0.22686278301736582  acc:  0.4375\n","ep: 19  bt: 780  loss: 0.0005929310522649599  acc:  1.0\n","('<cameralalo>__________________', '<కెమెరాలలో>____________', '<క్మరరాలల>_____________')\n","('<chepalunnayani>______________', '<చేపలున్నాయని>_________', '<చెపలునన్నాయని>________')\n","('<matacharalu>_________________', '<మతాచారాలు>____________', '<మాతచచాులు>____________')\n","('<kwilambo>____________________', '<క్విలంబో>_____________', '<క్విలంబో>_____________')\n","('<siddaantaanni>_______________', '<సిద్ధాంతాన్ని>________', '<సిద్దాంతాన్ని>________')\n","('<nibandhanalatoo>_____________', '<నిబంధనలతో>____________', '<నిబంధనలతో>____________')\n","('<pondhenu>____________________', '<పొందెను>______________', '<పొంధేను>______________')\n","('<vippabotondantu>_____________', '<విప్పబోతోందంటూ>_______', '<విప్పబోతోందంటూ>_______')\n","('<vijayaalu>___________________', '<విజయాలు>______________', '<విజయాలు>______________')\n","('<saralini>____________________', '<సరళిని>_______________', '<సరరిని>_______________')\n","('<koradaa>_____________________', '<కొరడా>________________', '<కోరాా>________________')\n","('<kolunu>______________________', '<కొలును>_______________', '<కొలును>_______________')\n","('<naukari>_____________________', '<నౌకరీ>________________', '<నౌకరి>________________')\n","('<aatmavisvaasaanni>___________', '<ఆత్మవిశ్వాసాన్ని>_____', '<ఆత్మవిస్వాసాస్ని>_____')\n","('<shatanaama>__________________', '<శతనామ>________________', '<శతనామ>________________')\n","('<truu>________________________', '<ట్రూ>_________________', '<ట్యూ>_________________')\n","('<evidhamaina>_________________', '<ఏవిధమైన>______________', '<ఏవిధమైన>______________')\n","('<varadala>____________________', '<వరదల>_________________', '<వరదల>_________________')\n","('<miisaaluu>___________________', '<మీసాలు>_______________', '<మిషీలాలూ>_____________')\n","('<rudhra>______________________', '<రుద్ర>________________', '<రు్రహ_________________')\n","ep:  19  train acc: 0.73140625  train loss: 0.12178981651645661  val acc: 0.412109375  val loss: 0.2770661685777747\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▅▆▇▇▇▇▇▇▇███▇█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▄▆▆▇▇▇▇█▇▇▆▇█▇▆▇█▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▃▃▂▂▂▂▁▁▁▁▂▂▁▁▃▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 19\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.73141\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.12179\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.41211\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 6.37252\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfine-sweep-31\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/94mujjqa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_183515-94mujjqa/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rb8hmn48 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_185521-rb8hmn48\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcharmed-sweep-32\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/rb8hmn48\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"name":"stdout","output_type":"stream","text":["('<parvathaarogya>______________', '<పర్వతారోగ్య>__________', 'గగగగగగగగగగకఫగగగబబబబబబబబ')\n","('<maaldeevulugaa>______________', '<మాల్దీవులుగా>_________', 'గగగగఉగగగగగఢఢగగగగృృబబబబబ')\n","('<vinakapoyina>________________', '<వినకపోయినా>___________', 'గగగగగగగగగగగగగగృృృృృృబబబ')\n","('<chesinavarinigurchi>_________', '<చేసినవారినిగూర్చి>____', 'గగభఖగఛగగగగఛగగగగగబగగగగృృ')\n","('<tangallapallini>_____________', '<తంగళ్లపల్లిని>________', 'గగగఓఘకగగగబగగఝగగగగబబబబబబ')\n","('<daaninani>___________________', '<దానినని>______________', 'గగగగగగఝగగగగగగగగబబబబబబబబ')\n","('<harshistunnadani>____________', '<హర్షిస్తున్నదని>______', 'గగగగఘగగగగగఛగఛగగగగగగగృృృ')\n","('<sambandheekulato>____________', '<సంబంధీకులతో>__________', 'గగగఛఈగగగగగగగగగగృృృృృృృఫ')\n","('<gabbilalake>_________________', '<గబ్బిలాలకే>___________', 'గగఓకఛగగగగగైగగగృృబబబబబబబ')\n","('<cheppalanukunte>_____________', '<చెప్పాలనుకుంటే>_______', 'గగుగగగగగగగగోగగైగగగగృృృఫ')\n","('<samaachaaramadi>_____________', '<సమాచారమది>____________', 'గగగగగగగగగగగగగగబబబబబబబబబ')\n","('<ardhaalankaaram>_____________', '<అర్థాలంకారం>__________', 'గగగగగగగగగగగగగగగబబబబబబబబ')\n","('<adapuliki>___________________', '<అడపులికి>_____________', 'గగగగగగగగగగగగగగబబబబబబబబబ')\n","('<poorniyaalo>_________________', '<పూర్ణియాలో>___________', 'గగగగగగగగగగగగగగృృృృృృృృృ')\n","('<kodutoo>_____________________', '<కొడుతూ>_______________', 'గగగగగగగగగగగగగృృబబబబబబబబ')\n","('<amaluchesta>_________________', '<అమలుచేస్తా>___________', 'గగగగగగభఢగగగగగగగృృృృబబబబ')\n","('<tagilindani>_________________', '<తగిలిందని>____________', 'గగగగగగగగగగగగగగగబబబబబబబబ')\n","('<peddabbaiki>_________________', '<పెద్దబ్బాయికి>________', 'గగగగగగఛఛఛగఢగగగగగగగగృృబబ')\n","('<nelakorigaaru>_______________', '<నెలకొరిగారు>__________', 'గగఘగగఉగగగగగగగగగగబబబబబబబ')\n","('<dakshanaadi>_________________', '<దక్షణాది>_____________', 'గగగగఉఉగగగగగగగబబబబబబబబబబ')\n","ep: 0  bt: 0  loss: 4.258240409519361  acc:  0.0\n","ep: 0  bt: 20  loss: 1.3976690872855808  acc:  0.0\n","ep: 0  bt: 40  loss: 1.074780671492867  acc:  0.0\n","ep: 0  bt: 60  loss: 0.5160998883454696  acc:  0.359375\n","ep: 0  bt: 80  loss: 2.0896785570227583  acc:  0.0\n","ep: 0  bt: 100  loss: 1.922456160835598  acc:  0.0\n","ep: 0  bt: 120  loss: 1.9188758186672046  acc:  0.0\n","ep: 0  bt: 140  loss: 0.2034871889197308  acc:  0.71875\n","ep: 0  bt: 160  loss: 1.766536049220873  acc:  0.0\n","ep: 0  bt: 180  loss: 0.18478936734406845  acc:  0.8125\n","ep: 0  bt: 200  loss: 0.11827126793239427  acc:  0.859375\n","ep: 0  bt: 220  loss: 0.09513416497603706  acc:  0.9375\n","ep: 0  bt: 240  loss: 0.12104792180268661  acc:  0.859375\n","ep: 0  bt: 260  loss: 1.1235461027725884  acc:  0.0\n","ep: 0  bt: 280  loss: 1.1618321460226309  acc:  0.0\n","ep: 0  bt: 300  loss: 1.035270525061566  acc:  0.0\n","ep: 0  bt: 320  loss: 0.11784578406292459  acc:  0.90625\n","ep: 0  bt: 340  loss: 0.07779803483382515  acc:  0.9375\n","ep: 0  bt: 360  loss: 0.8377571935239045  acc:  0.0\n","ep: 0  bt: 380  loss: 0.8334660737410836  acc:  0.0\n","ep: 0  bt: 400  loss: 0.8226596998131793  acc:  0.0\n","ep: 0  bt: 420  loss: 1.0828293510105298  acc:  0.03125\n","ep: 0  bt: 440  loss: 1.117994391399881  acc:  0.0\n","ep: 0  bt: 460  loss: 0.04503474028214165  acc:  0.921875\n","ep: 0  bt: 480  loss: 0.8527379243270211  acc:  0.0625\n","ep: 0  bt: 500  loss: 0.03394194271253503  acc:  0.9375\n","ep: 0  bt: 520  loss: 0.1039483236229938  acc:  0.859375\n","ep: 0  bt: 540  loss: 0.030466538408528202  acc:  1.0\n","ep: 0  bt: 560  loss: 0.02562785407771235  acc:  1.0\n","ep: 0  bt: 580  loss: 0.17799145242442255  acc:  0.734375\n","ep: 0  bt: 600  loss: 0.7529340826946757  acc:  0.015625\n","ep: 0  bt: 620  loss: 0.7221643199091372  acc:  0.015625\n","ep: 0  bt: 640  loss: 0.6768932342529297  acc:  0.15625\n","ep: 0  bt: 660  loss: 0.014638593663340029  acc:  1.0\n","ep: 0  bt: 680  loss: 0.6477261418881624  acc:  0.046875\n","ep: 0  bt: 700  loss: 0.01684569016746853  acc:  0.953125\n","ep: 0  bt: 720  loss: 0.011092496954876444  acc:  1.0\n","ep: 0  bt: 740  loss: 0.47058773040771484  acc:  0.171875\n","ep: 0  bt: 760  loss: 0.5542730663133704  acc:  0.140625\n","ep: 0  bt: 780  loss: 0.46087136475936225  acc:  0.21875\n","('<kandaraalu>__________________', '<కండరాలు>______________', '<కనడాాలల>>_____________')\n","('<bhavanamlone>________________', '<భవనంలోనే>_____________', '<భవవంంలలే>_____________')\n","('<chitaa>______________________', '<చితా>_________________', '<చితా>>>_______________')\n","('<viliam>______________________', '<విలియం>_______________', '<విలిి>>_______________')\n","('<ias>_________________________', '<ఐఎఎస్>________________', '<ఇససస్్్్______________')\n","('<vijayaalu>___________________', '<విజయాలు>______________', '<విజయయయుు______________')\n","('<bharatiyulu>_________________', '<భారతీయులు>____________', '<భరరిిిలలల>>___________')\n","('<marblehead>__________________', '<మార్బుల్హెడ్>_________', '<మరర్్ేేేే>____________')\n","('<bhaagaswaamulatho>___________', '<భాగస్వాములతో>_________', '<భాగసససమమతత>___________')\n","('<yadhatadha>__________________', '<యథాతథ>________________', '<యయయయయయ>>______________')\n","('<naalikkaruchukoni>___________', '<నాలిక్కరుచుకొని>______', '<నాలికకుుుకకి>_________')\n","('<ruudi>_______________________', '<రూడి>_________________', '<రుడి>>>_______________')\n","('<coppar>______________________', '<కాపర్>________________', '<కొపప్్్్>>____________')\n","('<bartov>______________________', '<బార్టోవ్>_____________', '<బరర్్్్్______________')\n","('<noppentani>__________________', '<నొప్పేంటని>___________', '<నోపెెెెననిి___________')\n","('<akademia>____________________', '<అకాడెమియా>____________', '<ఆకడడమమమ>>_____________')\n","('<chesthunnadani>______________', '<చేస్తున్నాడని>________', '<చేస్్్నననననన>>________')\n","('<raajyapu>____________________', '<రాజ్యపు>______________', '<రాజ్్పప>>_____________')\n","('<nachindhani>_________________', '<నచ్చిందని>____________', '<నచచచచనన>>_____________')\n","('<mannanalanu>_________________', '<మన్ననలను>_____________', '<మన్్నననన>>____________')\n","ep:  0  train acc: 0.41716796875  train loss: 0.6807953492009436  val acc: 0.00146484375  val loss: 0.794686690620754\n","('<krushnamoorthilanu>__________', '<కృష్ణమూర్తిలను>_______', '<కృష్ణమూర్తిలను>_______')\n","('<assoil>______________________', '<అస్సోయిల్>____________', '<అస్సోయిల్>____________')\n","('<karavani>____________________', '<కరవని>________________', '<కరవని>________________')\n","('<odevi>_______________________', '<ఒదేవి>________________', '<ఒదేవి>________________')\n","('<chorampudi>__________________', '<చోరంపూడి>_____________', '<చోరంపూడి>_____________')\n","('<meethoti>____________________', '<మీతోటి>_______________', '<మీతోటి>_______________')\n","('<temperating>_________________', '<టెంపరేటింగ్>__________', '<టెంపరేటింగ్>__________')\n","('<muslack>_____________________', '<మస్లాక్>______________', '<మస్లాక్>______________')\n","('<karuchukovali>_______________', '<కరుచుకోవాలి>__________', '<కరుచుకోవాలి>__________')\n","('<krucchamu>___________________', '<క్రుచ్చము>____________', '<క్రుచ్చము>____________')\n","('<lettutaadu>__________________', '<లెత్తుతాడు>___________', '<లెత్తుతాడు>___________')\n","('<mugurujapai>_________________', '<ముగురుజపై>____________', '<ముగురుజపై>____________')\n","('<anjalipatra>_________________', '<అంజలిపాత్ర>___________', '<అంజలిపాత్ర>___________')\n","('<kaigo>_______________________', '<కైగో>_________________', '<కైగో>_________________')\n","('<asahyinchukuntaaranna>_______', '<అసహ్యించుకుంటారన్న>___', '<అసహ్యించుకుంటారన్న>___')\n","('<ivvaledannatlu>______________', '<ఇవ్వలేదన్నట్లు>_______', '<ఇవ్వలేదన్నట్లు>_______')\n","('<nyaayakuutami>_______________', '<న్యాయకూటమి>___________', '<న్యాయకూటమి>___________')\n","('<kaagalikaarani>______________', '<కాగలికారని>___________', '<కాగలికారని>___________')\n","('<snehitulavanka>______________', '<స్నేహితులవంకా>________', '<స్నేహితులవంకా>________')\n","('<desist>______________________', '<డెసిస్ట్>_____________', '<డెసిస్ట్>_____________')\n","ep: 1  bt: 0  loss: 0.043442625066508415  acc:  0.984375\n","ep: 1  bt: 20  loss: 0.018996587266092716  acc:  0.96875\n","ep: 1  bt: 40  loss: 0.03232063936150592  acc:  0.96875\n","ep: 1  bt: 60  loss: 0.011376457369845846  acc:  0.96875\n","ep: 1  bt: 80  loss: 0.40445182634436566  acc:  0.15625\n","ep: 1  bt: 100  loss: 0.4571838793547257  acc:  0.140625\n","ep: 1  bt: 120  loss: 0.4753760462221892  acc:  0.21875\n","ep: 1  bt: 140  loss: 0.004998623029045437  acc:  1.0\n","ep: 1  bt: 160  loss: 0.012152057626973028  acc:  0.984375\n","ep: 1  bt: 180  loss: 0.6719631941422172  acc:  0.078125\n","ep: 1  bt: 200  loss: 0.005084192947201107  acc:  1.0\n","ep: 1  bt: 220  loss: 0.4173396151998769  acc:  0.234375\n","ep: 1  bt: 240  loss: 0.003733438966066941  acc:  1.0\n","ep: 1  bt: 260  loss: 0.005068215339080147  acc:  1.0\n","ep: 1  bt: 280  loss: 0.047872517419897995  acc:  0.875\n","ep: 1  bt: 300  loss: 0.9076975117559019  acc:  0.015625\n","ep: 1  bt: 320  loss: 0.5188606510991636  acc:  0.140625\n","ep: 1  bt: 340  loss: 0.008433089308116747  acc:  1.0\n","ep: 1  bt: 360  loss: 0.007830948285434557  acc:  0.984375\n","ep: 1  bt: 380  loss: 0.5319232940673828  acc:  0.125\n","ep: 1  bt: 400  loss: 0.004992508046005083  acc:  1.0\n","ep: 1  bt: 420  loss: 0.010560891550520191  acc:  0.96875\n","ep: 1  bt: 440  loss: 0.005629146876542464  acc:  0.984375\n","ep: 1  bt: 460  loss: 0.005608680455581001  acc:  0.984375\n","ep: 1  bt: 480  loss: 0.31424574230028235  acc:  0.390625\n","ep: 1  bt: 500  loss: 0.003573792136233786  acc:  1.0\n","ep: 1  bt: 520  loss: 0.5084740182627803  acc:  0.21875\n","ep: 1  bt: 540  loss: 0.008210043544354647  acc:  0.96875\n","ep: 1  bt: 560  loss: 0.4779391496077828  acc:  0.265625\n","ep: 1  bt: 580  loss: 0.006974430187888767  acc:  0.984375\n","ep: 1  bt: 600  loss: 0.0031295545075250707  acc:  1.0\n","ep: 1  bt: 620  loss: 0.539605596791143  acc:  0.25\n","ep: 1  bt: 640  loss: 0.004095798925213192  acc:  1.0\n","ep: 1  bt: 660  loss: 0.0038179913940637007  acc:  1.0\n","ep: 1  bt: 680  loss: 0.49015580052914826  acc:  0.234375\n","ep: 1  bt: 700  loss: 0.00603979502035224  acc:  0.96875\n","ep: 1  bt: 720  loss: 0.0029083393190218053  acc:  1.0\n","ep: 1  bt: 740  loss: 0.36345577239990234  acc:  0.34375\n","ep: 1  bt: 760  loss: 0.003941438120344411  acc:  0.984375\n","ep: 1  bt: 780  loss: 0.008243712394133858  acc:  0.96875\n","('<sainikudu>___________________', '<సైనికుడి>_____________', '<సైనికుడు>_____________')\n","('<aadhaaraalu>_________________', '<ఆధారాలు>______________', '<ఆధారాలు>______________')\n","('<kaikotti>____________________', '<కైకొట్టి>_____________', '<కైకొట్టి>_____________')\n","('<sandhyavandanam>_____________', '<సంధ్యావందనం>__________', '<సంధ్యవందం>____________')\n","('<bhawanamu>___________________', '<భవనము>________________', '<భానము>________________')\n","('<tru>_________________________', '<ట్రూ>_________________', '<త్రు>_________________')\n","('<kreedaakaaruniki>____________', '<క్రీడాకారునికి>_______', '<క్రీదారునికి>_________')\n","('<praeminchu>__________________', '<ప్రేమించు>____________', '<ప్రేమించు>____________')\n","('<ranuranu>____________________', '<రానురాను>_____________', '<రణురను>_______________')\n","('<chaalaarojulavutundi>________', '<చాలారోజులవుతుంది>_____', '<చాలారోజులవుతుం>_______')\n","('<gelavalekapoyanannaru>_______', '<గెలవలేకపోయానన్నారు>___', '<గెలవలేకోయయన్నారు>_____')\n","('<kosindannaru>________________', '<కోసిందన్నారు>_________', '<కొసిందన్నరు>__________')\n","('<gobiernos>___________________', '<గోబిఎర్నోస్>__________', '<గోబీీ్న్స్____________')\n","('<titlapuraanaaniki>___________', '<తిట్లపురాణానికి>______', '<తిట్లపురానికి>________')\n","('<sachinodini>_________________', '<సచ్చినోడిని>__________', '<సచ్చిోదిిి____________')\n","('<amaruchu>____________________', '<అమరుచు>_______________', '<అమరుచు>_______________')\n","('<marchali>____________________', '<మార్చాలి>_____________', '<మా్్చలి>______________')\n","('<huggingu>____________________', '<హగ్గింగు>_____________', '<హగగగగగు>______________')\n","('<neethini>____________________', '<నీతిని>_______________', '<నీతిని>_______________')\n","('<cheppadaaniki>_______________', '<చెప్పడానికి>__________', '<చేప్పడానికి>__________')\n","ep:  1  train acc: 0.6106640625  train loss: 0.2550972646027398  val acc: 0.3095703125  val loss: 0.3640363527380902\n","('<vekkirimpu>__________________', '<వెక్కిరింపు>__________', '<వెక్కిరింపు>__________')\n","('<chuttachuttukoni>____________', '<చుట్టచుట్టుకొని>______', '<చుట్టచుతుకుని>________')\n","('<kandabalam>__________________', '<కండబలం>_______________', '<కందబలం>_______________')\n","('<aidellaku>___________________', '<ఐదేళ్ళకు>_____________', '<ఐదేళ్లకు>_____________')\n","('<pakyong>_____________________', '<పాక్యాంగ్>____________', '<పక్యోంగ్>_____________')\n","('<saarapaakalo>________________', '<సారపాకలో>_____________', '<సారపాకలో>_____________')\n","('<podukopetti>_________________', '<పొడుకోపెట్టి>_________', '<పొడుకోపెట్టి>_________')\n","('<sitin>_______________________', '<సిటిన్>_______________', '<సిటిన్>_______________')\n","('<phenolyzed>__________________', '<ఫెనోలైజ్డ్>___________', '<ఫెనోల్యేడ్>___________')\n","('<munsipaaleetilalo>___________', '<మున్సిపాలీటిలలో>______', '<మున్సిపాటీలలల>________')\n","('<amayasaramaite>______________', '<అమయసరమైతే>____________', '<అమయసామమతే>____________')\n","('<ambrosius>___________________', '<అంబ్రోసియస్>__________', '<అంబ్రోసిస_____________')\n","('<ghaatakamgaa>________________', '<ఘాతకంగా>______________', '<ఘాటకంగా>______________')\n","('<graphicsa>___________________', '<గ్రాఫిక్సా>___________', '<గ్రఫఫిసస>_____________')\n","('<saladlaloni>_________________', '<సలాడ్లలోని>___________', '<సలద్లలోని>____________')\n","('<plantulanu>__________________', '<ప్లాంటులను>___________', '<ప్లంతులను>____________')\n","('<saagistunnaaru>______________', '<సాగిస్తున్నారు>_______', '<సాగిస్తున్నారు>_______')\n","('<shraddhapettaalani>__________', '<శ్రద్ధపెట్టాలని>______', '<శ్రద్ధపెట్లాి>________')\n","('<upayoganche>_________________', '<ఉపయోగంచే>_____________', '<ఉపయోగంచే>_____________')\n","('<ooregimpulai>________________', '<ఊరేగింపులై>___________', '<ఊరెగింపులై>___________')\n","ep: 2  bt: 0  loss: 0.41882121044656506  acc:  0.265625\n","ep: 2  bt: 20  loss: 0.0032233689790186677  acc:  1.0\n","ep: 2  bt: 40  loss: 0.3567029289577318  acc:  0.296875\n","ep: 2  bt: 60  loss: 0.3203625264375106  acc:  0.390625\n","ep: 2  bt: 80  loss: 0.3498523131660793  acc:  0.296875\n","ep: 2  bt: 100  loss: 0.0026803969041160917  acc:  1.0\n","ep: 2  bt: 120  loss: 0.38320997486943786  acc:  0.203125\n","ep: 2  bt: 140  loss: 0.0034433651877486186  acc:  1.0\n","ep: 2  bt: 160  loss: 0.00263663689079492  acc:  1.0\n","ep: 2  bt: 180  loss: 0.0034483648512674413  acc:  1.0\n","ep: 2  bt: 200  loss: 0.3173216322193975  acc:  0.34375\n","ep: 2  bt: 220  loss: 0.37866069959557574  acc:  0.265625\n","ep: 2  bt: 240  loss: 0.39647587485935376  acc:  0.3125\n","ep: 2  bt: 260  loss: 0.0018079308388025863  acc:  1.0\n","ep: 2  bt: 280  loss: 0.0030135603054710055  acc:  1.0\n","ep: 2  bt: 300  loss: 0.38852434572966205  acc:  0.234375\n","ep: 2  bt: 320  loss: 0.33358049392700195  acc:  0.375\n","ep: 2  bt: 340  loss: 0.6190435990043308  acc:  0.09375\n","ep: 2  bt: 360  loss: 0.32653806520544965  acc:  0.390625\n","ep: 2  bt: 380  loss: 0.26226130775783374  acc:  0.515625\n","ep: 2  bt: 400  loss: 0.3376773958620818  acc:  0.140625\n","ep: 2  bt: 420  loss: 0.33274660939755646  acc:  0.296875\n","ep: 2  bt: 440  loss: 0.0018083189168702002  acc:  1.0\n","ep: 2  bt: 460  loss: 0.3104688395624575  acc:  0.4375\n","ep: 2  bt: 480  loss: 0.35562046714451  acc:  0.296875\n","ep: 2  bt: 500  loss: 0.4111159366110097  acc:  0.328125\n","ep: 2  bt: 520  loss: 0.01310705102008322  acc:  0.9375\n","ep: 2  bt: 540  loss: 0.4710550722868546  acc:  0.1875\n","ep: 2  bt: 560  loss: 0.0025626456607942996  acc:  1.0\n","ep: 2  bt: 580  loss: 0.44483910436215607  acc:  0.203125\n","ep: 2  bt: 600  loss: 0.43918704986572266  acc:  0.171875\n","ep: 2  bt: 620  loss: 0.0025721007715100827  acc:  1.0\n","ep: 2  bt: 640  loss: 0.35133415719737177  acc:  0.296875\n","ep: 2  bt: 660  loss: 0.001813602512297423  acc:  1.0\n","ep: 2  bt: 680  loss: 0.3558670126873514  acc:  0.375\n","ep: 2  bt: 700  loss: 0.0015729536180910857  acc:  1.0\n","ep: 2  bt: 720  loss: 0.0013200562933216925  acc:  1.0\n","ep: 2  bt: 740  loss: 0.0018014324747997782  acc:  1.0\n","ep: 2  bt: 760  loss: 0.3125055976535963  acc:  0.28125\n","ep: 2  bt: 780  loss: 0.3828072340592094  acc:  0.375\n","('<vamsiiyula>__________________', '<వంశీయుల>______________', '<వంశీయుల>______________')\n","('<vaanijyavettalaku>___________', '<వాణిజ్యవేత్తలకు>______', '<వానిజ్యవెట్టలకు>______')\n","('<nadipinanta>_________________', '<నడిపినంత>_____________', '<నడిపినంత>_____________')\n","('<dhudanu>_____________________', '<దూడను>________________', '<ధూడను>________________')\n","('<vaartallo>___________________', '<వార్తల్లో>____________', '<వార్తల్లో>____________')\n","('<saagindi>____________________', '<సాగింది>______________', '<సాగింది>______________')\n","('<bodhinchi>___________________', '<బోధించి>______________', '<బోధించి>______________')\n","('<unte>________________________', '<ఉంటె>_________________', '<ఉంటే>_________________')\n","('<phaarmar>____________________', '<ఫార్మర్>______________', '<ఫార్మరర్______________')\n","('<cheppadu>____________________', '<చెప్పాడు>_____________', '<చెప్పడు>______________')\n","('<mysamma>_____________________', '<మైసమ్మ>_______________', '<మైసమ్మా_______________')\n","('<upanishatth>_________________', '<ఉపనిషత్>______________', '<ఉపనిషత్థ్>____________')\n","('<taranni>_____________________', '<తరాన్ని>______________', '<తరణ్ని>_______________')\n","('<puttindamma>_________________', '<పుట్టిందమ్మా>_________', '<పుట్టిందమ్మ>__________')\n","('<anandame>____________________', '<ఆనందమే>_______________', '<అనందేే>_______________')\n","('<vudayapuur>__________________', '<ఉదయపూర్>______________', '<వుడయపూర్>_____________')\n","('<sogdiana>____________________', '<సోగ్డియానా>___________', '<సోగ్డియన>_____________')\n","('<phibravariloo>_______________', '<ఫిబ్రవరిలో>___________', '<ఫిబ్రవరిలోోో__________')\n","('<pradhana>____________________', '<ప్రధాన>_______________', '<ప్రధా>________________')\n","('<takajam>_____________________', '<తకజం>_________________', '<తకజం>_________________')\n","ep:  2  train acc: 0.65498046875  train loss: 0.19808051908287505  val acc: 0.3896484375  val loss: 0.29426802759585174\n","('<stuudies>____________________', '<స్టడీస్>______________', '<స్టూడిస్>_____________')\n","('<sambandhinchindo>____________', '<సంబంధించిందో>_________', '<సంబంధించిందో>_________')\n","('<swaralevi>___________________', '<స్వరాలేవి>____________', '<స్వరలేవవ>_____________')\n","('<shaasinchaale>_______________', '<శాసించాలే>____________', '<శాసించాలే>____________')\n","('<sutlu>_______________________', '<సుట్లు>_______________', '<సుట్లు>_______________')\n","('<mucchinta>___________________', '<ముచ్చింత>_____________', '<ముచ్చింత>_____________')\n","('<saambasivaraavu>_____________', '<సాంబశివరావు>__________', '<సాంబసివరావు>__________')\n","('<moosukupotunnappudu>_________', '<మూసుకుపోతున్నప్పుడు>__', '<మూసుకుపోతున్నప్పుడు>__')\n","('<kalavakolanu>________________', '<కలవకొలను>_____________', '<కలవకోలను>_____________')\n","('<punarmudritamainadi>_________', '<పునర్ముద్రితమైనది>____', '<పునర్ముద్రితమైనది>____')\n","('<punjukuntundantunna>_________', '<పుంజుకుంటుందంటున్న>___', '<పుంజుకుంటుందంటున్_____')\n","('<sampuurnagaa>________________', '<సంపూర్ణగా>____________', '<సంపూర్ఞగా>____________')\n","('<paalamuurula>________________', '<పాలమూరుల>_____________', '<పాలమూరుల>_____________')\n","('<pedachintakunta>_____________', '<పెదచింతకుంట>__________', '<పెదచింటకుంట>__________')\n","('<rememberance>________________', '<రిమెంబరెన్స్>_________', '<రేమెంబరరేం>___________')\n","('<nundochina>__________________', '<నుండొచ్చినా>__________', '<నుండొచిన>_____________')\n","('<argentino>___________________', '<అర్జెంటినో>___________', '<అర్గెంటినో>___________')\n","('<pampesaru>___________________', '<పంపేశారు>_____________', '<పంపేశరు>______________')\n","('<pradarshistunna>_____________', '<ప్రదర్శిస్తున్న>______', '<ప్రదర్షిస్తున్న>______')\n","('<asiabordulo>_________________', '<ఏసియాబోర్డులో>________', '<అసియోర్డులో>__________')\n","ep: 3  bt: 0  loss: 0.23497110864390497  acc:  0.421875\n","ep: 3  bt: 20  loss: 0.0014085359871387482  acc:  1.0\n","ep: 3  bt: 40  loss: 0.0018641378568566363  acc:  1.0\n","ep: 3  bt: 60  loss: 0.001616217843864275  acc:  1.0\n","ep: 3  bt: 80  loss: 0.0064383362946302996  acc:  0.953125\n","ep: 3  bt: 100  loss: 0.36057186126708984  acc:  0.34375\n","ep: 3  bt: 120  loss: 0.3537139063296111  acc:  0.328125\n","ep: 3  bt: 140  loss: 0.3472706960595172  acc:  0.328125\n","ep: 3  bt: 160  loss: 0.35616559567658795  acc:  0.359375\n","ep: 3  bt: 180  loss: 0.0018556842013545659  acc:  1.0\n","ep: 3  bt: 200  loss: 0.2788056912629501  acc:  0.265625\n","ep: 3  bt: 220  loss: 0.3254985394685165  acc:  0.34375\n","ep: 3  bt: 240  loss: 0.35813514046047046  acc:  0.296875\n","ep: 3  bt: 260  loss: 0.39966334467348846  acc:  0.328125\n","ep: 3  bt: 280  loss: 0.0027677322859349456  acc:  0.984375\n","ep: 3  bt: 300  loss: 0.0010067868329908538  acc:  1.0\n","ep: 3  bt: 320  loss: 0.4283240359762441  acc:  0.296875\n","ep: 3  bt: 340  loss: 0.003485516037629998  acc:  0.984375\n","ep: 3  bt: 360  loss: 0.2566489551378333  acc:  0.375\n","ep: 3  bt: 380  loss: 0.7839907770571501  acc:  0.125\n","ep: 3  bt: 400  loss: 0.003978881499041681  acc:  0.984375\n","ep: 3  bt: 420  loss: 0.3301955927973208  acc:  0.375\n","ep: 3  bt: 440  loss: 0.0015226806635441988  acc:  1.0\n","ep: 3  bt: 460  loss: 0.0016951280767502992  acc:  1.0\n","ep: 3  bt: 480  loss: 0.0037789701119713163  acc:  0.984375\n","ep: 3  bt: 500  loss: 0.3568538582843283  acc:  0.28125\n","ep: 3  bt: 520  loss: 0.0020742871515128923  acc:  1.0\n","ep: 3  bt: 540  loss: 0.001376314331655917  acc:  1.0\n","ep: 3  bt: 560  loss: 0.0015800962953463845  acc:  1.0\n","ep: 3  bt: 580  loss: 0.0022243847665579424  acc:  0.984375\n","ep: 3  bt: 600  loss: 0.0027402862906455994  acc:  1.0\n","ep: 3  bt: 620  loss: 0.3471818799557893  acc:  0.421875\n","ep: 3  bt: 640  loss: 0.34721438781074854  acc:  0.375\n","ep: 3  bt: 660  loss: 0.31632342545882514  acc:  0.453125\n","ep: 3  bt: 680  loss: 0.31123936694601306  acc:  0.40625\n","ep: 3  bt: 700  loss: 0.0008516431502673937  acc:  1.0\n","ep: 3  bt: 720  loss: 0.32981209132982336  acc:  0.5\n","ep: 3  bt: 740  loss: 0.001072550192475319  acc:  1.0\n","ep: 3  bt: 760  loss: 0.0015105601886044378  acc:  1.0\n","ep: 3  bt: 780  loss: 0.24985618176667587  acc:  0.40625\n","('<chaeyasaagaaru>______________', '<చేయసాగారు>____________', '<చేయసాగారు>____________')\n","('<mahanubhavulu>_______________', '<మహానుభావులు>__________', '<మహనుభభుుు>____________')\n","('<ashrayam>____________________', '<ఆశ్రయం>_______________', '<అష్రయం>_______________')\n","('<veedhikokatigaa>_____________', '<వీధికొకటిగా>__________', '<వీధికోకటిగా>__________')\n","('<paduthundi>__________________', '<పడుతుంది>_____________', '<పడుతుంది>_____________')\n","('<poosina>_____________________', '<పూసిన>________________', '<పూసిన>________________')\n","('<madhyagala>__________________', '<మధ్యగల>_______________', '<మధ్యగల>_______________')\n","('<cameraalo>___________________', '<కెమెరాలలో>____________', '<కామారో>_______________')\n","('<wicq>________________________', '<విక్>_________________', '<విక్>_________________')\n","('<dosa>________________________', '<దోస>__________________', '<దోసా>_________________')\n","('<sannivesamuu>________________', '<సన్నివేశమూ>___________', '<సన్నివేసమూ>___________')\n","('<aharamga>____________________', '<ఆహారంగా>______________', '<అహరంగా>_______________')\n","('<tayaarayina>_________________', '<తయారయిన>______________', '<తయారయిన>______________')\n","('<surekhatopatu>_______________', '<సురేఖతోపాటు>__________', '<సురేఖతోపాటు>__________')\n","('<vinaashanaaniki>_____________', '<వినాశనానికి>__________', '<వినాషణానికి>__________')\n","('<veerangam>___________________', '<వీరంగం>_______________', '<వీరంంం>_______________')\n","('<vansto>______________________', '<వాన్స్తో>_____________', '<వన్సస్ో_______________')\n","('<aavakaaya>___________________', '<ఆవకాయ>________________', '<ఆవకాయ>________________')\n","('<mehata>______________________', '<మెహతా>________________', '<మేహత>_________________')\n","('<bhadrataa>___________________', '<భద్రతా>_______________', '<భద్రతా>_______________')\n","ep:  3  train acc: 0.69658203125  train loss: 0.16146673059232683  val acc: 0.390625  val loss: 0.30367861623349396\n","('<neekorikalu>_________________', '<నీకోరికలు>____________', '<నీకోరికలు>____________')\n","('<gottipatike>_________________', '<గొట్టిపాటికే>_________', '<గోట్టిపటికే>__________')\n","('<kumaram>_____________________', '<కుమరం>________________', '<కుమారం>_______________')\n","('<mukhyaarthikasalahaadaaru>___', '<ముఖ్యఆర్థికసలహాదారు>__', '<ముఖ్యార్థికసలాారు>____')\n","('<teevravaadulo>_______________', '<తీవ్రవాదులో>__________', '<తీవ్రవాదులో>__________')\n","('<velladaani>__________________', '<వెళ్ళడాని>____________', '<వెళ్లడాని>____________')\n","('<pravaahamlaanti>_____________', '<ప్రవాహంలాంటి>_________', '<ప్రవాహంలాంటి>_________')\n","('<prerakulayyaaru>_____________', '<ప్రేరకులయ్యారు>_______', '<ప్రేరకులయ్యారు>_______')\n","('<krishnadevi>_________________', '<కృష్ణాదేవీ>___________', '<క్ష్ణదేవి>____________')\n","('<budhavaaru>__________________', '<బుధవారు>______________', '<బుధవారు>______________')\n","('<karnabherulu>________________', '<కర్ణభేరులు>___________', '<కర్ణభేరులు>___________')\n","('<sokutundemo>_________________', '<సోకుతుందేమో>__________', '<సోకుతుందేమో>__________')\n","('<potrakhanda>_________________', '<పొత్రఖండ>_____________', '<పోత్రఖంం>_____________')\n","('<manavahakkulante>____________', '<మానవహక్కులంటే>________', '<మనవహక్కలంటే>__________')\n","('<kadalutaayi>_________________', '<కదలుతాయి>_____________', '<కదలుతాయి>_____________')\n","('<baruvayyaayi>________________', '<బరువయ్యాయి>___________', '<బరువయ్యా>ి____________')\n","('<vadaprasaadam>_______________', '<వడప్రసాదం>____________', '<వదప్రశాడం>____________')\n","('<telusukunnanduna>____________', '<తెలుసుకున్నందున>______', '<తెలుసుకున్నందున>______')\n","('<drohulantu>__________________', '<ద్రోహులంటూ>___________', '<ద్రోహులంటూ>___________')\n","('<chuupistunnarani>____________', '<చూపిస్తున్నరని>_______', '<చూపిస్తున్నారని>______')\n","ep: 4  bt: 0  loss: 0.3052793378415315  acc:  0.53125\n","ep: 4  bt: 20  loss: 0.0008770187749810841  acc:  1.0\n","ep: 4  bt: 40  loss: 0.0015215996814810712  acc:  1.0\n","ep: 4  bt: 60  loss: 0.00218165406714315  acc:  0.984375\n","ep: 4  bt: 80  loss: 0.29003947714100714  acc:  0.515625\n","ep: 4  bt: 100  loss: 0.3236716104590375  acc:  0.390625\n","ep: 4  bt: 120  loss: 0.010565317843271338  acc:  0.984375\n","ep: 4  bt: 140  loss: 0.0007735600128121998  acc:  1.0\n","ep: 4  bt: 160  loss: 0.0013706876855829487  acc:  1.0\n","ep: 4  bt: 180  loss: 0.0009970199316740036  acc:  1.0\n","ep: 4  bt: 200  loss: 0.38176271189814026  acc:  0.3125\n","ep: 4  bt: 220  loss: 0.0009728090270705845  acc:  1.0\n","ep: 4  bt: 240  loss: 0.8481364042862601  acc:  0.15625\n","ep: 4  bt: 260  loss: 0.4944456763889479  acc:  0.203125\n","ep: 4  bt: 280  loss: 0.23101391999617868  acc:  0.484375\n","ep: 4  bt: 300  loss: 0.0010235175976286764  acc:  1.0\n","ep: 4  bt: 320  loss: 0.31346820748370624  acc:  0.4375\n","ep: 4  bt: 340  loss: 0.0017063198206217392  acc:  1.0\n","ep: 4  bt: 360  loss: 0.0023299554443877678  acc:  1.0\n","ep: 4  bt: 380  loss: 0.0017695435039375138  acc:  1.0\n","ep: 4  bt: 400  loss: 0.3784772209499193  acc:  0.328125\n","ep: 4  bt: 420  loss: 0.001105758402010669  acc:  1.0\n","ep: 4  bt: 440  loss: 0.0010339640404867089  acc:  1.0\n","ep: 4  bt: 460  loss: 0.0012890078613291616  acc:  1.0\n","ep: 4  bt: 480  loss: 0.0011589419420646584  acc:  1.0\n","ep: 4  bt: 500  loss: 0.0015050111257511637  acc:  1.0\n","ep: 4  bt: 520  loss: 0.005326784175375234  acc:  0.96875\n","ep: 4  bt: 540  loss: 0.7117123811141305  acc:  0.203125\n","ep: 4  bt: 560  loss: 0.6441295457922894  acc:  0.21875\n","ep: 4  bt: 580  loss: 0.3459462704865829  acc:  0.40625\n","ep: 4  bt: 600  loss: 0.3827726529992145  acc:  0.265625\n","ep: 4  bt: 620  loss: 0.0010152197564425676  acc:  1.0\n","ep: 4  bt: 640  loss: 0.28282412238742993  acc:  0.3125\n","ep: 4  bt: 660  loss: 0.0009710520667874295  acc:  1.0\n","ep: 4  bt: 680  loss: 0.000945674822382305  acc:  1.0\n","ep: 4  bt: 700  loss: 0.0013075246597113817  acc:  1.0\n","ep: 4  bt: 720  loss: 0.2705626695052437  acc:  0.359375\n","ep: 4  bt: 740  loss: 0.0011534682758476424  acc:  1.0\n","ep: 4  bt: 760  loss: 0.37707693680473  acc:  0.40625\n","ep: 4  bt: 780  loss: 0.2997933885325556  acc:  0.40625\n","('<skaatlaamd>__________________', '<స్కాట్లాండ్>__________', '<స్కాట్లాండ్>__________')\n","('<kulchi>______________________', '<కూల్చి>_______________', '<కుల్చి>_______________')\n","('<turaga>______________________', '<తురగా>________________', '<తురగా>________________')\n","('<bagu>________________________', '<బాగు>_________________', '<బాగు>_________________')\n","('<cathalic>____________________', '<కాథలిక్>______________', '<కథథిిిక్>_____________')\n","('<pramaaneekarinchaali>________', '<ప్రమాణీకరించాలి>______', '<ప్రమానీకరించాలి>______')\n","('<pantha>______________________', '<పంథా>_________________', '<పంత>__________________')\n","('<abaddhapu>___________________', '<అబద్ధపు>______________', '<అబద్ధపుు______________')\n","('<parinaamamu>_________________', '<పరిణామము>_____________', '<పరినామము>_____________')\n","('<manasulalo>__________________', '<మనసులలో>______________', '<మనసులలో>______________')\n","('<dhairyamgaa>_________________', '<ధైర్యంగా>_____________', '<ధైర్యంగా>_____________')\n","('<tibust>______________________', '<టిబస్ట్>______________', '<టిబుస్ట్>_____________')\n","('<ghanathanu>__________________', '<ఘనతను>________________', '<ఘనతను>________________')\n","('<matladatamga>________________', '<మాట్లాడటంగా>__________', '<మతట్లాడటంగా>__________')\n","('<vetakaraadu>_________________', '<వెతకరాదు>_____________', '<వెతకరాదు>_____________')\n","('<tuluva>______________________', '<తుళువ>________________', '<తులువ>________________')\n","('<rathasaarathiki>_____________', '<రథసారథికి>____________', '<రతతారతికి>____________')\n","('<tayaaridaaru>________________', '<తయారీదారు>____________', '<తయారిదారు>____________')\n","('<chaeraadu>___________________', '<చేరాడు>_______________', '<చేరాదు>_______________')\n","('<agraharalanni>_______________', '<అగ్రహారాలన్నీ>________', '<అగ్రహరలల్ని>__________')\n","ep:  4  train acc: 0.71431640625  train loss: 0.15270656627370063  val acc: 0.390869140625  val loss: 0.2992336439049762\n","('<samyuta>_____________________', '<సంయుత>________________', '<సమ్య>>________________')\n","('<alankarinchulo>______________', '<అలంకరించులో>__________', '<అలంకరించులో>__________')\n","('<yivvaleranipinchindi>________', '<యివ్వలేరనిపించింది>___', '<యివ్వలేరనిపించింది>___')\n","('<sivajyotilato>_______________', '<శివజ్యోతిలతో>_________', '<సివజ్యోతిలతో>_________')\n","('<gamdaragolam>________________', '<గందరగోళం>_____________', '<గండరగోలం>_____________')\n","('<panchamukhini>_______________', '<పంచముఖిని>____________', '<పంచముఖిని>____________')\n","('<palakarinchukuni>____________', '<పలకరించుకుని>_________', '<పలకరించుకుని>_________')\n","('<venuyadav>___________________', '<వేణుయాదవ్>____________', '<వెనుయడవ్్_____________')\n","('<tappiponi>___________________', '<తప్పిపోని>____________', '<తప్పిపోని>____________')\n","('<anaemia>_____________________', '<ఎనిమియాతో>____________', '<అనేమియాయ______________')\n","('<moodurojulavaraku>___________', '<మూడురోజులవరకు>________', '<మూదురోజులవరకు>________')\n","('<jarakkapoyedi>_______________', '<జరక్కపోయేది>__________', '<జరక్కపోయేది>__________')\n","('<gullabarchi>_________________', '<గుల్లబార్చి>__________', '<గుళ్లబర్చి>___________')\n","('<veyyikilomeetarla>___________', '<వెయ్యికిలోమీటర్ల>_____', '<వెయ్యికిలోమీటర్ల>_____')\n","('<inkemando>___________________', '<ఇంకేమందో>_____________', '<ఇంకేమందో>_____________')\n","('<majjigapyaketlu>_____________', '<మజ్జిగప్యాకెట్లు>_____', '<మజజజిగప్యకేట్లు>______')\n","('<borrelia>____________________', '<బోర్రేలియా>___________', '<బోర్రేలియా____________')\n","('<taaganeeru>__________________', '<తాగనీరు>______________', '<తాగనీరు>______________')\n","('<etluntam>____________________', '<ఎట్లుంటం>_____________', '<ఎత్లుంటం>_____________')\n","('<lesimaniasis>________________', '<లెసిమానియాసిస్>_______', '<లేసిమానిససస్__________')\n","ep: 5  bt: 0  loss: 0.2682229539622431  acc:  0.421875\n","ep: 5  bt: 20  loss: 0.0011396384595528893  acc:  1.0\n","ep: 5  bt: 40  loss: 0.26295286676158075  acc:  0.4375\n","ep: 5  bt: 60  loss: 0.0007007861428934594  acc:  1.0\n","ep: 5  bt: 80  loss: 0.0008841770012741504  acc:  1.0\n","ep: 5  bt: 100  loss: 0.000991645471557327  acc:  1.0\n","ep: 5  bt: 120  loss: 0.21446756694627844  acc:  0.453125\n","ep: 5  bt: 140  loss: 0.3306720360465672  acc:  0.40625\n","ep: 5  bt: 160  loss: 0.25542522513348126  acc:  0.53125\n","ep: 5  bt: 180  loss: 0.27127564471700916  acc:  0.40625\n","ep: 5  bt: 200  loss: 0.3023957584215247  acc:  0.40625\n","ep: 5  bt: 220  loss: 0.0027341907439024553  acc:  0.984375\n","ep: 5  bt: 240  loss: 0.3701460879781972  acc:  0.5\n","ep: 5  bt: 260  loss: 0.0008907197448222533  acc:  1.0\n","ep: 5  bt: 280  loss: 0.2653513369352921  acc:  0.53125\n","ep: 5  bt: 300  loss: 0.2703421426856  acc:  0.421875\n","ep: 5  bt: 320  loss: 0.22855619762254797  acc:  0.53125\n","ep: 5  bt: 340  loss: 0.30240852936454443  acc:  0.515625\n","ep: 5  bt: 360  loss: 0.0009953525403271551  acc:  1.0\n","ep: 5  bt: 380  loss: 0.22013067162555197  acc:  0.40625\n","ep: 5  bt: 400  loss: 0.0008102657030458035  acc:  1.0\n","ep: 5  bt: 420  loss: 0.0014154473724572556  acc:  1.0\n","ep: 5  bt: 440  loss: 0.0009371705515229183  acc:  1.0\n","ep: 5  bt: 460  loss: 0.000985718858630761  acc:  1.0\n","ep: 5  bt: 480  loss: 0.23628220350846  acc:  0.484375\n","ep: 5  bt: 500  loss: 0.2674619633218516  acc:  0.40625\n","ep: 5  bt: 520  loss: 0.0006302113280348156  acc:  1.0\n","ep: 5  bt: 540  loss: 0.36290110712466034  acc:  0.375\n","ep: 5  bt: 560  loss: 0.31480917723282525  acc:  0.421875\n","ep: 5  bt: 580  loss: 0.0008372054637774178  acc:  1.0\n","ep: 5  bt: 600  loss: 0.35789784141208814  acc:  0.25\n","ep: 5  bt: 620  loss: 0.3737442597098973  acc:  0.390625\n","ep: 5  bt: 640  loss: 0.002533012272223182  acc:  0.984375\n","ep: 5  bt: 660  loss: 0.34968065178912616  acc:  0.328125\n","ep: 5  bt: 680  loss: 0.30004760493402893  acc:  0.453125\n","ep: 5  bt: 700  loss: 0.2753790979800017  acc:  0.390625\n","ep: 5  bt: 720  loss: 0.0011053311274103496  acc:  1.0\n","ep: 5  bt: 740  loss: 0.0008925618198902711  acc:  1.0\n","ep: 5  bt: 760  loss: 0.0018575763248878977  acc:  0.984375\n","ep: 5  bt: 780  loss: 0.311202857805335  acc:  0.34375\n","('<gaambheeryam>________________', '<గాంభీర్యం>____________', '<గాంభీర్యం>____________')\n","('<dheepaalanu>_________________', '<దీపాలను>______________', '<ధీపాలను>______________')\n","('<marchali>____________________', '<మార్చాలి>_____________', '<మార్చలల>______________')\n","('<phormat>_____________________', '<ఫార్మాట్>_____________', '<ఫోర్మాట్>_____________')\n","('<nachhindani>_________________', '<నచ్చిందని>____________', '<నచ్చిందని>____________')\n","('<chinnavayasuloo>_____________', '<చిన్నవయసులో>__________', '<చిన్నవాయయులో>_________')\n","('<samarthavantamaina>__________', '<సమర్థవంతమైన>__________', '<సమర్థవంతమైన>__________')\n","('<kaligivundavachho>___________', '<కలిగివుండవచ్చో>_______', '<కలిగివుండవచ్చో>_______')\n","('<rathasaarathiki>_____________', '<రథసారథికి>____________', '<రతసారతికి>____________')\n","('<regadam>_____________________', '<రేగడం>________________', '<రెగడం>________________')\n","('<angeekarinchani>_____________', '<అంగీకరించని>__________', '<అంగీకరించని>__________')\n","('<sheershikalanu>______________', '<శీర్షికలను>___________', '<శీర్షికలను>___________')\n","('<dhoolipa>____________________', '<ధూళిపాళ>______________', '<ధూలిపా________________')\n","('<kirtiki>_____________________', '<కీర్తికి>_____________', '<కిర్తికి>_____________')\n","('<dhraaksha>___________________', '<ద్రాక్ష>______________', '<ధ్రాక్ష>______________')\n","('<samudayamlo>_________________', '<సముదాయంలో>____________', '<సముదయంలో>_____________')\n","('<returnsulo>__________________', '<రిటర్న్సులో>__________', '<రెతుర్నులలో>__________')\n","('<paristhitulappudu>___________', '<పరిస్థితులప్పుడు>_____', '<పరిస్థితులప్పుడు>_____')\n","('<varninchadaaniki>____________', '<వర్ణించడానికి>________', '<వర్ణించడానికి>________')\n","('<koradaa>_____________________', '<కొరడా>________________', '<కొరడా>________________')\n","ep:  5  train acc: 0.69755859375  train loss: 0.15785673833917843  val acc: 0.390380859375  val loss: 0.28837500447812286\n","('<vigathajeevudayyaadu>________', '<విగతజీవుడయ్యాడు>______', '<విగతజీవుడయ్యాడు>______')\n","('<korealloki>__________________', '<కొరియాల్లోకి>_________', '<కొరేయాల్లోకి>_________')\n","('<dammukoti>___________________', '<దమ్ముకోటి>____________', '<దమ్ముకోతి>____________')\n","('<khauri>______________________', '<ఖౌరీ>_________________', '<ఖౌరి>_________________')\n","('<arjunudipai>_________________', '<అర్జునుడిపై>__________', '<అర్జునుడిపై>__________')\n","('<mudurutuntundi>______________', '<ముదురుతుంటుంది>_______', '<ముడురుతుంటుంది>_______')\n","('<manushulathoni>______________', '<మనుషులతోని>___________', '<మనుషులతోని>___________')\n","('<ivvaantuu>___________________', '<ఇవ్వాంటూ>_____________', '<ఇవ్వాంతూ>_____________')\n","('<polampanulapai>______________', '<పొలంపనులపై>___________', '<పొలంపనులపై>___________')\n","('<fleez>_______________________', '<ఫ్లీజ్>_______________', '<ఫెలీజ్>_______________')\n","('<atrachali>___________________', '<అత్రచలి>______________', '<అత్రాలలల>_____________')\n","('<dakshinaatyulu>______________', '<దక్షిణాత్యులు>________', '<దక్షినాత్యులు>________')\n","('<meekishta>___________________', '<మీకిష్ట>______________', '<మీకిష్ట>______________')\n","('<vichesinattu>________________', '<విచ్చేసినట్టు>________', '<విచేసినట్టు>__________')\n","('<tarigipoyaaru>_______________', '<తరిగిపోయారు>__________', '<తరిగిపోయారు>__________')\n","('<kottakuntaku>________________', '<కొత్తకుంటకు>__________', '<కొత్టకుంటకు>__________')\n","('<roomarsenani>________________', '<రూమర్సేనని>___________', '<రూమర్శేనని>___________')\n","('<mammalnila>__________________', '<మమ్మల్నిలా>___________', '<మమ్మల్నిల>____________')\n","('<avtunnarta>__________________', '<అవుతున్నార్ట>_________', '<అవ్తున్నార్త>_________')\n","('<caltex>______________________', '<కాల్టెక్స్>___________', '<కాయ్టెక్స్>___________')\n","ep: 6  bt: 0  loss: 0.2276997358902641  acc:  0.390625\n","ep: 6  bt: 20  loss: 0.21643053967019785  acc:  0.484375\n","ep: 6  bt: 40  loss: 0.23252603282099185  acc:  0.421875\n","ep: 6  bt: 60  loss: 0.34071955473526666  acc:  0.421875\n","ep: 6  bt: 80  loss: 0.36428633980129077  acc:  0.3125\n","ep: 6  bt: 100  loss: 0.3122022877568784  acc:  0.484375\n","ep: 6  bt: 120  loss: 0.3409586367399796  acc:  0.484375\n","ep: 6  bt: 140  loss: 0.24937832873800528  acc:  0.359375\n","ep: 6  bt: 160  loss: 0.21615892907847528  acc:  0.453125\n","ep: 6  bt: 180  loss: 0.0009733502469632937  acc:  1.0\n","ep: 6  bt: 200  loss: 0.0014638453722000122  acc:  1.0\n","ep: 6  bt: 220  loss: 0.005271098859932112  acc:  0.984375\n","ep: 6  bt: 240  loss: 0.0014629950341971023  acc:  1.0\n","ep: 6  bt: 260  loss: 0.27915191650390625  acc:  0.421875\n","ep: 6  bt: 280  loss: 0.0008598046134347501  acc:  1.0\n","ep: 6  bt: 300  loss: 0.23396255659020465  acc:  0.40625\n","ep: 6  bt: 320  loss: 0.0019113776152548583  acc:  1.0\n","ep: 6  bt: 340  loss: 0.42157289256220276  acc:  0.28125\n","ep: 6  bt: 360  loss: 0.0009134019846501558  acc:  1.0\n","ep: 6  bt: 380  loss: 0.34284506673398224  acc:  0.34375\n","ep: 6  bt: 400  loss: 0.0007348714153403821  acc:  1.0\n","ep: 6  bt: 420  loss: 0.0008727943767672  acc:  1.0\n","ep: 6  bt: 440  loss: 0.3222416587497877  acc:  0.328125\n","ep: 6  bt: 460  loss: 0.0008888038928094118  acc:  1.0\n","ep: 6  bt: 480  loss: 0.001017762510024983  acc:  1.0\n","ep: 6  bt: 500  loss: 0.0008359274462513301  acc:  1.0\n","ep: 6  bt: 520  loss: 0.1830498031947924  acc:  0.5\n","ep: 6  bt: 540  loss: 0.003840376501498015  acc:  0.984375\n","ep: 6  bt: 560  loss: 0.00232055329758188  acc:  1.0\n","ep: 6  bt: 580  loss: 0.0010616694293592286  acc:  1.0\n","ep: 6  bt: 600  loss: 0.2694092211516007  acc:  0.40625\n","ep: 6  bt: 620  loss: 0.29400176587312116  acc:  0.4375\n","ep: 6  bt: 640  loss: 0.0006630984337433525  acc:  1.0\n","ep: 6  bt: 660  loss: 0.0010955149874739025  acc:  1.0\n","ep: 6  bt: 680  loss: 0.0006001304187204527  acc:  1.0\n","ep: 6  bt: 700  loss: 0.29902006232220196  acc:  0.5\n","ep: 6  bt: 720  loss: 0.0007204619114813597  acc:  1.0\n","ep: 6  bt: 740  loss: 0.26656455579011334  acc:  0.5\n","ep: 6  bt: 760  loss: 0.3327035903930664  acc:  0.40625\n","ep: 6  bt: 780  loss: 0.001216796834183776  acc:  1.0\n","('<niroodhinche>________________', '<నిరోధించే>____________', '<నిరూధించే>____________')\n","('<sabhamukamgaa>_______________', '<సభాముకంగా>____________', '<సభముకంగా>_____________')\n","('<muppemundi>__________________', '<ముప్పేముంది>__________', '<ముప్పేముంది>__________')\n","('<zelly>_______________________', '<జెల్లీ>_______________', '<జెల్లీ>_______________')\n","('<vannelu>_____________________', '<వన్నెలు>______________', '<వన్నెలు>______________')\n","('<infester>____________________', '<ఇన్ఫెస్టర్>___________', '<ఇన్ఫెసెటర్>___________')\n","('<sakhyatha>___________________', '<సఖ్యత>________________', '<సఖఖ్యా>_______________')\n","('<nuuthi>______________________', '<నూతి>_________________', '<నూతి>_________________')\n","('<mookuduklo>__________________', '<మూకుడులో>_____________', '<మూకుడుక్లో>___________')\n","('<sangramamloo>________________', '<సంగ్రామంలో>___________', '<సంగ్రామంలో>___________')\n","('<telusukoovaalanipinchadu>____', '<తెలుసుకోవాలనిపించదు>__', '<తెలుసుకోవాలనించడు>____')\n","('<yerpariche>__________________', '<ఏర్పరిచే>_____________', '<యర్పరిచే>_____________')\n","('<rakshinchina>________________', '<రక్షించిన>____________', '<రక్షించిన>____________')\n","('<pravahinchaelaa>_____________', '<ప్రవహించేలా>__________', '<ప్రవహించేలా>__________')\n","('<caryala>_____________________', '<చర్యల>________________', '<కార్యలల>______________')\n","('<dampathyam>__________________', '<దాంపత్యం>_____________', '<దంపత్యం>______________')\n","('<dhraaksha>___________________', '<ద్రాక్ష>______________', '<ధ్రాక్ష>______________')\n","('<bheeshmuni>__________________', '<భీష్ముని>_____________', '<భీష్ముని>_____________')\n","('<kaadhal>_____________________', '<కాదల్>________________', '<కాధల్>________________')\n","('<pakutondani>_________________', '<పాకుతోందని>___________', '<పకకుతోందని>___________')\n","ep:  6  train acc: 0.72125  train loss: 0.13609443764126378  val acc: 0.41259765625  val loss: 0.29081958273182745\n","('<ruddochu>____________________', '<రుద్దొచ్చు>___________', '<రుద్దొచ్చు>___________')\n","('<takkuvayyaayi>_______________', '<తక్కువయ్యాయి>_________', '<తక్కువయ్యాయి>_________')\n","('<yalatippa>___________________', '<యాలతిప్ప>_____________', '<యలలిప్ప్>_____________')\n","('<dbt>_________________________', '<డిబిటి>_______________', '<డిబిిీ________________')\n","('<dasaabdulaloonu>_____________', '<దశాబ్దులలోను>_________', '<దశాబ్దులలోను>_________')\n","('<lembus>______________________', '<లెంబస్>_______________', '<లింబబ్స్______________')\n","('<koorgeeyulu>_________________', '<కూర్గీయులు>___________', '<కూర్గీయులు>___________')\n","('<lings>_______________________', '<లింగ్స్>______________', '<లింగ్స్స్_____________')\n","('<kougilinchukuntundo>_________', '<కౌగిలించుకుంటుందో>____', '<కౌగిలించుకుంటుందో>____')\n","('<padadaluchukokapovadame>_____', '<పాడదలుచుకోకపోవడమే>____', '<పదదలలచచకకోోవవమే>______')\n","('<rokshaannu>__________________', '<రోక్షాన్ను>___________', '<రోక్షాన్ను>___________')\n","('<vetanamlonchi>_______________', '<వేతనంలోంచి>___________', '<వేతనంలోంచి>___________')\n","('<formalin>____________________', '<ఫార్మలిన్>____________', '<ఫార్మలలిన్>___________')\n","('<malboro>_____________________', '<మల్బోరో>______________', '<మాబ్బోరో>_____________')\n","('<marooviepu>__________________', '<మరోవైపు>______________', '<మరూవిపే>______________')\n","('<mouchat>_____________________', '<మౌచాట్>_______________', '<మౌచ్>్________________')\n","('<voorikellinapudu>____________', '<వూరికెళ్ళినపుడు>______', '<వూరికెళ్లినపుడు>______')\n","('<labdipondaro>________________', '<లబ్దిపొందారో>_________', '<లబ్దిపోందరో>__________')\n","('<pogayye>_____________________', '<పోగయ్యే>______________', '<పోగాయయయ>>_____________')\n","('<swaarthamlo>_________________', '<స్వార్థంలో>___________', '<స్వార్థంలో>___________')\n","ep: 7  bt: 0  loss: 0.2751059739486031  acc:  0.46875\n","ep: 7  bt: 20  loss: 0.0015231455149857895  acc:  1.0\n","ep: 7  bt: 40  loss: 0.0006411985048781271  acc:  1.0\n","ep: 7  bt: 60  loss: 0.0008616866138966187  acc:  1.0\n","ep: 7  bt: 80  loss: 0.2041424668353537  acc:  0.53125\n","ep: 7  bt: 100  loss: 0.4514519650003184  acc:  0.28125\n","ep: 7  bt: 120  loss: 0.2945212695909583  acc:  0.4375\n","ep: 7  bt: 140  loss: 0.0013425415786712067  acc:  1.0\n","ep: 7  bt: 160  loss: 0.355113527049189  acc:  0.34375\n","ep: 7  bt: 180  loss: 0.001368388047684794  acc:  1.0\n","ep: 7  bt: 200  loss: 0.247323056925898  acc:  0.453125\n","ep: 7  bt: 220  loss: 0.001023594451987225  acc:  1.0\n","ep: 7  bt: 240  loss: 0.0007685967113660729  acc:  1.0\n","ep: 7  bt: 260  loss: 0.0007504349493462106  acc:  1.0\n","ep: 7  bt: 280  loss: 0.3722276272981063  acc:  0.4375\n","ep: 7  bt: 300  loss: 0.2300756081290867  acc:  0.375\n","ep: 7  bt: 320  loss: 0.0006141414379943972  acc:  1.0\n","ep: 7  bt: 340  loss: 0.22153823272041653  acc:  0.484375\n","ep: 7  bt: 360  loss: 0.2988728025685186  acc:  0.453125\n","ep: 7  bt: 380  loss: 0.0011464575224596522  acc:  1.0\n","ep: 7  bt: 400  loss: 0.0017314966282118921  acc:  1.0\n","ep: 7  bt: 420  loss: 0.2867549398671026  acc:  0.5\n","ep: 7  bt: 440  loss: 0.0015134583024874978  acc:  1.0\n","ep: 7  bt: 460  loss: 0.2867641656295113  acc:  0.390625\n","ep: 7  bt: 480  loss: 0.21243700773819632  acc:  0.421875\n","ep: 7  bt: 500  loss: 0.3001925634301227  acc:  0.375\n","ep: 7  bt: 520  loss: 0.0010419931748638983  acc:  1.0\n","ep: 7  bt: 540  loss: 0.2501445023909859  acc:  0.421875\n","ep: 7  bt: 560  loss: 0.0023801736533641815  acc:  0.984375\n","ep: 7  bt: 580  loss: 0.0006829589281393135  acc:  1.0\n","ep: 7  bt: 600  loss: 0.3027579473412555  acc:  0.40625\n","ep: 7  bt: 620  loss: 0.33959900814553967  acc:  0.453125\n","ep: 7  bt: 640  loss: 0.2103343217269234  acc:  0.53125\n","ep: 7  bt: 660  loss: 0.32755488934724225  acc:  0.375\n","ep: 7  bt: 680  loss: 0.27763068157693616  acc:  0.328125\n","ep: 7  bt: 700  loss: 0.2612501641978388  acc:  0.546875\n","ep: 7  bt: 720  loss: 0.0013485128140967825  acc:  0.984375\n","ep: 7  bt: 740  loss: 0.44113648456075916  acc:  0.3125\n","ep: 7  bt: 760  loss: 0.0007805868821299595  acc:  1.0\n","ep: 7  bt: 780  loss: 0.0007973122208014778  acc:  1.0\n","('<ayyangaar>___________________', '<అయ్యంగార్>____________', '<అయ్యాగార్>____________')\n","('<bharathiloo>_________________', '<భారతిలో>______________', '<భాతథలో>_______________')\n","('<unnaava>_____________________', '<ఉన్నావ>_______________', '<ఉన్నావ>_______________')\n","('<varadhi>_____________________', '<వారధి>________________', '<వరాధి>________________')\n","('<asthipanjaraalachae>_________', '<అస్థిపంజరాలచే>________', '<అస్థిపంజరాలచే>________')\n","('<pakshaallo>__________________', '<పక్షాల్లో>____________', '<పక్షాల్లో>____________')\n","('<viktari>_____________________', '<విక్టరీ>______________', '<విక్తరి>______________')\n","('<filmsh>______________________', '<ఫిల్మ్ష్>_____________', '<ఫిల్మ్ష్>_____________')\n","('<nivarinchi>__________________', '<నివారించి>____________', '<నివరించి>_____________')\n","('<nadipai>_____________________', '<నదిపై>________________', '<నడిపై>________________')\n","('<ohu>_________________________', '<ఓహొ>__________________', '<ఓహు>__________________')\n","('<uchchaaranha>________________', '<ఉచ్చారణ>______________', '<ఉచ్చారన>______________')\n","('<voting>______________________', '<వోటింగ్>______________', '<వోటింగ్>్_____________')\n","('<aasistunnaamu>_______________', '<ఆశిస్తున్నాము>________', '<ఆసిస్తున్నాము>________')\n","('<rowdylu>_____________________', '<రౌడీలు>_______________', '<రౌడ్ల్లు>_____________')\n","('<abhivruddiki>________________', '<అభివృద్దికి>__________', '<అభివృద్దికి>__________')\n","('<sagauravangaa>_______________', '<సగౌరవంగా>_____________', '<సగౌరవంగా>_____________')\n","('<tempoo>______________________', '<టెంపో>________________', '<తెంపూ>________________')\n","('<parisheelistoo>______________', '<పరిశీలిస్తూ>__________', '<పరిషీలిస్తో>__________')\n","('<nimushaalalo>________________', '<నిముషాలలో>____________', '<నిముషాలలో>____________')\n","ep:  7  train acc: 0.7083203125  train loss: 0.1422295037514287  val acc: 0.398193359375  val loss: 0.29292873714281165\n","('<cancomiumto>_________________', '<కాంకోమియంతో>__________', '<కాంకంమమమయంట>ో_________')\n","('<thuria>______________________', '<థురియా>_______________', '<థురియా>_______________')\n","('<teevrangaapariganinchina>____', '<తీవ్రంగాపరిగణించిన>___', '<తీవ్రంగాపరిించిన>_____')\n","('<ooregadame>__________________', '<ఊరేగడమే>______________', '<ఊరేగదమే>______________')\n","('<pattanundadamtho>____________', '<పట్టనుండడంతో>_________', '<పట్టణుండడంతో>_________')\n","('<nirvaristaarane>_____________', '<నిర్వరిస్తారనే>_______', '<నిర్వరిస్తారనే>_______')\n","('<eragamu>_____________________', '<ఎరగము>________________', '<ఏరగము>________________')\n","('<cheseyyochule>_______________', '<చేసెయ్యొచ్చులే>_______', '<చేసేయ్యోచ్చులే>_______')\n","('<nartinchaalsinde>____________', '<నర్తించాల్సిందే>______', '<నర్తించాల్సిందే>______')\n","('<ruchirartha>_________________', '<రుచిరార్థ>____________', '<రుచిర్రథథ>____________')\n","('<doosukellipotondi>___________', '<దూసుకెళ్లిపోతోంది>____', '<దూసుకెళ్లిపోతోంది>____')\n","('<panicheeyaalantee>___________', '<పనిచేయాలంటే>__________', '<పనిచేయాలంటీ>__________')\n","('<tangaili>____________________', '<తంగైలీ>_______________', '<తంగైలి>_______________')\n","('<iruvazjinjpuzha>_____________', '<ఇరువజింజ్పుజ>_________', '<ఇరువజ్జింజ్పు>________')\n","('<sylbert>_____________________', '<సిల్బెర్ట్>___________', '<సిలబబర్ట్>____________')\n","('<seetakalyanamu>______________', '<సీతాకల్యాణము>_________', '<సీటకల్యామము>__________')\n","('<sadurni>_____________________', '<సదుర్ని>______________', '<సదుర్ని>______________')\n","('<beerannalaku>________________', '<బీరన్నలకు>____________', '<బీరన్నలకు>____________')\n","('<murisipoyenduku>_____________', '<మురిసిపోయేందుకు>______', '<మురిసిపోయేందుకు>______')\n","('<katoju>______________________', '<కటోజు>________________', '<కటోజు>________________')\n","ep: 8  bt: 0  loss: 0.2814908442289933  acc:  0.40625\n","ep: 8  bt: 20  loss: 0.29126308275305707  acc:  0.359375\n","ep: 8  bt: 40  loss: 0.33905371375705884  acc:  0.4375\n","ep: 8  bt: 60  loss: 0.0008342223160940667  acc:  1.0\n","ep: 8  bt: 80  loss: 0.0008187881954338239  acc:  1.0\n","ep: 8  bt: 100  loss: 0.2989545490430749  acc:  0.46875\n","ep: 8  bt: 120  loss: 0.2645588543104089  acc:  0.375\n","ep: 8  bt: 140  loss: 0.0006447399197065312  acc:  1.0\n","ep: 8  bt: 160  loss: 0.2733780404795771  acc:  0.390625\n","ep: 8  bt: 180  loss: 0.0008478218122668889  acc:  1.0\n","ep: 8  bt: 200  loss: 0.31538880389669666  acc:  0.546875\n","ep: 8  bt: 220  loss: 0.3194311390752378  acc:  0.296875\n","ep: 8  bt: 240  loss: 0.0011488681092210438  acc:  1.0\n","ep: 8  bt: 260  loss: 0.002908994646176048  acc:  0.984375\n","ep: 8  bt: 280  loss: 0.3282483764316725  acc:  0.40625\n","ep: 8  bt: 300  loss: 0.0012781625694554784  acc:  1.0\n","ep: 8  bt: 320  loss: 0.5400917633720066  acc:  0.15625\n","ep: 8  bt: 340  loss: 0.3438528517018194  acc:  0.46875\n","ep: 8  bt: 360  loss: 0.23339109835417374  acc:  0.421875\n","ep: 8  bt: 380  loss: 0.31816308394722315  acc:  0.46875\n","ep: 8  bt: 400  loss: 0.0017478007661259692  acc:  1.0\n","ep: 8  bt: 420  loss: 0.28701062824415124  acc:  0.359375\n","ep: 8  bt: 440  loss: 0.000796416774392128  acc:  1.0\n","ep: 8  bt: 460  loss: 0.001580501218204913  acc:  0.984375\n","ep: 8  bt: 480  loss: 0.24098902163298233  acc:  0.40625\n","ep: 8  bt: 500  loss: 0.2601629754771357  acc:  0.59375\n","ep: 8  bt: 520  loss: 0.0005935643921079843  acc:  1.0\n","ep: 8  bt: 540  loss: 0.0008669358718654383  acc:  1.0\n","ep: 8  bt: 560  loss: 0.24667072296142578  acc:  0.484375\n","ep: 8  bt: 580  loss: 0.0007658808127693508  acc:  1.0\n","ep: 8  bt: 600  loss: 0.21154156975124194  acc:  0.5625\n","ep: 8  bt: 620  loss: 0.0009694775647443274  acc:  1.0\n","ep: 8  bt: 640  loss: 0.0021794491811938906  acc:  0.984375\n","ep: 8  bt: 660  loss: 0.23448585427325705  acc:  0.515625\n","ep: 8  bt: 680  loss: 0.28953311754309613  acc:  0.390625\n","ep: 8  bt: 700  loss: 0.3160534734311311  acc:  0.375\n","ep: 8  bt: 720  loss: 0.0006928992984087571  acc:  1.0\n","ep: 8  bt: 740  loss: 0.0008378130910189255  acc:  1.0\n","ep: 8  bt: 760  loss: 0.2921121224113133  acc:  0.515625\n","ep: 8  bt: 780  loss: 0.0008760070347267648  acc:  1.0\n","('<korikalu>____________________', '<కోరికలు>______________', '<కొరికలు>______________')\n","('<maadhavarao>_________________', '<మాధవరావు>_____________', '<మాధవరవవ>______________')\n","('<garvamga>____________________', '<గర్వంగా>______________', '<గర్వంగా>______________')\n","('<aaropanalatone>______________', '<ఆరోపణలతోనే>___________', '<ఆరోపనలతోనే>___________')\n","('<amaruchu>____________________', '<అమరుచు>_______________', '<అమరుచు>_______________')\n","('<poojinchee>__________________', '<పూజించే>______________', '<పూజించే>______________')\n","('<tayaarayina>_________________', '<తయారయిన>______________', '<తయారయిన>______________')\n","('<pakutondani>_________________', '<పాకుతోందని>___________', '<పకుుోందని>____________')\n","('<gelavalekapoyanannaru>_______', '<గెలవలేకపోయానన్నారు>___', '<గెలవలేకపోయానన్నారు>___')\n","('<srila>_______________________', '<శ్రీల>________________', '<శ్రీల>________________')\n","('<saraswatikuda>_______________', '<సరస్వతికూడా>__________', '<సరస్వతికుడా>__________')\n","('<praatipadika>________________', '<ప్రాతిపదిక>___________', '<ప్రాతిపదిక>___________')\n","('<raayis>______________________', '<రాయిస్>_______________', '<రాయిస్>_______________')\n","('<noppentani>__________________', '<నొప్పేంటని>___________', '<నొప్పెంటని>___________')\n","('<aethar>______________________', '<ఈథర్>_________________', '<ఈతర్>_________________')\n","('<crajy>_______________________', '<క్రేజీ>_______________', '<క్రాజీ>_______________')\n","('<vedajallutune>_______________', '<వెదజల్లుతూనే>_________', '<వేదజల్లుతునే>_________')\n","('<akramamgaa>__________________', '<అక్రమంగా>_____________', '<అక్రమంగా>_____________')\n","('<vijayaalu>___________________', '<విజయాలు>______________', '<విజయాలు>______________')\n","('<prajektula>__________________', '<ప్రాజెక్టుల>__________', '<ప్రజేక్తుల>___________')\n","ep:  8  train acc: 0.72408203125  train loss: 0.13524183839093887  val acc: 0.410400390625  val loss: 0.2928167218747346\n","('<gurraalatandaaku>____________', '<గుర్రాలతండాకు>________', '<గుర్రాలతందాకు>________')\n","('<kavalantunnava>______________', '<కావాలంటున్నావా>_______', '<కవవాంటున్నవవా_________')\n","('<gorillaallo>_________________', '<గొరిల్లాల్లో>_________', '<గోరిళ్లాళ్లో>_________')\n","('<monteed>_____________________', '<మాంటీడ్>______________', '<మోంటీడ్>______________')\n","('<shootinguku>_________________', '<షూటింగుకు>____________', '<షూటింగుకు>____________')\n","('<chuttukovadaanni>____________', '<చుట్టుకోవడాన్ని>______', '<చుట్టుకోవడాన్ని>______')\n","('<drukpathapu>_________________', '<దృక్పథపు>_____________', '<దృక్పథపు>_____________')\n","('<artharahitamainavi>__________', '<అర్థరహితమైనవి>________', '<అర్థరహితమైనవి_________')\n","('<glamarasamtho>_______________', '<గ్లామరసంతో>___________', '<గ్లామరసంతో>___________')\n","('<vibhavambulu>________________', '<విభవంబులు>____________', '<విభవంబులు>____________')\n","('<drhushti>____________________', '<దృష్టి>_______________', '<ద్రుష్టి>_____________')\n","('<lekkinchaleka>_______________', '<లెక్కించలేక>__________', '<లెక్కించలేక>__________')\n","('<prayoginchinatlugaa>_________', '<ప్రయోగించినట్లుగా>____', '<ప్రయోగించినట్లుగా>____')\n","('<unstarstat>__________________', '<అన్స్టార్స్టాట్>______', '<ఉన్స్టార్స్టట్్_______')\n","('<adugubhaagamlo>______________', '<అడుగుభాగంలో>__________', '<అదుగుభాగంలో>__________')\n","('<rakshasapalanalo>____________', '<రాక్షసపాలనలో>_________', '<రక్షసపపలనలో>__________')\n","('<prashnichadam>_______________', '<ప్రశ్నిచడం>___________', '<ప్రశ్నిచదం>___________')\n","('<melodramalanu>_______________', '<మెలోడ్రామాలను>________', '<మెలోద్రమలను>__________')\n","('<nindituni>___________________', '<నిందితుని>____________', '<నిందితుని>____________')\n","('<mascardi>____________________', '<మస్కార్డి>____________', '<మసస్కార్డి>___________')\n","ep: 9  bt: 0  loss: 0.2448937581933063  acc:  0.484375\n","ep: 9  bt: 20  loss: 0.0006568162175624267  acc:  1.0\n","ep: 9  bt: 40  loss: 0.2633301486139712  acc:  0.5\n","ep: 9  bt: 60  loss: 0.00047504808753728867  acc:  1.0\n","ep: 9  bt: 80  loss: 0.41579271399456524  acc:  0.390625\n","ep: 9  bt: 100  loss: 0.0014426975470522175  acc:  1.0\n","ep: 9  bt: 120  loss: 0.0006584225465422091  acc:  1.0\n","ep: 9  bt: 140  loss: 0.2491078169449516  acc:  0.453125\n","ep: 9  bt: 160  loss: 0.27333327998285706  acc:  0.40625\n","ep: 9  bt: 180  loss: 0.25348323324452277  acc:  0.484375\n","ep: 9  bt: 200  loss: 0.0008865470957496892  acc:  1.0\n","ep: 9  bt: 220  loss: 0.0005860095033827035  acc:  1.0\n","ep: 9  bt: 240  loss: 0.002135378025148226  acc:  1.0\n","ep: 9  bt: 260  loss: 0.35460758209228516  acc:  0.359375\n","ep: 9  bt: 280  loss: 0.2555795959804369  acc:  0.453125\n","ep: 9  bt: 300  loss: 0.0007558069799257362  acc:  1.0\n","ep: 9  bt: 320  loss: 0.0007973580580690633  acc:  1.0\n","ep: 9  bt: 340  loss: 0.0009258636475905129  acc:  1.0\n","ep: 9  bt: 360  loss: 0.2214468873065451  acc:  0.421875\n","ep: 9  bt: 380  loss: 0.2407930208289105  acc:  0.453125\n","ep: 9  bt: 400  loss: 0.28409184580263885  acc:  0.296875\n","ep: 9  bt: 420  loss: 0.0012580184670894043  acc:  1.0\n","ep: 9  bt: 440  loss: 0.0008149296045303345  acc:  1.0\n","ep: 9  bt: 460  loss: 0.003281954190005427  acc:  0.984375\n","ep: 9  bt: 480  loss: 0.0006459277204197386  acc:  1.0\n","ep: 9  bt: 500  loss: 0.21322609030682108  acc:  0.484375\n","ep: 9  bt: 520  loss: 0.0005831701842987019  acc:  1.0\n","ep: 9  bt: 540  loss: 0.28076882984327234  acc:  0.484375\n","ep: 9  bt: 560  loss: 0.0009827250209839447  acc:  1.0\n","ep: 9  bt: 580  loss: 0.3026882669200068  acc:  0.4375\n","ep: 9  bt: 600  loss: 0.0006024050323859505  acc:  1.0\n","ep: 9  bt: 620  loss: 0.0012428660107695539  acc:  1.0\n","ep: 9  bt: 640  loss: 0.23852804432744565  acc:  0.46875\n","ep: 9  bt: 660  loss: 0.0006871810587851897  acc:  1.0\n","ep: 9  bt: 680  loss: 0.0008009509385927864  acc:  1.0\n","ep: 9  bt: 700  loss: 0.0023746185976526012  acc:  0.984375\n","ep: 9  bt: 720  loss: 0.28857614683068317  acc:  0.359375\n","ep: 9  bt: 740  loss: 0.0011451362591722736  acc:  1.0\n","ep: 9  bt: 760  loss: 0.000804075323369192  acc:  1.0\n","ep: 9  bt: 780  loss: 0.32973532054735266  acc:  0.484375\n","('<punarvaasukaartelo>__________', '<పునర్వాసుకార్తెలో>____', '<పునర్వాసుకార్టేలో>____')\n","('<pradarsinchabadadam>_________', '<ప్రదర్శించబడడం>_______', '<ప్రదర్శించబడడం>_______')\n","('<prajasyaamyaanni>____________', '<ప్రజాస్యామ్యాన్ని>____', '<ప్రజస్యామ్యాన్ని>_____')\n","('<ghanathanu>__________________', '<ఘనతను>________________', '<ఘనతతను>_______________')\n","('<vaaradhiga>__________________', '<వారధిగా>______________', '<వారధధిగ>______________')\n","('<aatmarakshana>_______________', '<ఆత్మరక్షణ>____________', '<ఆత్మరక్షణ>____________')\n","('<sathya>______________________', '<సత్య>_________________', '<సథ్యా_________________')\n","('<kommanu>_____________________', '<కొమ్మను>______________', '<కొమ్మను>______________')\n","('<yuddhaaniki>_________________', '<యుద్ధానికి>___________', '<యుద్ధానికి>___________')\n","('<libya>_______________________', '<లిబియా>_______________', '<లిబ్యా>_______________')\n","('<chethabadi>__________________', '<చేతబడి>_______________', '<చేతబడి>_______________')\n","('<mahabharathamlo>_____________', '<మహాభారతంలో>___________', '<మహాభతంంలో>____________')\n","('<aenugu>______________________', '<ఏనుగు>________________', '<ఏనుగు>________________')\n","('<bhaavaatmakam>_______________', '<భావాత్మకం>____________', '<భావాత్మకం>____________')\n","('<desha>_______________________', '<దేశ>__________________', '<దేషా__________________')\n","('<bhaagavatham>________________', '<భాగవతం>_______________', '<భాగవతం>_______________')\n","('<alen>________________________', '<అలెన్>________________', '<అలెన్>________________')\n","('<yuvatapie>___________________', '<యువతపై>_______________', '<యువతపి>_______________')\n","('<chaya>_______________________', '<ఛాయా>_________________', '<చయయా__________________')\n","('<minto>_______________________', '<మింటో>________________', '<మింటో>________________')\n","ep:  9  train acc: 0.7304296875  train loss: 0.13196934065093163  val acc: 0.410400390625  val loss: 0.2803390751714292\n","('<balaannicchinatlu>___________', '<బలాన్నిచ్చినట్లు>_____', '<బలాన్నిచ్చినట్లు>_____')\n","('<dappule>_____________________', '<డప్పులే>______________', '<దప్పులే>______________')\n","('<pareekshinchivaddani>________', '<పరీక్షించివద్దని>_____', '<పరీక్షించివద్దని>_____')\n","('<ammakaladaarudigaa>__________', '<అమ్మకాలదారుడిగా>______', '<అమ్మకలదారుడిగా>_______')\n","('<antigenlu>___________________', '<యాంటీజెన్లు>__________', '<అంటిగెన్లు>___________')\n","('<kodene>______________________', '<కోడెనే>_______________', '<కోడెనే>_______________')\n","('<puraskaanni>_________________', '<పురస్కాన్ని>__________', '<పురస్కాన్ని>__________')\n","('<vadilivelite>________________', '<వదిలివెళితే>__________', '<వదిలివేలితే>__________')\n","('<kodimba>_____________________', '<కొడింబా>______________', '<కొడింబా>______________')\n","('<vennupuusalooni>_____________', '<వెన్నుపూసలోని>________', '<వెన్నుపూసలోని>________')\n","('<swargalokaadulanu>___________', '<స్వర్గలోకాదులను>______', '<స్వర్గలోకాదులను>______')\n","('<paedavaaram>_________________', '<పేదవారం>______________', '<పేదవారం>______________')\n","('<hubert>______________________', '<హుబెర్ట్>_____________', '<హబబె్్ట_______________')\n","('<anigrahudiki>________________', '<అనిగ్రహుడికి>_________', '<అనిగ్రహుడికి>_________')\n","('<nannaandhra>_________________', '<నాన్నఆంధ్ర>___________', '<నన్నాంధ్ర>____________')\n","('<naagalingaanni>______________', '<నాగలింగాన్ని>_________', '<నాగలింగాన్ని>_________')\n","('<okavaipayithe>_______________', '<ఒకవైపయితే>____________', '<ఓకవైపయితే>____________')\n","('<mettanayina>_________________', '<మెత్తనయిన>____________', '<మెట్టనయిన>____________')\n","('<subbaramayyagari>____________', '<సుబ్బరామయ్యగారి>______', '<సుబ్బరమయ్యారి>________')\n","('<paharaalo>___________________', '<పహరాలో>_______________', '<పహరాలో>_______________')\n","ep: 10  bt: 0  loss: 0.3018424199975055  acc:  0.421875\n","ep: 10  bt: 20  loss: 0.0007621564133011776  acc:  1.0\n","ep: 10  bt: 40  loss: 0.22565097394196884  acc:  0.484375\n","ep: 10  bt: 60  loss: 0.0006274053341020708  acc:  1.0\n","ep: 10  bt: 80  loss: 0.0006835973295180694  acc:  1.0\n","ep: 10  bt: 100  loss: 0.2027373106583305  acc:  0.5\n","ep: 10  bt: 120  loss: 0.19370885517286218  acc:  0.546875\n","ep: 10  bt: 140  loss: 0.0006879281414591748  acc:  1.0\n","ep: 10  bt: 160  loss: 0.3030531924703847  acc:  0.359375\n","ep: 10  bt: 180  loss: 0.000515403226017952  acc:  1.0\n","ep: 10  bt: 200  loss: 0.000574460172134897  acc:  1.0\n","ep: 10  bt: 220  loss: 0.3084475061167841  acc:  0.359375\n","ep: 10  bt: 240  loss: 0.22993643387504245  acc:  0.546875\n","ep: 10  bt: 260  loss: 0.32454494808031165  acc:  0.375\n","ep: 10  bt: 280  loss: 0.0008667533326408137  acc:  1.0\n","ep: 10  bt: 300  loss: 0.2518663199051567  acc:  0.515625\n","ep: 10  bt: 320  loss: 0.0008537855161272961  acc:  1.0\n","ep: 10  bt: 340  loss: 0.0008436775887789934  acc:  1.0\n","ep: 10  bt: 360  loss: 0.0005246928852537404  acc:  1.0\n","ep: 10  bt: 380  loss: 0.0009378868600596552  acc:  1.0\n","ep: 10  bt: 400  loss: 0.24264992838320526  acc:  0.421875\n","ep: 10  bt: 420  loss: 0.0010308280749165494  acc:  1.0\n","ep: 10  bt: 440  loss: 0.0005336689317355986  acc:  1.0\n","ep: 10  bt: 460  loss: 0.20961172684379245  acc:  0.46875\n","ep: 10  bt: 480  loss: 0.0009882284895233486  acc:  1.0\n","ep: 10  bt: 500  loss: 0.2526196811510169  acc:  0.34375\n","ep: 10  bt: 520  loss: 0.34692219029302185  acc:  0.359375\n","ep: 10  bt: 540  loss: 0.0008412239992100259  acc:  1.0\n","ep: 10  bt: 560  loss: 0.26800808699234674  acc:  0.421875\n","ep: 10  bt: 580  loss: 0.0014292882836383321  acc:  1.0\n","ep: 10  bt: 600  loss: 0.27417137311852496  acc:  0.484375\n","ep: 10  bt: 620  loss: 0.1700413641722306  acc:  0.53125\n","ep: 10  bt: 640  loss: 0.2306037778439729  acc:  0.453125\n","ep: 10  bt: 660  loss: 0.15141576269398566  acc:  0.578125\n","ep: 10  bt: 680  loss: 0.0005842093378305435  acc:  1.0\n","ep: 10  bt: 700  loss: 0.2671498215716818  acc:  0.453125\n","ep: 10  bt: 720  loss: 0.37446984000827954  acc:  0.421875\n","ep: 10  bt: 740  loss: 0.0015810135266055231  acc:  0.984375\n","ep: 10  bt: 760  loss: 0.0008360288389351057  acc:  1.0\n","ep: 10  bt: 780  loss: 0.2185586639072584  acc:  0.4375\n","('<saakshyangaa>________________', '<సాక్ష్యంగా>___________', '<సాక్ష్యంగా>___________')\n","('<vyasanalaku>_________________', '<వ్యసనాలకు>____________', '<వ్యసనలలక>_____________')\n","('<mahadeva>____________________', '<మహాదేవ>_______________', '<మహాడవవ>_______________')\n","('<poinatla>____________________', '<పోయినట్లా>____________', '<పోయినట్ల>_____________')\n","('<vishnubhaktudaina>___________', '<విష్ణుభక్తుడైన>_______', '<విష్ణుభక్తుడైన>_______')\n","('<thaanaa>_____________________', '<తానా>_________________', '<తానా>_________________')\n","('<nacchindhani>________________', '<నచ్చిందని>____________', '<నచ్చింధని>____________')\n","('<miisaaluu>___________________', '<మీసాలు>_______________', '<మీసాలూ>_______________')\n","('<ramana>______________________', '<రమణా>_________________', '<రామా>_________________')\n","('<paninee>_____________________', '<పనినీ>________________', '<పనినీ>________________')\n","('<canadalo>____________________', '<కెనడాలో>______________', '<కనడాలో>_______________')\n","('<angeekarinchaadani>__________', '<అంగీకరించాడని>________', '<అంగీకరించాడని>________')\n","('<aajnaapinchaadu>_____________', '<ఆజ్ఞాపించాడు>_________', '<ఆజ్ఞాపించాడు>_________')\n","('<abhinandinchadamtopaatu>_____', '<అభినందించడంతోపాటు>____', '<అభినందించడంతోపాటు>____')\n","('<edlanu>______________________', '<ఎడ్లను>_______________', '<ఎడ్లను>_______________')\n","('<mahabharathamulo>____________', '<మహాభారతములో>__________', '<మహారతమమలోో>___________')\n","('<oligorarcular>_______________', '<ఒలిగోరార్కులర్>_______', '<ఆలిగోరార్కలర్>________')\n","('<kadhanaalanu>________________', '<కథనాలను>______________', '<కధనాలను>______________')\n","('<puraanhamulu>________________', '<పురాణములు>____________', '<పురాణంహులు>___________')\n","('<vrathamu>____________________', '<వ్రతము>_______________', '<వృరతము>_______________')\n","ep:  10  train acc: 0.71197265625  train loss: 0.13669023089571447  val acc: 0.39697265625  val loss: 0.28424074338830035\n","('<neetikaakini>________________', '<నీటికాకిని>___________', '<నీటికాకిని>___________')\n","('<eeyananta>___________________', '<ఈయనంత>________________', '<ఈయనంత>________________')\n","('<sedyapurangam>_______________', '<సేద్యపురంగం>__________', '<సేద్యపూరంం>>__________')\n","('<apoyindi>____________________', '<అపోయింది>_____________', '<ఆపోయింది>_____________')\n","('<vadalutaadu>_________________', '<వదలుతాడు>_____________', '<వదలుతాడు>_____________')\n","('<corge>_______________________', '<కార్జ్>_______________', '<కార్స్>_______________')\n","('<niluchuntunnara>_____________', '<నిలుచుంటున్నారా>______', '<నిలుచుంటున్నారా>______')\n","('<treatop>_____________________', '<ట్రీటాప్>_____________', '<ట్రీటోప్>_____________')\n","('<paeshentuu>__________________', '<పేషెంటూ>______________', '<పేషేంటూ>______________')\n","('<injeshan>____________________', '<ఇంజెషన్>______________', '<ఇంజేశనన>______________')\n","('<desadrohamenani>_____________', '<దేశద్రోహమేనని>________', '<దేశద్రోహమేనని>________')\n","('<akeshiyo>____________________', '<అకేషియో>______________', '<అకేషియో>______________')\n","('<veluvarinchabotundi>_________', '<వెలువరించబోతుంది>_____', '<వెలువరించబోతుంది>_____')\n","('<ramaniyartha>________________', '<రమణీయార్థ>____________', '<రామనియా్ర్>___________')\n","('<sambhandinchinanta>__________', '<సంభందించినంత>_________', '<సంభందించినంత>_________')\n","('<baddhamaipovaalo>____________', '<బద్ధమైపోవాలో>_________', '<బద్ధమైపోవాలో>_________')\n","('<baadhyatalane>_______________', '<బాధ్యతలనే>____________', '<బాధ్యతలనే>____________')\n","('<padnalugendlayina>___________', '<పద్నాలుగేండ్లయినా>____', '<పద్నలుగెంద్లాిన>______')\n","('<peddaayana>__________________', '<పెద్దాయన>_____________', '<పెద్దాయన>_____________')\n","('<ragilipoyevi>________________', '<రగిలిపోయేవి>__________', '<రాగలలిపోయేవి>_________')\n","ep: 11  bt: 0  loss: 0.2744929894157078  acc:  0.484375\n","ep: 11  bt: 20  loss: 0.0005929067973857341  acc:  1.0\n","ep: 11  bt: 40  loss: 0.0008292478387770445  acc:  1.0\n","ep: 11  bt: 60  loss: 0.16168917780337128  acc:  0.59375\n","ep: 11  bt: 80  loss: 0.0007803126683701639  acc:  1.0\n","ep: 11  bt: 100  loss: 0.00044768846229366636  acc:  1.0\n","ep: 11  bt: 120  loss: 0.0006904227092214253  acc:  1.0\n","ep: 11  bt: 140  loss: 0.0007092257880646249  acc:  1.0\n","ep: 11  bt: 160  loss: 0.0011075591749471166  acc:  1.0\n","ep: 11  bt: 180  loss: 0.0006539489992934724  acc:  1.0\n","ep: 11  bt: 200  loss: 0.0014684156555196514  acc:  1.0\n","ep: 11  bt: 220  loss: 0.2109016750169837  acc:  0.484375\n","ep: 11  bt: 240  loss: 0.19301758641782013  acc:  0.5\n","ep: 11  bt: 260  loss: 0.004118976061758788  acc:  1.0\n","ep: 11  bt: 280  loss: 0.001368044835069905  acc:  0.984375\n","ep: 11  bt: 300  loss: 0.2865840041119119  acc:  0.40625\n","ep: 11  bt: 320  loss: 0.18421857253364896  acc:  0.53125\n","ep: 11  bt: 340  loss: 0.0007823664371086204  acc:  1.0\n","ep: 11  bt: 360  loss: 0.0009615036618450414  acc:  1.0\n","ep: 11  bt: 380  loss: 0.18802089276521103  acc:  0.4375\n","ep: 11  bt: 400  loss: 0.13965928036233652  acc:  0.65625\n","ep: 11  bt: 420  loss: 0.1591924584430197  acc:  0.65625\n","ep: 11  bt: 440  loss: 0.0010605814016383627  acc:  1.0\n","ep: 11  bt: 460  loss: 0.0017634542739909628  acc:  0.984375\n","ep: 11  bt: 480  loss: 0.3428955078125  acc:  0.328125\n","ep: 11  bt: 500  loss: 0.2963853297026261  acc:  0.4375\n","ep: 11  bt: 520  loss: 0.0015057984577572863  acc:  0.984375\n","ep: 11  bt: 540  loss: 0.0012149956563244696  acc:  1.0\n","ep: 11  bt: 560  loss: 0.0007745285882897999  acc:  1.0\n","ep: 11  bt: 580  loss: 0.0007362565754548363  acc:  1.0\n","ep: 11  bt: 600  loss: 0.0022545461745365806  acc:  0.984375\n","ep: 11  bt: 620  loss: 0.0020451134313707767  acc:  0.984375\n","ep: 11  bt: 640  loss: 0.13408152953438138  acc:  0.53125\n","ep: 11  bt: 660  loss: 0.0012508216111556344  acc:  1.0\n","ep: 11  bt: 680  loss: 0.19043221681014352  acc:  0.375\n","ep: 11  bt: 700  loss: 0.2981230486994204  acc:  0.53125\n","ep: 11  bt: 720  loss: 0.0005520179300852444  acc:  1.0\n","ep: 11  bt: 740  loss: 0.2953534748243249  acc:  0.40625\n","ep: 11  bt: 760  loss: 0.0009336409199496974  acc:  1.0\n","ep: 11  bt: 780  loss: 0.0006307744137618853  acc:  1.0\n","('<sabhaapati>__________________', '<సభాపతి>_______________', '<సభాపతి>_______________')\n","('<thaedaani>___________________', '<తేడాని>_______________', '<తేదాని>_______________')\n","('<vadakamto>___________________', '<వాడకంతో>______________', '<వదకంంో>_______________')\n","('<minchakudadani>______________', '<మించకూడదని>___________', '<మించకూడదని>___________')\n","('<beckman>_____________________', '<బెక్మాన్>_____________', '<బెక్మాన్>_____________')\n","('<yendina>_____________________', '<ఎండిన>________________', '<యందదనన>_______________')\n","('<chesthunnadhi>_______________', '<చేస్తున్నది>__________', '<చేస్తున్నాధి>_________')\n","('<caruvu>______________________', '<కరువు>________________', '<కారువు>_______________')\n","('<oppukoodu>___________________', '<ఒప్పుకోడు>____________', '<ఒప్పుకోడు>____________')\n","('<rabbai>______________________', '<రబ్బాయి>______________', '<రాబబబయి>______________')\n","('<samudrateeraalloki>__________', '<సముద్రతీరాల్లోకి>_____', '<సముద్రతీరాల్లో>_______')\n","('<jamadagni>___________________', '<జమదగ్ని>______________', '<జమమాగ్ని>_____________')\n","('<anivaaryamaindoo>____________', '<అనివార్యమైందో>________', '<అనివార్యమైందో>________')\n","('<yash>________________________', '<యశ్>__________________', '<యా్్>_________________')\n","('<bhawanamu>___________________', '<భవనము>________________', '<భావమము>_______________')\n","('<pratiyap>____________________', '<ప్రతియాప్>____________', '<ప్రతియప్్_____________')\n","('<ayurvedamlo>_________________', '<ఆయుర్వేదంలో>__________', '<అయుర్వేదంలో>__________')\n","('<dheepaalaku>_________________', '<దీపాలకు>______________', '<ధీపాలకు>______________')\n","('<pampevallame>________________', '<పంపేవాళ్ళమే>__________', '<పంపేవాళ్మమే>__________')\n","('<naajar>______________________', '<నాజర్>________________', '<నాజర్>________________')\n","ep:  11  train acc: 0.72166015625  train loss: 0.13397161641702282  val acc: 0.37841796875  val loss: 0.3024521495984948\n","('<spandinchinanduke>___________', '<స్పందించినందుకే>______', '<స్పందించినందుకే>______')\n","('<poledanta>___________________', '<పోలేదంట>______________', '<పోలేదంట>______________')\n","('<netivi>______________________', '<నేటివి>_______________', '<నేటివి>_______________')\n","('<meekandajestamu>_____________', '<మీకందజేస్తాము>________', '<మీకందజేస్తాము>________')\n","('<gono>________________________', '<గొనో>_________________', '<గొనో>_________________')\n","('<formalin>____________________', '<ఫార్మలిన్>____________', '<ఫార్మలిన్>____________')\n","('<veyyakudada>_________________', '<వెయ్యకూడదా>___________', '<వెయ్యకూడదా>___________')\n","('<satyasodhaka>________________', '<సత్యశోధక>_____________', '<సత్యశోధక>_____________')\n","('<daatinappatikee>_____________', '<దాటినప్పటికీ>_________', '<దాటినప్పటికీ>_________')\n","('<niruktiki>___________________', '<నిరుక్తికి>___________', '<నిరుక్తికి>___________')\n","('<talapankistuu>_______________', '<తలపంకిస్తూ>___________', '<తలపంకిస్తూ>___________')\n","('<inkokkaru>___________________', '<ఇంకొక్కరు>____________', '<ఇంకొక్కరు>____________')\n","('<vivina>______________________', '<వివిన>________________', '<వివిన>________________')\n","('<namaskarincheppudu>__________', '<నమస్కరించేప్పుడు>_____', '<నమస్కరించేప్పుడు>_____')\n","('<oglu>________________________', '<ఒగ్లు>________________', '<ఒగ్లు>________________')\n","('<puurticheeyadaanni>__________', '<పూర్తిచేయడాన్ని>______', '<పూర్తిచేయడాన్ని>______')\n","('<veskunnaka>__________________', '<వేస్కున్నాక>__________', '<వేస్కున్నాక>__________')\n","('<takkuvachesinatte>___________', '<తక్కువచేసినట్టే>______', '<తక్కువచేసినట్టే>______')\n","('<wiest>_______________________', '<వైస్ట్>_______________', '<వైస్ట్>_______________')\n","('<shinoz>______________________', '<షినోజ్>_______________', '<షినోజ్>_______________')\n","ep: 12  bt: 0  loss: 0.0005396593605046687  acc:  1.0\n","ep: 12  bt: 20  loss: 0.0005267211033598236  acc:  1.0\n","ep: 12  bt: 40  loss: 0.000998233647450157  acc:  1.0\n","ep: 12  bt: 60  loss: 0.21319870326829993  acc:  0.515625\n","ep: 12  bt: 80  loss: 0.25999960692032525  acc:  0.484375\n","ep: 12  bt: 100  loss: 0.21408920702726944  acc:  0.46875\n","ep: 12  bt: 120  loss: 0.26610766286435333  acc:  0.59375\n","ep: 12  bt: 140  loss: 0.20826148986816406  acc:  0.375\n","ep: 12  bt: 160  loss: 0.25496578216552734  acc:  0.53125\n","ep: 12  bt: 180  loss: 0.0008386963087579478  acc:  1.0\n","ep: 12  bt: 200  loss: 0.0005953496969912363  acc:  1.0\n","ep: 12  bt: 220  loss: 0.25359083258587384  acc:  0.46875\n","ep: 12  bt: 240  loss: 0.21088620890741763  acc:  0.578125\n","ep: 12  bt: 260  loss: 0.0006339729399136875  acc:  1.0\n","ep: 12  bt: 280  loss: 0.2166517506475034  acc:  0.53125\n","ep: 12  bt: 300  loss: 0.2030782699584961  acc:  0.546875\n","ep: 12  bt: 320  loss: 0.24457382119220236  acc:  0.40625\n","ep: 12  bt: 340  loss: 0.30182757584945014  acc:  0.4375\n","ep: 12  bt: 360  loss: 0.0005082742756475573  acc:  1.0\n","ep: 12  bt: 380  loss: 0.001021884300786516  acc:  1.0\n","ep: 12  bt: 400  loss: 0.1923373263815175  acc:  0.515625\n","ep: 12  bt: 420  loss: 0.3421312207761018  acc:  0.4375\n","ep: 12  bt: 440  loss: 0.2327476584393045  acc:  0.484375\n","ep: 12  bt: 460  loss: 0.27129268646240234  acc:  0.40625\n","ep: 12  bt: 480  loss: 0.2016934933869735  acc:  0.4375\n","ep: 12  bt: 500  loss: 0.2883896827697754  acc:  0.40625\n","ep: 12  bt: 520  loss: 0.0008321921948505485  acc:  1.0\n","ep: 12  bt: 540  loss: 0.21329110601673956  acc:  0.484375\n","ep: 12  bt: 560  loss: 0.18803082341733185  acc:  0.53125\n","ep: 12  bt: 580  loss: 0.0015733664774376414  acc:  0.984375\n","ep: 12  bt: 600  loss: 0.0007215070983637934  acc:  1.0\n","ep: 12  bt: 620  loss: 0.0005894526434333428  acc:  1.0\n","ep: 12  bt: 640  loss: 0.001131991491369579  acc:  1.0\n","ep: 12  bt: 660  loss: 0.2681332256482995  acc:  0.40625\n","ep: 12  bt: 680  loss: 0.000627682665767877  acc:  1.0\n","ep: 12  bt: 700  loss: 0.000556687136059222  acc:  1.0\n","ep: 12  bt: 720  loss: 0.2714854116025178  acc:  0.5\n","ep: 12  bt: 740  loss: 0.00045763438000627184  acc:  1.0\n","ep: 12  bt: 760  loss: 0.0005696468136232833  acc:  1.0\n","ep: 12  bt: 780  loss: 0.0006697914846565412  acc:  1.0\n","('<gowtama>_____________________', '<గౌతమ>_________________', '<గౌతమమ>________________')\n","('<pattaalandinchaaru>__________', '<పట్టాలందించారు>_______', '<పట్టాలందించారు>_______')\n","('<kolandi>_____________________', '<కొలంది>_______________', '<కొలంది>_______________')\n","('<paarse>______________________', '<పార్సీ>_______________', '<పార్సే>_______________')\n","('<dheshaalaloo>________________', '<దేశాలలో>______________', '<ధేశాలలో>______________')\n","('<itivruttamto>________________', '<ఇతివృత్తంతో>__________', '<ఇటివృత్తంతో>__________')\n","('<slangulo>____________________', '<స్లాంగులో>____________', '<స్లాగులో>_____________')\n","('<sankshemaniki>_______________', '<సంక్షేమానికి>_________', '<సంక్షేమనిికి>_________')\n","('<varasalu>____________________', '<వరసలు>________________', '<వరసలు>________________')\n","('<leout>_______________________', '<లేఅవుట్>______________', '<లేయూట్>_______________')\n","('<this>________________________', '<థిస్>_________________', '<థిస్>_________________')\n","('<nirodhinche>_________________', '<నిరోధించే>____________', '<నిరోధించే>____________')\n","('<gramalapai>__________________', '<గ్రామాలపై>____________', '<గ్రామలపై>_____________')\n","('<mayuura>_____________________', '<మయూర>_________________', '<మాూూర>________________')\n","('<obidableu>___________________', '<ఒబిడబ్ల్యు>___________', '<ఒబిడబ్లే>_____________')\n","('<parshiyan>___________________', '<పర్షియన్>_____________', '<పర్షియన్>_____________')\n","('<khandoba>____________________', '<ఖండోబా>_______________', '<ఖండడబా>_______________')\n","('<roopamulaku>_________________', '<రూపములకు>_____________', '<రూపములకు>_____________')\n","('<aandhrula>___________________', '<ఆంధ్రుల>______________', '<ఆంధృరుల>______________')\n","('<ivi>_________________________', '<ఇవి>__________________', '<ఇవి>__________________')\n","ep:  12  train acc: 0.7405859375  train loss: 0.12063969381700215  val acc: 0.43359375  val loss: 0.2752130757207456\n","('<sadivinchaale>_______________', '<సదివించాలె>___________', '<సదివించాలే>___________')\n","('<jurrukosagaru>_______________', '<జుర్రుకోసాగారు>_______', '<జుర్రుకోసాగరరు________')\n","('<devarayalantati>_____________', '<దేవరాయలంతటి>__________', '<దేవరాయంంటట>>__________')\n","('<prakopaalanu>________________', '<ప్రకోపాలను>___________', '<ప్రకొపాలను>___________')\n","('<nerpinchaleka>_______________', '<నేర్పించలేక>__________', '<నెర్పించలేక>__________')\n","('<appikatlaloni>_______________', '<అప్పికట్లలోని>________', '<అప్పికట్లలోని>________')\n","('<bhavoha>_____________________', '<భావోహ>________________', '<భావోహ>________________')\n","('<sadassumugisindi>____________', '<సదస్సుముగిసింది>______', '<సదస్సుముగిసింది>______')\n","('<deshaaloni>__________________', '<దేశాలోని>_____________', '<దేశాలోని>_____________')\n","('<adagalekapotunnaro>__________', '<అడగలేకపోతున్నారో>_____', '<అదగలేకపోతున్నారో>_____')\n","('<kachaabharana>_______________', '<కచాభరణ>_______________', '<కచాభరణ>_______________')\n","('<sikhararohaname>_____________', '<శిఖరారోహణమే>__________', '<సిఖరరోహహామ>>__________')\n","('<meedigaane>__________________', '<మీదిగానే>_____________', '<మీదిగానే>_____________')\n","('<vihruthulan>_________________', '<విహృతులన్>____________', '<విహ్హులన్>____________')\n","('<viriyalsina>_________________', '<విరియాల్సిన>__________', '<విరియాల్సిన>__________')\n","('<tagilindabba>________________', '<తగిలిందబ్బా>__________', '<తగిలిండబ్బా>__________')\n","('<mokshakaraka>________________', '<మోక్షకారక>____________', '<మొక్షకరకక>____________')\n","('<erivo>_______________________', '<ఎరివో>________________', '<ఎరివో>________________')\n","('<vibhajinchinaaru>____________', '<విభజించినారు>_________', '<విభజించినారు>_________')\n","('<reactive>____________________', '<రియాక్టివ్>___________', '<రియాక్వివ్>___________')\n","ep: 13  bt: 0  loss: 0.230969470480214  acc:  0.5\n","ep: 13  bt: 20  loss: 0.0008068989314462827  acc:  1.0\n","ep: 13  bt: 40  loss: 0.0005590717267730962  acc:  1.0\n","ep: 13  bt: 60  loss: 0.2652711453645126  acc:  0.453125\n","ep: 13  bt: 80  loss: 0.0006422127556541692  acc:  1.0\n","ep: 13  bt: 100  loss: 0.1594457833663277  acc:  0.5625\n","ep: 13  bt: 120  loss: 0.0006457594344797342  acc:  1.0\n","ep: 13  bt: 140  loss: 0.0010492871312991433  acc:  1.0\n","ep: 13  bt: 160  loss: 0.2418414820795474  acc:  0.421875\n","ep: 13  bt: 180  loss: 0.0006129391410428544  acc:  1.0\n","ep: 13  bt: 200  loss: 0.161867152089658  acc:  0.53125\n","ep: 13  bt: 220  loss: 0.3096521211707074  acc:  0.34375\n","ep: 13  bt: 240  loss: 0.28345033396845276  acc:  0.390625\n","ep: 13  bt: 260  loss: 0.0007098536614490592  acc:  1.0\n","ep: 13  bt: 280  loss: 0.287237664927607  acc:  0.5\n","ep: 13  bt: 300  loss: 0.000511960004982741  acc:  1.0\n","ep: 13  bt: 320  loss: 0.2433615560116975  acc:  0.515625\n","ep: 13  bt: 340  loss: 0.2947921545609184  acc:  0.4375\n","ep: 13  bt: 360  loss: 0.35043749601944635  acc:  0.375\n","ep: 13  bt: 380  loss: 0.1422228191209876  acc:  0.578125\n","ep: 13  bt: 400  loss: 0.0014694274767585423  acc:  1.0\n","ep: 13  bt: 420  loss: 0.0005687037887780563  acc:  1.0\n","ep: 13  bt: 440  loss: 0.000706228873004084  acc:  1.0\n","ep: 13  bt: 460  loss: 0.0006085545148538506  acc:  1.0\n","ep: 13  bt: 480  loss: 0.22867105318152386  acc:  0.5625\n","ep: 13  bt: 500  loss: 0.25063941789710004  acc:  0.53125\n","ep: 13  bt: 520  loss: 0.0005870548117419947  acc:  1.0\n","ep: 13  bt: 540  loss: 0.16540414354075555  acc:  0.546875\n","ep: 13  bt: 560  loss: 0.000563913389392521  acc:  1.0\n","ep: 13  bt: 580  loss: 0.0006118002145186714  acc:  1.0\n","ep: 13  bt: 600  loss: 0.15763924432837445  acc:  0.515625\n","ep: 13  bt: 620  loss: 0.0005440332967302073  acc:  1.0\n","ep: 13  bt: 640  loss: 0.0006482345254524895  acc:  1.0\n","ep: 13  bt: 660  loss: 0.2640607046044391  acc:  0.46875\n","ep: 13  bt: 680  loss: 0.28694944796354876  acc:  0.421875\n","ep: 13  bt: 700  loss: 0.0009017910646355671  acc:  1.0\n","ep: 13  bt: 720  loss: 0.0009158865103255148  acc:  1.0\n","ep: 13  bt: 740  loss: 0.000757487490773201  acc:  1.0\n","ep: 13  bt: 760  loss: 0.0007527330485375031  acc:  1.0\n","ep: 13  bt: 780  loss: 0.0031795666917510653  acc:  0.984375\n","('<manasuloni>__________________', '<మనసులోని>_____________', '<మనసులోని>_____________')\n","('<culture>_____________________', '<కల్చర్>_______________', '<క్్టర్>>>_____________')\n","('<vinimayam>___________________', '<వినిమయం>______________', '<వినిమయం>______________')\n","('<turagaa>_____________________', '<తురగా>________________', '<తురగా>________________')\n","('<toedaelhlhu>_________________', '<తోడేళ్ళు>_____________', '<తోడేల్లు>_____________')\n","('<qant>________________________', '<కాంట్>________________', '<కం్టట>________________')\n","('<karyalayam>__________________', '<కార్యాలయం>____________', '<కర్యలలయం>_____________')\n","('<vippabotondantu>_____________', '<విప్పబోతోందంటూ>_______', '<విప్పబోతోందంటూ>_______')\n","('<margamlo>____________________', '<మార్గంలో>_____________', '<మరర్మంలో>_____________')\n","('<natisthoo>___________________', '<నటిస్తూ>______________', '<నటిస్తో>______________')\n","('<chuusaaru>___________________', '<చూసారు>_______________', '<చూసారు>_______________')\n","('<vyaktulugaa>_________________', '<వ్యక్తులుగా>__________', '<వ్యక్తులుగా>__________')\n","('<contanese>___________________', '<కాంటనీస్>_____________', '<కంంననేసస>_____________')\n","('<venukanja>___________________', '<వెనుకంజ>______________', '<వెనుకంజా>_____________')\n","('<vetikiuntaaru>_______________', '<వెతికిఉంటారు>_________', '<వేతికిఉంటారు>_________')\n","('<appaginchevaaru>_____________', '<అప్పగించేవారు>________', '<అప్పగించేవారు>________')\n","('<tharkam>_____________________', '<తర్కం>________________', '<తర్కం>________________')\n","('<samudaayam>__________________', '<సముదాయం>______________', '<సముదాయం>______________')\n","('<satiga>______________________', '<సాటిగా>_______________', '<సతిిా>________________')\n","('<taaragaa>____________________', '<తారగా>________________', '<తారగా>________________')\n","ep:  13  train acc: 0.731875  train loss: 0.1245736879582071  val acc: 0.39306640625  val loss: 0.2812506219615107\n","('<sarisrupala>_________________', '<సరిసృపాల>_____________', '<సరిసృపాల>_____________')\n","('<mitralo>_____________________', '<మిత్రలో>______________', '<మిత్రలో>______________')\n","('<khangutinipinchaaru>_________', '<ఖంగుతినిపించారు>______', '<ఖంగుతినిపించారు>______')\n","('<cheddamannadata>_____________', '<చేద్దామన్నాడట>________', '<చేద్దామన్నాడట>________')\n","('<adhyakshatavahistaaru>_______', '<అధ్యక్షతవహిస్తారు>____', '<అధ్యక్షతవహిస్తారు>____')\n","('<dasuraliga>__________________', '<దాసురాలిగా>___________', '<దాసురాలిగా>___________')\n","('<diddalsindenani>_____________', '<దిద్దాల్సిందేనని>_____', '<దిద్దాల్సిందేనని>_____')\n","('<adenocarcinoma>______________', '<అడెనొకార్సినోమా>______', '<అడెనొకార్సినోమా>______')\n","('<antanemo>____________________', '<అంటానేమో>_____________', '<అంటానేమో>_____________')\n","('<varninchinave>_______________', '<వర్ణించినవే>__________', '<వర్ణించినవే>__________')\n","('<firaayinchadaanni>___________', '<ఫిరాయించడాన్ని>_______', '<ఫిరాయించడాన్ని>_______')\n","('<mavillapadu>_________________', '<మావిళ్లపాడు>__________', '<మావిళ్లపాడు>__________')\n","('<kaavalanukune>_______________', '<కావలనుకునే>___________', '<కావలనుకునే>___________')\n","('<nishkraminchinapudu>_________', '<నిష్క్రమించినపుడు>____', '<నిష్క్రమించినపుడు>____')\n","('<samosalanu>__________________', '<సమోసాలను>_____________', '<సమోసాలను>_____________')\n","('<sampannulapaina>_____________', '<సంపన్నులపైనా>_________', '<సంపన్నులపైనా>_________')\n","('<beaser>______________________', '<బీజర్>________________', '<బీజర్>________________')\n","('<mabhyapettevaadu>____________', '<మభ్యపెట్టేవాడు>_______', '<మభ్యపెట్టేవాడు>_______')\n","('<unnayanalemu>________________', '<ఉన్నాయనలేము>__________', '<ఉన్నాయనలేము>__________')\n","('<nanusarinchuta>______________', '<ననుసరించుట>___________', '<ననుసరించుట>___________')\n","ep: 14  bt: 0  loss: 0.0005475406170539234  acc:  1.0\n","ep: 14  bt: 20  loss: 0.23772534080173657  acc:  0.453125\n","ep: 14  bt: 40  loss: 0.0008967557679051938  acc:  1.0\n","ep: 14  bt: 60  loss: 0.25287306827047595  acc:  0.453125\n","ep: 14  bt: 80  loss: 0.2377157625944718  acc:  0.375\n","ep: 14  bt: 100  loss: 0.0004954052603115206  acc:  1.0\n","ep: 14  bt: 120  loss: 0.3231217757515285  acc:  0.3125\n","ep: 14  bt: 140  loss: 0.0011246518596358921  acc:  1.0\n","ep: 14  bt: 160  loss: 0.0007190926243429599  acc:  1.0\n","ep: 14  bt: 180  loss: 0.0005810030776521434  acc:  1.0\n","ep: 14  bt: 200  loss: 0.18924406300420346  acc:  0.4375\n","ep: 14  bt: 220  loss: 0.000738924450200537  acc:  1.0\n","ep: 14  bt: 240  loss: 0.0011273451635371084  acc:  1.0\n","ep: 14  bt: 260  loss: 0.20377430708512015  acc:  0.40625\n","ep: 14  bt: 280  loss: 0.0007245055521311967  acc:  1.0\n","ep: 14  bt: 300  loss: 0.000786663316514181  acc:  1.0\n","ep: 14  bt: 320  loss: 0.2816044351328974  acc:  0.46875\n","ep: 14  bt: 340  loss: 0.0009346984974716021  acc:  1.0\n","ep: 14  bt: 360  loss: 0.18057362929634427  acc:  0.5\n","ep: 14  bt: 380  loss: 0.2424249027086341  acc:  0.453125\n","ep: 14  bt: 400  loss: 0.0008323755439208901  acc:  1.0\n","ep: 14  bt: 420  loss: 0.18523027585900348  acc:  0.5\n","ep: 14  bt: 440  loss: 0.0006589684635400772  acc:  1.0\n","ep: 14  bt: 460  loss: 0.00048332211925931597  acc:  1.0\n","ep: 14  bt: 480  loss: 0.0009104055554970452  acc:  1.0\n","ep: 14  bt: 500  loss: 0.0035498799837153892  acc:  0.984375\n","ep: 14  bt: 520  loss: 0.23357244159864343  acc:  0.5\n","ep: 14  bt: 540  loss: 0.001314712850295979  acc:  0.984375\n","ep: 14  bt: 560  loss: 0.0007283047813436259  acc:  1.0\n","ep: 14  bt: 580  loss: 0.0008203231770059336  acc:  1.0\n","ep: 14  bt: 600  loss: 0.0006201266027663065  acc:  1.0\n","ep: 14  bt: 620  loss: 0.24895997669385828  acc:  0.5\n","ep: 14  bt: 640  loss: 0.0007977984521699989  acc:  1.0\n","ep: 14  bt: 660  loss: 0.23838163458782693  acc:  0.46875\n","ep: 14  bt: 680  loss: 0.0005753238725921382  acc:  1.0\n","ep: 14  bt: 700  loss: 0.24065442707227624  acc:  0.421875\n","ep: 14  bt: 720  loss: 0.0004644987942731899  acc:  1.0\n","ep: 14  bt: 740  loss: 0.0008498083638108295  acc:  1.0\n","ep: 14  bt: 760  loss: 0.3150634765625  acc:  0.359375\n","ep: 14  bt: 780  loss: 0.00044343717720197594  acc:  1.0\n","('<awaards>_____________________', '<అవార్డ్స్>____________', '<అవార్డ్్్>____________')\n","('<kadataam>____________________', '<కడతాం>________________', '<కదతాం>________________')\n","('<goovulaku>___________________', '<గోవులకు>______________', '<గూవులకు>______________')\n","('<soomaritanam>________________', '<సోమరితనం>_____________', '<సూమరితనం>_____________')\n","('<chaeyaka>____________________', '<చేయక>_________________', '<చేయక>_________________')\n","('<vikrayinchaaranedi>__________', '<విక్రయించారనేది>______', '<విక్రయించారనేది>______')\n","('<laestaayi>___________________', '<లేస్తాయి>_____________', '<లేస్తాయి>_____________')\n","('<nilustunnavaaru>_____________', '<నిలుస్తున్నవారు>______', '<నిలుస్తున్నవారు>______')\n","('<bhaashallooki>_______________', '<భాషల్లోకి>____________', '<భాషల్లోకి>____________')\n","('<yentho>______________________', '<యెంతో>________________', '<యంంతో>________________')\n","('<aaharamlo>___________________', '<ఆహారంలో>______________', '<ఆహరంలో>_______________')\n","('<vastaayane>__________________', '<వస్తాయనే>_____________', '<వస్తాయనే>_____________')\n","('<pipu>________________________', '<పైపు>_________________', '<పిపు>_________________')\n","('<samardhinchaaka>_____________', '<సమర్ధించాక>___________', '<సమర్ధించాక>___________')\n","('<thuraayi>____________________', '<తురాయి>_______________', '<తురాయి>_______________')\n","('<sareeram>____________________', '<శరీరం>________________', '<సరీరం>________________')\n","('<cherataaniki>________________', '<చేరటానికి>____________', '<చేరతానికి>____________')\n","('<preemikula>__________________', '<ప్రేమికుల>____________', '<ప్రీమికుల>____________')\n","('<chadavaledinka>______________', '<చదవలేదింకా>___________', '<చదవలేదినంక____________')\n","('<tolagimpabaddaayi>___________', '<తొలగింపబడ్డాయి>_______', '<తొలగింపబడ్డాయి>_______')\n","ep:  14  train acc: 0.7343359375  train loss: 0.12400665316766152  val acc: 0.418701171875  val loss: 0.288469936536706\n","('<chuuchitivi>_________________', '<చూచితివి>_____________', '<చూచితివి>_____________')\n","('<sabhalanna>__________________', '<సభలన్నా>______________', '<సభలన్నా>______________')\n","('<bhistundannaru>______________', '<భిస్తుందన్నారు>_______', '<భిస్తుందన్నారు>_______')\n","('<bratukanta>__________________', '<బ్రతుకంత>_____________', '<బ్రతుకంత>_____________')\n","('<defaulteruga>________________', '<డిఫాల్టరుగా>__________', '<డిఫాల్టరుగా>__________')\n","('<veeravidheyudigaa>___________', '<వీరవిధేయుడిగా>________', '<వీరవిధేయుడిగా>________')\n","('<bitsku>______________________', '<బిట్స్కు>_____________', '<బిట్స్కు>_____________')\n","('<neellalaagaa>________________', '<నీళ్ళలాగా>____________', '<నీళ్ళలాగా>____________')\n","('<avutunnarannade>_____________', '<అవుతున్నారన్నదే>______', '<అవుతున్నారన్నదే>______')\n","('<klaasutho>___________________', '<క్లాసుతో>_____________', '<క్లాసుతో>_____________')\n","('<kudullaloni>_________________', '<కుదుళ్ళలోని>__________', '<కుదుళ్ళలోని>__________')\n","('<lemunu>______________________', '<లేమును>_______________', '<లేమును>_______________')\n","('<sapistaanani>________________', '<శపిస్తానని>___________', '<శపిస్తానని>___________')\n","('<lingoti>_____________________', '<లింగోటి>______________', '<లింగోటి>______________')\n","('<pujith>______________________', '<పుజుత్>_______________', '<పుజుత్>_______________')\n","('<horsley>_____________________', '<హార్సిలీ>_____________', '<హార్సిలీ>_____________')\n","('<vratakalpamulu>______________', '<వ్రతకల్పములు>_________', '<వ్రతకల్పములు>_________')\n","('<haritosha>___________________', '<హరితోష>_______________', '<హరితోష>_______________')\n","('<icreate>_____________________', '<ఐక్రియేట్>____________', '<ఐక్రియేట్>____________')\n","('<chadivinanu>_________________', '<చదివినను>_____________', '<చదివినను>_____________')\n","ep: 15  bt: 0  loss: 0.0005418646108845006  acc:  1.0\n","ep: 15  bt: 20  loss: 0.2285116444463315  acc:  0.4375\n","ep: 15  bt: 40  loss: 0.26237525110659393  acc:  0.515625\n","ep: 15  bt: 60  loss: 0.299261010211447  acc:  0.421875\n","ep: 15  bt: 80  loss: 0.21174312674480936  acc:  0.5\n","ep: 15  bt: 100  loss: 0.2020497529403023  acc:  0.53125\n","ep: 15  bt: 120  loss: 0.0010446521413067112  acc:  1.0\n","ep: 15  bt: 140  loss: 0.3102276221565578  acc:  0.375\n","ep: 15  bt: 160  loss: 0.0006609255771922028  acc:  1.0\n","ep: 15  bt: 180  loss: 0.2646213199781335  acc:  0.453125\n","ep: 15  bt: 200  loss: 0.22121278099391772  acc:  0.515625\n","ep: 15  bt: 220  loss: 0.2768017934716266  acc:  0.46875\n","ep: 15  bt: 240  loss: 0.0006653439334553221  acc:  1.0\n","ep: 15  bt: 260  loss: 0.0006463327647551247  acc:  1.0\n","ep: 15  bt: 280  loss: 0.0007540807127952576  acc:  1.0\n","ep: 15  bt: 300  loss: 0.0007210317999124527  acc:  1.0\n","ep: 15  bt: 320  loss: 0.22036100470501443  acc:  0.515625\n","ep: 15  bt: 340  loss: 0.2341416607732358  acc:  0.46875\n","ep: 15  bt: 360  loss: 0.40050328296163806  acc:  0.40625\n","ep: 15  bt: 380  loss: 0.2433974017267642  acc:  0.53125\n","ep: 15  bt: 400  loss: 0.23887789767721426  acc:  0.40625\n","ep: 15  bt: 420  loss: 0.24352260257886804  acc:  0.53125\n","ep: 15  bt: 440  loss: 0.0005563213692411133  acc:  1.0\n","ep: 15  bt: 460  loss: 0.0004482181418849074  acc:  1.0\n","ep: 15  bt: 480  loss: 0.24805809103924295  acc:  0.421875\n","ep: 15  bt: 500  loss: 0.00040656988225553346  acc:  1.0\n","ep: 15  bt: 520  loss: 0.0005959357418443846  acc:  1.0\n","ep: 15  bt: 540  loss: 0.1984181196793266  acc:  0.421875\n","ep: 15  bt: 560  loss: 0.28853592665299127  acc:  0.5\n","ep: 15  bt: 580  loss: 0.0005304409676919813  acc:  1.0\n","ep: 15  bt: 600  loss: 0.001968692502249842  acc:  0.984375\n","ep: 15  bt: 620  loss: 0.21198336974434231  acc:  0.5\n","ep: 15  bt: 640  loss: 0.29120940747468366  acc:  0.578125\n","ep: 15  bt: 660  loss: 0.23925526245780612  acc:  0.46875\n","ep: 15  bt: 680  loss: 0.1950488919797151  acc:  0.390625\n","ep: 15  bt: 700  loss: 0.0005090518490127895  acc:  1.0\n","ep: 15  bt: 720  loss: 0.2909829305565875  acc:  0.421875\n","ep: 15  bt: 740  loss: 0.0007534061113129492  acc:  1.0\n","ep: 15  bt: 760  loss: 0.2297818142434825  acc:  0.578125\n","ep: 15  bt: 780  loss: 0.29241932993349823  acc:  0.390625\n","('<event>_______________________', '<ఈవెంట్>_______________', '<ఎవెటట్>_______________')\n","('<teechar>_____________________', '<టీచర్>________________', '<తీచర్>________________')\n","('<amaayakudu>__________________', '<అమాయకుడు>_____________', '<అమాయకుడు>_____________')\n","('<shaastravettalanu>___________', '<శాస్త్రవేత్తలను>______', '<షాస్త్రవేత్తలను>______')\n","('<phiryaaduloe>________________', '<ఫిర్యాదులో>___________', '<ఫిర్యాదులో>___________')\n","('<kaaryaacharanam>_____________', '<కార్యాచరణం>___________', '<కార్యాచరణం>___________')\n","('<bandhinchaaru>_______________', '<బంధించారు>____________', '<బంధించారు>____________')\n","('<produktion>__________________', '<ప్రొడక్షన్>___________', '<ప్రాడ్క్షన్>__________')\n","('<niganigalada>________________', '<నిగనిగలాడా>___________', '<నిగనిగలదా>____________')\n","('<aadamudi>____________________', '<ఆదముడి>_______________', '<ఆడముది>_______________')\n","('<maadyamaalalo>_______________', '<మాద్యమాలలో>___________', '<మాద్యమాలలో>___________')\n","('<pooland>_____________________', '<పోలండ్>_______________', '<పూలండ్>_______________')\n","('<rooje>_______________________', '<రోజే>_________________', '<రూజే>_________________')\n","('<screan>______________________', '<స్క్రీన్>_____________', '<స్క్రెన్>_____________')\n","('<bodhinchi>___________________', '<బోధించి>______________', '<బోధించి>______________')\n","('<bhashyam>____________________', '<భాష్యమ్>______________', '<భాష్యం>_______________')\n","('<lesthaayi>___________________', '<లేస్తాయి>_____________', '<లేస్తాయి>_____________')\n","('<moodunelalako>_______________', '<మూడునెలలకో>___________', '<మూడునేలలకో>___________')\n","('<mumbaiki>____________________', '<ముంబైకి>______________', '<ముంబాయికి>____________')\n","('<deenjar>_____________________', '<డేంజర్>_______________', '<దీంజర్>_______________')\n","ep:  15  train acc: 0.74220703125  train loss: 0.11678375562931322  val acc: 0.427734375  val loss: 0.2946304860322372\n","('<nokkesukundi>________________', '<నొక్కేసుకుంది>________', '<నొక్కేసుకుంది>________')\n","('<tirumaligalo>________________', '<తిరుమాళిగలో>__________', '<తిరుమలిగలో>___________')\n","('<ammukunnara>_________________', '<అమ్ముకున్నారా>________', '<అమ్ముకున్నారా>________')\n","('<kashtaalupadadam>____________', '<కష్టాలుపడడం>__________', '<కష్టాలుపడం>___________')\n","('<meedakellaaru>_______________', '<మీదకెళ్లారు>__________', '<మీడకెళ్లారు>__________')\n","('<aasannamaindaa>______________', '<ఆసన్నమైందా>___________', '<ఆసన్నమైందా>___________')\n","('<floorings>___________________', '<ఫ్లోరింగ్స్>__________', '<ఫ్లూరింగ్స్>__________')\n","('<mopinatlu>___________________', '<మోపినట్లు>____________', '<మోపినట్లు>____________')\n","('<ajospermiyaga>_______________', '<అజోస్పెర్మియాగా>______', '<అజోస్పెర్ియాగా>_______')\n","('<undakudadanukunnam>__________', '<ఉండకూడదనుకున్నాం>_____', '<ఉండకూడదనుకున్నాం>_____')\n","('<kuudagattaali>_______________', '<కూడగట్టాలి>___________', '<కూూగట్టాలి>___________')\n","('<narukkuntunnamanna>__________', '<నరుక్కుంటున్నామన్న>___', '<నరుక్కుంటున్నామన్న>___')\n","('<atyantapramadakaramaina>_____', '<అత్యంతప్రమాదకరమైన>____', '<అత్యంతప్రమదకరరమైన>____')\n","('<mukhyatri>___________________', '<ముఖ్యత్రి>____________', '<ముఖ్యత్రి>____________')\n","('<pradhamaavibhaktiloo>________', '<ప్రథమావిభక్తిలో>______', '<ప్రధమావిభక్తిలో>______')\n","('<ummadidi>____________________', '<ఉమ్మడిది>_____________', '<ఉమ్మడిది>_____________')\n","('<saranyamannatlu>_____________', '<శరణ్యమన్నట్లు>________', '<సరణ్యమన్నట్లు>________')\n","('<aayanano>____________________', '<ఆయననో>________________', '<ఆయననో>________________')\n","('<valulu>______________________', '<వాలులు>_______________', '<వలుుు>________________')\n","('<chigurinchavacchu>___________', '<చిగురించవచ్చు>________', '<చిగురించవచ్చు>________')\n","ep: 16  bt: 0  loss: 0.24796514925749405  acc:  0.53125\n","ep: 16  bt: 20  loss: 0.0006920109786417173  acc:  1.0\n","ep: 16  bt: 40  loss: 0.0006386441705019578  acc:  1.0\n","ep: 16  bt: 60  loss: 0.00042423006633053655  acc:  1.0\n","ep: 16  bt: 80  loss: 0.2730986968330715  acc:  0.4375\n","ep: 16  bt: 100  loss: 0.000474589552892291  acc:  1.0\n","ep: 16  bt: 120  loss: 0.2496553503948709  acc:  0.515625\n","ep: 16  bt: 140  loss: 0.16334617656210196  acc:  0.53125\n","ep: 16  bt: 160  loss: 0.24398830662602963  acc:  0.4375\n","ep: 16  bt: 180  loss: 0.00039492749973483706  acc:  1.0\n","ep: 16  bt: 200  loss: 0.22243972446607507  acc:  0.53125\n","ep: 16  bt: 220  loss: 0.0003215062189037385  acc:  1.0\n","ep: 16  bt: 240  loss: 0.24995946884155273  acc:  0.390625\n","ep: 16  bt: 260  loss: 0.000539892069671465  acc:  1.0\n","ep: 16  bt: 280  loss: 0.00042026518317668336  acc:  1.0\n","ep: 16  bt: 300  loss: 0.18040536797564963  acc:  0.484375\n","ep: 16  bt: 320  loss: 0.00163973203819731  acc:  0.984375\n","ep: 16  bt: 340  loss: 0.0016222139415533645  acc:  0.984375\n","ep: 16  bt: 360  loss: 0.28121859094370966  acc:  0.484375\n","ep: 16  bt: 380  loss: 0.20289767306783926  acc:  0.53125\n","ep: 16  bt: 400  loss: 0.0005774557914422905  acc:  1.0\n","ep: 16  bt: 420  loss: 0.20113196580306344  acc:  0.5\n","ep: 16  bt: 440  loss: 0.00044355780372153157  acc:  1.0\n","ep: 16  bt: 460  loss: 0.2188375307166058  acc:  0.421875\n","ep: 16  bt: 480  loss: 0.1997878240502399  acc:  0.5625\n","ep: 16  bt: 500  loss: 0.0004926175688919814  acc:  1.0\n","ep: 16  bt: 520  loss: 0.290682025577711  acc:  0.46875\n","ep: 16  bt: 540  loss: 0.2234851588373599  acc:  0.515625\n","ep: 16  bt: 560  loss: 0.2536664631055749  acc:  0.4375\n","ep: 16  bt: 580  loss: 0.00047382473459710246  acc:  1.0\n","ep: 16  bt: 600  loss: 0.16927919180496878  acc:  0.59375\n","ep: 16  bt: 620  loss: 0.0005154331093249114  acc:  1.0\n","ep: 16  bt: 640  loss: 0.0007639918476343155  acc:  1.0\n","ep: 16  bt: 660  loss: 0.18906632713649585  acc:  0.484375\n","ep: 16  bt: 680  loss: 0.2152322478916334  acc:  0.46875\n","ep: 16  bt: 700  loss: 0.21998668753582498  acc:  0.453125\n","ep: 16  bt: 720  loss: 0.0008161959602780964  acc:  1.0\n","ep: 16  bt: 740  loss: 0.2707163147304369  acc:  0.4375\n","ep: 16  bt: 760  loss: 0.12190536830736243  acc:  0.640625\n","ep: 16  bt: 780  loss: 0.0005593662676603898  acc:  1.0\n","('<krushnadevaraayalu>__________', '<కృష్ణదేవరాయలు>________', '<కృష్ణదేవరాయలు>________')\n","('<grahinchabadavu>_____________', '<గ్రహించబడవు>__________', '<గ్రహించబడవు>__________')\n","('<haibrid>_____________________', '<హైబ్రిడ్>_____________', '<హైబ్రిడ్>_____________')\n","('<sankshemaniki>_______________', '<సంక్షేమానికి>_________', '<సంక్షషేమనికి>_________')\n","('<samakurchenduku>_____________', '<సమకూర్చేందుకు>________', '<సమకుర్చచంంుకు>________')\n","('<lebal>_______________________', '<లేబుల్>_______________', '<లెబల్>>_______________')\n","('<tern>________________________', '<టర్న్>________________', '<టె్న్్>_______________')\n","('<saraku>______________________', '<సరకు>_________________', '<సరకు>_________________')\n","('<laksha>______________________', '<లక్ష>_________________', '<లక్ష>_________________')\n","('<md>__________________________', '<ఎండి>_________________', '<మిడీడీ>_______________')\n","('<puurtavutadi>________________', '<పూర్తవుతది>___________', '<పూర్తవుతది>___________')\n","('<bhooswaamula>________________', '<భూస్వాములు>___________', '<భూస్వాముల>____________')\n","('<saraagaalu>__________________', '<సరాగాలు>______________', '<సరాగాలు>______________')\n","('<seejanlalo>__________________', '<సీజన్లలో>_____________', '<సీజన్లలో>_____________')\n","('<kopantopatu>_________________', '<కోపంతోపాటు>___________', '<కొపంటోపాటు>___________')\n","('<chestunnadani>_______________', '<చేస్తున్నాడని>________', '<చేస్తున్నాడిి>________')\n","('<naaginiidu>__________________', '<నాగినీడు>_____________', '<నాగినీదు>_____________')\n","('<australiato>_________________', '<ఆస్ట్రేలియాతో>________', '<ఆస్త్రలలిితో>_________')\n","('<percian>_____________________', '<పర్షియన్>_____________', '<పెర్సియన్>____________')\n","('<prabhutvamvachaaka>__________', '<ప్రభుత్వంవచ్చాక>______', '<ప్రభుత్వమవచ్చాక>______')\n","ep:  16  train acc: 0.73765625  train loss: 0.117618961472206  val acc: 0.423583984375  val loss: 0.27845517448757007\n","('<jimmula>_____________________', '<జిమ్ముల>______________', '<జిమ్ముల>______________')\n","('<dharmakartalla>______________', '<ధర్మకర్తల్లా>_________', '<ధర్మకర్తల్లా>_________')\n","('<udvaasanalapai>______________', '<ఉద్వాసనలపై>___________', '<ఉద్వాసనలపై>___________')\n","('<tetotylerga>_________________', '<టీటోటైలర్గా>__________', '<టీటోటైలర్గా>__________')\n","('<vignaanamlo>_________________', '<విజ్ఞానంలో>___________', '<విజ్ఞానంలో>___________')\n","('<resigns>_____________________', '<రిజైన్స్>_____________', '<రిజైన్స్>_____________')\n","('<cleveland>___________________', '<క్లీవ్లెండ్>__________', '<క్లీవ్లెండ్>__________')\n","('<sargaon>_____________________', '<సర్గాన్>______________', '<సర్గాన్>______________')\n","('<vaahini>_____________________', '<వాహిని>_______________', '<వాహిని>_______________')\n","('<firaayincheshaadu>___________', '<ఫిరాయించేశాడు>________', '<ఫిరాయించేశాడు>________')\n","('<zometa>______________________', '<జోమెటా>_______________', '<జోమెటా>_______________')\n","('<ghnaapakamochindi>___________', '<జ్ఞాపకమొచ్చింది>______', '<జ్ఞాపకమొచ్చింది>______')\n","('<tappulekapovachhu>___________', '<తప్పులేకపోవచ్చు>______', '<తప్పులేకపోవచ్చు>______')\n","('<paaripoyaayani>______________', '<పారిపోయాయని>__________', '<పారిపోయాయని>__________')\n","('<techhaadani>_________________', '<తెచ్చాడని>____________', '<తెచ్చాడని>____________')\n","('<diddaka>_____________________', '<దిద్దక>_______________', '<దిద్దక>_______________')\n","('<matadveshamto>_______________', '<మతద్వేషంతో>___________', '<మతద్వేషంతో>___________')\n","('<telupukonenduku>_____________', '<తెలుపుకొనేందుకు>______', '<తెలుపుకొనేందుకు>______')\n","('<sabhyulundela>_______________', '<సభ్యులుండేలా>_________', '<సభ్యులుండేలా>_________')\n","('<vilasillutunnaa>_____________', '<విలసిల్లుతున్నా>______', '<విలసిల్లుతున్నా>______')\n","ep: 17  bt: 0  loss: 0.0005555151678297831  acc:  1.0\n","ep: 17  bt: 20  loss: 0.00046819962723099667  acc:  1.0\n","ep: 17  bt: 40  loss: 0.0003160641366696876  acc:  1.0\n","ep: 17  bt: 60  loss: 0.23709189373513925  acc:  0.484375\n","ep: 17  bt: 80  loss: 0.0009729638695716858  acc:  0.984375\n","ep: 17  bt: 100  loss: 0.2794232575789742  acc:  0.390625\n","ep: 17  bt: 120  loss: 0.2160541700280231  acc:  0.484375\n","ep: 17  bt: 140  loss: 0.0005628305041919584  acc:  1.0\n","ep: 17  bt: 160  loss: 0.32908493539561395  acc:  0.484375\n","ep: 17  bt: 180  loss: 0.18073523562887442  acc:  0.5625\n","ep: 17  bt: 200  loss: 0.0004968625776793646  acc:  1.0\n","ep: 17  bt: 220  loss: 0.2662299612294073  acc:  0.515625\n","ep: 17  bt: 240  loss: 0.2471793216207753  acc:  0.296875\n","ep: 17  bt: 260  loss: 0.0003555170462831207  acc:  1.0\n","ep: 17  bt: 280  loss: 0.0010610888509646707  acc:  1.0\n","ep: 17  bt: 300  loss: 0.0018463589898917985  acc:  0.984375\n","ep: 17  bt: 320  loss: 0.0006227071356514226  acc:  1.0\n","ep: 17  bt: 340  loss: 0.0010360303618337798  acc:  1.0\n","ep: 17  bt: 360  loss: 0.0005302985968149227  acc:  1.0\n","ep: 17  bt: 380  loss: 0.0005764706546197767  acc:  1.0\n","ep: 17  bt: 400  loss: 0.00038764277554076653  acc:  1.0\n","ep: 17  bt: 420  loss: 0.000430945226031801  acc:  1.0\n","ep: 17  bt: 440  loss: 0.22202257488084876  acc:  0.515625\n","ep: 17  bt: 460  loss: 0.00035623347629671514  acc:  1.0\n","ep: 17  bt: 480  loss: 0.34001358695652173  acc:  0.4375\n","ep: 17  bt: 500  loss: 0.0005265222457440003  acc:  1.0\n","ep: 17  bt: 520  loss: 0.2521640943444293  acc:  0.5\n","ep: 17  bt: 540  loss: 0.0005435137402104294  acc:  1.0\n","ep: 17  bt: 560  loss: 0.0007528622189293737  acc:  1.0\n","ep: 17  bt: 580  loss: 0.0010732263326644897  acc:  1.0\n","ep: 17  bt: 600  loss: 0.2737753702246625  acc:  0.390625\n","ep: 17  bt: 620  loss: 0.000521791572480098  acc:  1.0\n","ep: 17  bt: 640  loss: 0.0009188050323206445  acc:  1.0\n","ep: 17  bt: 660  loss: 0.0006271713696744131  acc:  1.0\n","ep: 17  bt: 680  loss: 0.21570831796397333  acc:  0.5\n","ep: 17  bt: 700  loss: 0.00039663959456526715  acc:  1.0\n","ep: 17  bt: 720  loss: 0.0004001755024427953  acc:  1.0\n","ep: 17  bt: 740  loss: 0.2666209884311842  acc:  0.546875\n","ep: 17  bt: 760  loss: 0.0019181027360584426  acc:  0.984375\n","ep: 17  bt: 780  loss: 0.3044901308806046  acc:  0.4375\n","('<mitrudavaina>________________', '<మిత్రుడవైన>___________', '<మిత్రుడవైన>___________')\n","('<edlanu>______________________', '<ఎడ్లను>_______________', '<ఎడ్లను>_______________')\n","('<gayatri>_____________________', '<గాయత్రి>______________', '<గాయతత్రి>_____________')\n","('<saakshyangaa>________________', '<సాక్ష్యంగా>___________', '<సాక్ష్యంగా>___________')\n","('<poeduuru>____________________', '<పోడూరు>_______________', '<పోదూరు>_______________')\n","('<thar>________________________', '<థార్>_________________', '<తర్్>_________________')\n","('<mrhugashira>_________________', '<మృగశిర>_______________', '<మృృశశశశరర>>___________')\n","('<narasaraavupeta>_____________', '<నరసరావుపేట>___________', '<నరసరావుపేట>___________')\n","('<aracheethiloo>_______________', '<అరచేతిలో>_____________', '<అరచీతిలో>_____________')\n","('<yaanaadulaku>________________', '<యానాదులకు>____________', '<యానాదులకు>____________')\n","('<aakraminchina>_______________', '<ఆక్రమించిన>___________', '<ఆక్రమించిన>___________')\n","('<kaagaa>______________________', '<కాగా>_________________', '<కాగా>_________________')\n","('<khasim>______________________', '<ఖాసిం>________________', '<ఖసిి>>>_______________')\n","('<kshetramulu>_________________', '<క్షేత్రములు>__________', '<క్షేత్రములు>__________')\n","('<shareeram>___________________', '<శరీరం>________________', '<శరీరం>________________')\n","('<skaen>_______________________', '<స్కాన్>_______________', '<స్కేన్>_______________')\n","('<bertan>______________________', '<బర్టన్>_______________', '<బెర్టాన్>_____________')\n","('<lunchukoni>__________________', '<లుంచుకొని>____________', '<లుంచుకొని>____________')\n","('<cheeyadamtoo>________________', '<చేయడంతో>______________', '<చీయడంతో>______________')\n","('<reddii>______________________', '<రెడ్డీ>_______________', '<రెడ్డీ>_______________')\n","ep:  17  train acc: 0.756953125  train loss: 0.11147148321691462  val acc: 0.41064453125  val loss: 0.30049877581389056\n","('<kakkulur>____________________', '<కక్కులూర్>____________', '<కక్కులూర్>____________')\n","('<naukaadalamunaku>____________', '<నౌకాదళమునకు>__________', '<నౌకాడలమునకు>__________')\n","('<velampai>____________________', '<వేలంపై>_______________', '<వెలంపై>_______________')\n","('<kalisipotunnarantu>__________', '<కలిసిపోతున్నారంటూ>____', '<కలిసిపోతున్నారంటూ>____')\n","('<batikipoyadu>________________', '<బతికిపోయాడు>__________', '<బతికిపోయాడు>__________')\n","('<muchhatistuntaaru>___________', '<ముచ్చటిస్తుంటారు>_____', '<ముచ్చటిస్తుంటారు>_____')\n","('<moodavadasalo>_______________', '<మూడవదశలో>_____________', '<మూడవడసలో>_____________')\n","('<pojulaku>____________________', '<పోజులకు>______________', '<పోజులకు>______________')\n","('<anesindi>____________________', '<అనేసింది>_____________', '<అనేసింది>_____________')\n","('<aakalayinanta>_______________', '<ఆకలయినంత>_____________', '<ఆకలయినంత>_____________')\n","('<vamsikrishnagoud>____________', '<వంశీకృష్ణగౌడ్>________', '<వంసికృష్ణగౌడ్>________')\n","('<bongu>_______________________', '<బొంగు>________________', '<బొంగు>________________')\n","('<harasmentutho>_______________', '<హరాస్మెంటుతో>_________', '<హరస్మమంంతతతో>_________')\n","('<kalvarlu>____________________', '<కల్వర్లు>_____________', '<కల్వర్లు>_____________')\n","('<palnadupai>__________________', '<పల్నాడుపై>____________', '<పా్నాుపపై>____________')\n","('<kalvaral>____________________', '<కల్వరాల్>_____________', '<కల్వరల్>______________')\n","('<tamakunnavi>_________________', '<తమకున్నవి>____________', '<తమకున్నవి>>___________')\n","('<manasoppaledani>_____________', '<మనసొప్పలేదని>_________', '<మనసోప్పలేదని>_________')\n","('<jimka>_______________________', '<జింక>_________________', '<జింక>>________________')\n","('<nilabadipovalante>___________', '<నిలబడిపోవాలంటే>_______', '<నిలబడిపోవాలంటే>_______')\n","ep: 18  bt: 0  loss: 0.25236571353414783  acc:  0.421875\n","ep: 18  bt: 20  loss: 0.3528253306513247  acc:  0.25\n","ep: 18  bt: 40  loss: 0.0004710432384972987  acc:  1.0\n","ep: 18  bt: 60  loss: 0.2874265131743058  acc:  0.46875\n","ep: 18  bt: 80  loss: 0.0006156478725049807  acc:  1.0\n","ep: 18  bt: 100  loss: 0.1950422369915506  acc:  0.546875\n","ep: 18  bt: 120  loss: 0.3410272805587105  acc:  0.46875\n","ep: 18  bt: 140  loss: 0.19038911487745203  acc:  0.53125\n","ep: 18  bt: 160  loss: 0.2284568289051885  acc:  0.484375\n","ep: 18  bt: 180  loss: 0.2418142608974291  acc:  0.5625\n","ep: 18  bt: 200  loss: 0.0005779517004671304  acc:  1.0\n","ep: 18  bt: 220  loss: 0.000310494281027628  acc:  1.0\n","ep: 18  bt: 240  loss: 0.00030967012128752213  acc:  1.0\n","ep: 18  bt: 260  loss: 0.22935670355091925  acc:  0.5\n","ep: 18  bt: 280  loss: 0.3300644418467646  acc:  0.453125\n","ep: 18  bt: 300  loss: 0.0028978132683297863  acc:  0.984375\n","ep: 18  bt: 320  loss: 0.001038140252880428  acc:  1.0\n","ep: 18  bt: 340  loss: 0.0009391460081805353  acc:  1.0\n","ep: 18  bt: 360  loss: 0.0008955839211526124  acc:  1.0\n","ep: 18  bt: 380  loss: 0.28591935530952783  acc:  0.453125\n","ep: 18  bt: 400  loss: 0.21303303345389987  acc:  0.515625\n","ep: 18  bt: 420  loss: 0.0009414389243592386  acc:  1.0\n","ep: 18  bt: 440  loss: 0.000988156575223674  acc:  1.0\n","ep: 18  bt: 460  loss: 0.0012405206165883851  acc:  1.0\n","ep: 18  bt: 480  loss: 0.212625441343888  acc:  0.46875\n","ep: 18  bt: 500  loss: 0.1680111366769542  acc:  0.53125\n","ep: 18  bt: 520  loss: 0.0009142000066197437  acc:  1.0\n","ep: 18  bt: 540  loss: 0.0011847957806742709  acc:  1.0\n","ep: 18  bt: 560  loss: 0.0006913459333388702  acc:  1.0\n","ep: 18  bt: 580  loss: 0.0008169964927694072  acc:  1.0\n","ep: 18  bt: 600  loss: 0.0006813923625842384  acc:  1.0\n","ep: 18  bt: 620  loss: 0.3221914664558742  acc:  0.515625\n","ep: 18  bt: 640  loss: 0.0011811385984006135  acc:  1.0\n","ep: 18  bt: 660  loss: 0.0007610723538243252  acc:  1.0\n","ep: 18  bt: 680  loss: 0.28846560353818146  acc:  0.421875\n","ep: 18  bt: 700  loss: 0.28315245586892834  acc:  0.484375\n","ep: 18  bt: 720  loss: 0.2803063807280167  acc:  0.421875\n","ep: 18  bt: 740  loss: 0.0004533566129596337  acc:  1.0\n","ep: 18  bt: 760  loss: 0.0005147856376741244  acc:  1.0\n","ep: 18  bt: 780  loss: 0.23977853940880817  acc:  0.453125\n","('<vayasukoenae>________________', '<వయసులోనే>_____________', '<వయసుకోనే>_____________')\n","('<dalichina>___________________', '<దలిచిన>_______________', '<దలిచిన>_______________')\n","('<thiraalu>____________________', '<తీరాలు>_______________', '<తిరాలు>_______________')\n","('<andajesukuntu>_______________', '<అందజేసుకుంటూ>_________', '<అందజేసుకుంటూ>_________')\n","('<muppemundi>__________________', '<ముప్పేముంది>__________', '<ముప్పేముంది>__________')\n","('<kaligivunnadhi>______________', '<కలిగివున్నది>_________', '<కలిగివున్నధధ>_________')\n","('<dharmal>_____________________', '<థర్మల్>_______________', '<ధర్మల్>_______________')\n","('<teechar>_____________________', '<టీచర్>________________', '<తీచర్>________________')\n","('<devaalayamulo>_______________', '<దేవాలయములో>___________', '<దేవాలయములో>___________')\n","('<labdhidaarudaina>____________', '<లబ్ధిదారుడైన>_________', '<లబ్ధిదారుడైన>_________')\n","('<aakseekaranigaa>_____________', '<ఆక్సీకరణిగా>__________', '<ఆక్సీకరణిగగా>_________')\n","('<prasamgaalakuu>______________', '<ప్రసంగాలకూ>___________', '<ప్రసంగాలకూ>___________')\n","('<bahishkarinchindhi>__________', '<బహిష్కరించింది>_______', '<బహిష్కరించింది>_______')\n","('<glucoze>_____________________', '<గ్లూకోజ్>_____________', '<గ్లూకోజ్>_____________')\n","('<graamar>_____________________', '<గ్రామర్>______________', '<గ్రామర్>______________')\n","('<sanghalu>____________________', '<సంఘాలు>_______________', '<సంఘలు>________________')\n","('<cant>________________________', '<కాంట్>________________', '<కెంం్్________________')\n","('<kaatuurivaari>_______________', '<కాటూరివారి>___________', '<కాటూరివారి>___________')\n","('<area>________________________', '<ఏరియా>________________', '<అరే>__________________')\n","('<samudaayam>__________________', '<సముదాయం>______________', '<సముదాయం>______________')\n","ep:  18  train acc: 0.7398046875  train loss: 0.11935869632607693  val acc: 0.433837890625  val loss: 0.2859468460083008\n","('<kalisochindanede>____________', '<కలిసొచ్చిందనేదే>______', '<కలిసొచ్చిందనేదే>______')\n","('<karusaipotavuro>_____________', '<కరుసైపోతావురో>________', '<కరుసైపోతావురో>________')\n","('<titanobova>__________________', '<టిటానోబొవా>___________', '<టిటానోబొవా>___________')\n","('<taaramlone>__________________', '<తారంలోనే>_____________', '<తారంలోనే>_____________')\n","('<aapagalugutunnaru>___________', '<ఆపగలుగుతున్నారు>______', '<ఆపగలుగుతున్నారు>______')\n","('<nogitlo>_____________________', '<నోగిట్లో>_____________', '<నోగిట్లో>_____________')\n","('<ettenu>______________________', '<ఎత్తెను>______________', '<ఎత్తెను>______________')\n","('<tanavepe>____________________', '<తనవేపే>_______________', '<తనవేపే>_______________')\n","('<dimmaloki>___________________', '<దిమ్మలోకి>____________', '<దిమ్మలోకి>____________')\n","('<dhanalakshmipuraniki>________', '<ధనలక్ష్మీపురానికి>____', '<ధనలక్ష్మీపురానికి>____')\n","('<anirvachaneeyame>____________', '<అనిర్వచనీయమే>_________', '<అనిర్వచనీయమే>_________')\n","('<pareekshinchabadadu>_________', '<పరీక్షించబడదు>________', '<పరీక్షించబడదు>________')\n","('<brahmabhyaja>________________', '<బ్రహ్మాభ్యజ>__________', '<బ్రహ్మాభ్యజ>__________')\n","('<falgun>______________________', '<ఫాల్గుణ>______________', '<ఫాల్గుణ>______________')\n","('<saanukuulatatoebaatu>________', '<సానుకూలతతోబాటు>_______', '<సానుకూలతతోబాటు>_______')\n","('<nandalapaduku>_______________', '<నందలపాడుకు>___________', '<నందలపాడుకు>___________')\n","('<maarakamloni>________________', '<మారకంలోని>____________', '<మారకంలోని>____________')\n","('<cheppumanenu>________________', '<చెప్పుమనెను>__________', '<చెప్పుమనెను>__________')\n","('<samoohapu>___________________', '<సమూహపు>_______________', '<సమూహపు>_______________')\n","('<mayabandham>_________________', '<మాయాబంధం>_____________', '<మాయాబంధం>_____________')\n","ep: 19  bt: 0  loss: 0.0005809035881058029  acc:  1.0\n","ep: 19  bt: 20  loss: 0.0008869857081900473  acc:  1.0\n","ep: 19  bt: 40  loss: 0.3063088499981424  acc:  0.484375\n","ep: 19  bt: 60  loss: 0.0005251262336969376  acc:  1.0\n","ep: 19  bt: 80  loss: 0.0005637609359362851  acc:  1.0\n","ep: 19  bt: 100  loss: 0.27818526392397674  acc:  0.40625\n","ep: 19  bt: 120  loss: 0.2529563903808594  acc:  0.40625\n","ep: 19  bt: 140  loss: 0.25291206525719684  acc:  0.484375\n","ep: 19  bt: 160  loss: 0.2097536791925845  acc:  0.515625\n","ep: 19  bt: 180  loss: 0.0003534481739220412  acc:  1.0\n","ep: 19  bt: 200  loss: 0.0006018938172770584  acc:  1.0\n","ep: 19  bt: 220  loss: 0.0004640418388273405  acc:  1.0\n","ep: 19  bt: 240  loss: 0.0010920899069827535  acc:  1.0\n","ep: 19  bt: 260  loss: 0.23791315244591754  acc:  0.359375\n","ep: 19  bt: 280  loss: 0.0013013008334066558  acc:  0.984375\n","ep: 19  bt: 300  loss: 0.1667819541433583  acc:  0.5625\n","ep: 19  bt: 320  loss: 0.262129431185515  acc:  0.453125\n","ep: 19  bt: 340  loss: 0.001016689626419026  acc:  1.0\n","ep: 19  bt: 360  loss: 0.0008425371640402337  acc:  1.0\n","ep: 19  bt: 380  loss: 0.24711343516474185  acc:  0.515625\n","ep: 19  bt: 400  loss: 0.00042070209494103557  acc:  1.0\n","ep: 19  bt: 420  loss: 0.29990782945052435  acc:  0.328125\n","ep: 19  bt: 440  loss: 0.001161455217262973  acc:  1.0\n","ep: 19  bt: 460  loss: 0.495916573897652  acc:  0.34375\n","ep: 19  bt: 480  loss: 0.2765342670938243  acc:  0.484375\n","ep: 19  bt: 500  loss: 0.0008248489188111347  acc:  1.0\n","ep: 19  bt: 520  loss: 0.001065933671982392  acc:  1.0\n","ep: 19  bt: 540  loss: 0.0006109400368902994  acc:  1.0\n","ep: 19  bt: 560  loss: 0.0005214928203950758  acc:  1.0\n","ep: 19  bt: 580  loss: 0.000511143842469091  acc:  1.0\n","ep: 19  bt: 600  loss: 0.0005134033120196799  acc:  1.0\n","ep: 19  bt: 620  loss: 0.2064977106840714  acc:  0.546875\n","ep: 19  bt: 640  loss: 0.000504724640885125  acc:  1.0\n","ep: 19  bt: 660  loss: 0.0006677805162642313  acc:  1.0\n","ep: 19  bt: 680  loss: 0.000693850300234297  acc:  1.0\n","ep: 19  bt: 700  loss: 0.2548891565074091  acc:  0.328125\n","ep: 19  bt: 720  loss: 0.0006390971172115077  acc:  1.0\n","ep: 19  bt: 740  loss: 0.0008008312433958054  acc:  1.0\n","ep: 19  bt: 760  loss: 0.00047881350569103074  acc:  1.0\n","ep: 19  bt: 780  loss: 0.24754130322000253  acc:  0.453125\n","('<nityaagnilooniki>____________', '<నిత్యాగ్నిలోనికి>_____', '<నిత్యాగ్నిలోనికి>_____')\n","('<kshetralu>___________________', '<క్షేత్రాలు>___________', '<క్షేత్రలు>____________')\n","('<padmanabha>__________________', '<పద్మనాభ>______________', '<పడ్మనభభ>______________')\n","('<indooneeshiyaaloo>___________', '<ఇండోనేషియాలో>_________', '<ఇందోనీషియాలో>_________')\n","('<noppulu>_____________________', '<నొప్పులు>_____________', '<నోప్పులు>_____________')\n","('<deeshaalaloo>________________', '<దేశాలలో>______________', '<దీషాలలో>______________')\n","('<rakshinchabadina>____________', '<రక్షించబడిన>__________', '<రక్షించబడిన>__________')\n","('<gurranni>____________________', '<గుర్రాన్ని>___________', '<గుర్రన్ని>____________')\n","('<bharatiyulu>_________________', '<భారతీయులు>____________', '<భరరతియులు>____________')\n","('<vinaashanaaniki>_____________', '<వినాశనానికి>__________', '<వినాషణానికి>__________')\n","('<qatar>_______________________', '<ఖతార్>________________', '<కాకర్>________________')\n","('<vidilinchaleru>______________', '<విదిలించలేరు>_________', '<విడిలించలేరు>_________')\n","('<medaloo>_____________________', '<మేడలో>________________', '<మెదలో>________________')\n","('<vaadakamloo>_________________', '<వాడకంలో>______________', '<వాడకంలో>______________')\n","('<praanatyagam>________________', '<ప్రాణత్యాగం>__________', '<ప్రాణత్యగం>___________')\n","('<chaeramani>__________________', '<చేరమని>_______________', '<చేరమని>_______________')\n","('<varudu>______________________', '<వరుడు>________________', '<వరుదు>________________')\n","('<cheralani>___________________', '<చేరాలని>______________', '<చేరలలని>______________')\n","('<soofi>_______________________', '<సూఫీ>_________________', '<సూఫి>_________________')\n","('<parikararaallo>______________', '<పరికరాల్లో>___________', '<పరికరరాల్లో>__________')\n","ep:  19  train acc: 0.73990234375  train loss: 0.11891383554869941  val acc: 0.424072265625  val loss: 0.2694677891938583\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▅▆▇▇▇▇▇▇▇▇▇█▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▆▇▇▇▇█▇██▇▇█▇██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 19\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.7399\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.11891\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.42407\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 6.19776\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcharmed-sweep-32\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/rb8hmn48\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_185521-rb8hmn48/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j9w0dy3a with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m To use W&B in kaggle you must enable internet in the settings panel on the right.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_191556-j9w0dy3a\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeft-sweep-33\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/j9w0dy3a\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"]},{"name":"stdout","output_type":"stream","text":["('<mudiram>_____________________', '<ముదిరామ్>_____________', 'ూఓఓఓఓఓఓఓఓఉఉఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<vaastavaanaki>_______________', '<వాస్తవానకి>___________', 'ూచఔఇఇఇఇఇఇఉఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<siggupadadaniki>_____________', '<సిగ్గుపడడానికి>_______', 'ూలలసతఉతళళళఉఉఔఉబఔఉఓఓఓఓఓఓ')\n","('<oteyavaa>____________________', '<ఓటేయవా>_______________', 'ూలలఓతతఉఉఉఓఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<poyedemunda>_________________', '<పోయేదేముంద>___________', 'ూఔఔఓాఓఓఓతతఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<nirmaanayatnaalaloo>_________', '<నిర్మాణయత్నాలలో>______', 'చఔఔఔఔఉఉఉఏఏళఱఉఔఔఔఔఉఉఉఓఓఓ')\n","('<durber>______________________', '<డర్బర్>_______________', 'ూఔఓఓఓఉఉఉఉఓఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<tirunavukkarsar>_____________', '<తిరునావుక్కరసర్>______', 'ూఔఔఔఔఔఔఔఔళళఉఉఉఉఉఉఉఉఓఓఓఓ')\n","('<viphalamayyevaaru>___________', '<విఫలమయ్యేవారు>________', 'ూఔఔఔఔఔఔళాాతఉఉఉఉఉఉఓఓఓఓథథ')\n","('<antukuntadi>_________________', '<అంటుకుంటది>___________', 'ూఓఓలలఓళళలఓఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<aavaasapraantaalu>___________', '<ఆవాసప్రాంతాలు>________', 'చలలలఈధఇఉఉఉఉఉోఔఔఔఓఓఓఓఓఓథ')\n","('<ahalaku>_____________________', '<అహాలకు>_______________', 'ూఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<tatatata>____________________', '<తటతట>_________________', 'ూఔలళళెఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<ranjimpachestundi>___________', '<రంజింపచేస్తుంది>______', 'ూఔలలలలలఔఉతతతతతతఢఔఓఓఓఓఓఓ')\n","('<nalikandla>__________________', '<నలికండ్ల>_____________', 'ూఓఓఔఓఓఉఉఔఔఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<ekkadunnatlu>________________', '<ఎక్కడున్నట్లు>________', 'ూఔఓఓఉఉళఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<siding>______________________', '<సిడింగ్>______________', 'ూలఓఓఔఔదఱఉఓఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<sukuomi>_____________________', '<సుకుయోమి>_____________', 'ూలఓఓఓఓాాశఔఓఓఓఓఓఓఓఓఓఓఓఓఓ')\n","('<bayaludaeraaranukundaam>_____', '<బయలుదేరారనుకుందాం>____', 'చచలలఔఓఓఉఉఉఉఏఉళళళఉఉఉఉఉఉఉ')\n","('<septemberunelala>____________', '<సెప్టెంబరునెలల>_______', 'ూలఓఓబబబతదఉతఓఓఓఓఔఓఓఓఓఓఓఓ')\n","ep: 0  bt: 0  loss: 4.21838146707286  acc:  0.0\n","ep: 0  bt: 20  loss: 1.6381787839143171  acc:  0.0\n","ep: 0  bt: 40  loss: 1.2350381768268088  acc:  0.0078125\n","ep: 0  bt: 60  loss: 1.0183481133502463  acc:  0.03125\n","ep: 0  bt: 80  loss: 0.534695459448773  acc:  0.2578125\n","ep: 0  bt: 100  loss: 0.4492913121762483  acc:  0.2109375\n","ep: 0  bt: 120  loss: 1.7815037602963655  acc:  0.0\n","ep: 0  bt: 140  loss: 0.23366129916647208  acc:  0.703125\n","ep: 0  bt: 160  loss: 0.13625643564307172  acc:  0.8515625\n","ep: 0  bt: 180  loss: 1.8083900783372961  acc:  0.0\n","ep: 0  bt: 200  loss: 1.5277053169582202  acc:  0.0\n","ep: 0  bt: 220  loss: 0.1876992764680282  acc:  0.765625\n","ep: 0  bt: 240  loss: 0.11282237716343092  acc:  0.9296875\n","ep: 0  bt: 260  loss: 0.151498908581941  acc:  0.8515625\n","ep: 0  bt: 280  loss: 0.10192705237347147  acc:  0.9375\n","ep: 0  bt: 300  loss: 0.09658254747805388  acc:  0.875\n","ep: 0  bt: 320  loss: 1.0815117048180622  acc:  0.0\n","ep: 0  bt: 340  loss: 1.677933236827021  acc:  0.0\n","ep: 0  bt: 360  loss: 0.051878866942032524  acc:  0.953125\n","ep: 0  bt: 380  loss: 0.03357333981472513  acc:  0.9765625\n","('<chaeta>______________________', '<చేట>__________________', '<చేేేత>>_______________')\n","('<sinnappatikelle>_____________', '<సిన్నప్పటికెల్లే>_____', '<సిస్పపట్లల్___________')\n","('<godalapai>___________________', '<గోడలపై>_______________', '<గోదాపప>_______________')\n","('<hrudayamloonee>______________', '<హృదయంలోనే>____________', '<హరరుయంంంంంం___________')\n","('<shyamalaa>___________________', '<శ్యామలా>______________', '<శ్్్మాాా______________')\n","('<gramalaku>___________________', '<గ్రామాలకు>____________', '<గ్రమమాకు>_____________')\n","('<thaagatam>___________________', '<తాగటం>________________', '<తాగమమమ>_______________')\n","('<sudikar>_____________________', '<సుదీకర్>______________', '<సుడికకర్______________')\n","('<biological>__________________', '<బయోలాజికల్>___________', '<బిలోగిలలల_____________')\n","('<mahabharatamulo>_____________', '<మహాభారతములో>__________', '<మహహహరరులలలల___________')\n","('<sastravettalaku>_____________', '<శాస్త్రవేత్తలకు>______', '<సస్్్్్్్్్>>_________')\n","('<nageesh>_____________________', '<నగేష్>________________', '<నగేసస్్>______________')\n","('<aranyala>____________________', '<అరణ్యాల>______________', '<అర్యాాా_______________')\n","('<doshinichesi>________________', '<దోషినిచేసి>___________', '<దోషిిిచిిిి___________')\n","('<tooku>_______________________', '<టోకు>_________________', '<తోకుకు>_______________')\n","('<nacchindhi>__________________', '<నచ్చింది>_____________', '<నన్చిచిిి_____________')\n","('<satyasayibaba>_______________', '<సత్యసాయిబాబా>_________', '<సత్యయయాాా_____________')\n","('<churchini>___________________', '<చర్చిని>______________', '<చుర్చిిిి_____________')\n","('<varalakshmini>_______________', '<వరలక్ష్మిని>__________', '<వరరల్షషిిిిి__________')\n","('<palpade>_____________________', '<పాల్పడే>______________', '<పల్పపడ>>______________')\n","ep:  0  train acc: 0.30443359375  train loss: 1.0210079832115904  val acc: 0.0009765625  val loss: 0.9600389729375425\n","('<multivarkloni>_______________', '<మల్టీవార్క్లోని>______', '<ముల్్్ర్్్్్్_________')\n","('<sanketapadamunu>_____________', '<సంకేతపదమును>__________', '<సంంటటంంంు>>___________')\n","('<seethaaraamayatheendrula>____', '<సీతారామయతీంద్రుల>_____', '<సెతారరర్్్్్__________')\n","('<terbutallin>_________________', '<టెర్బుటాలిన్>_________', '<తెర్త్ల్ల్____________')\n","('<rayalsivuntundi>_____________', '<రాయాల్సివుంటుంది>_____', '<రాయస్్్్్్్్్_________')\n","('<rajakapotasana>______________', '<రాజకపోతాసనం>__________', '<రాజజజససస>>____________')\n","('<electricidade>_______________', '<ఎలక్ట్రిసిడాడ్>_______', '<ఎల్చ్్్ిిిి___________')\n","('<ferrini>_____________________', '<ఫెర్రిని>_____________', '<ఫెర్రిిిి_____________')\n","('<deepanarsinhulu>_____________', '<దీపనర్సింహులు>________', '<దెపార్్్్్్___________')\n","('<veliveyani>__________________', '<వెలివేయని>____________', '<వెలివ్యయిిి___________')\n","('<aatmaasrayudai>______________', '<ఆత్మాశ్రయుడై>_________', '<అాామసస్్ాాాాా_________')\n","('<mmeya>_______________________', '<మ్మేయ>________________', '<మ్్ీీీీ>______________')\n","('<ravada>______________________', '<రావడ>_________________', '<రావడడడ________________')\n","('<konasana>____________________', '<కోణాసనం>______________', '<కొం్ననన_______________')\n","('<kalaamni>____________________', '<కలాంని>_______________', '<కలలమిని>______________')\n","('<cheppiyundenu>_______________', '<చెప్పియుండెను>________', '<చెప్ప్ంంంంు___________')\n","('<kalasamharamurthy>___________', '<కాలసంహారమూర్తి>_______', '<కల్సమరరరర్్్్_________')\n","('<ekkuvaste>___________________', '<ఎక్కువస్తే>___________', '<ఎక్కువ్్్్____________')\n","('<tippalivanni>________________', '<తిప్పలివన్నీ>_________', '<తిప్లినినిి___________')\n","('<boosi>_______________________', '<బూసి>_________________', '<బోసిబ్>>______________')\n","ep: 1  bt: 0  loss: 1.1828411765720532  acc:  0.0\n","ep: 1  bt: 20  loss: 0.019761168438455334  acc:  0.984375\n","ep: 1  bt: 40  loss: 0.017105491265006687  acc:  0.9609375\n","ep: 1  bt: 60  loss: 0.7297850899074388  acc:  0.0625\n","ep: 1  bt: 80  loss: 0.014953764884368233  acc:  0.9921875\n","ep: 1  bt: 100  loss: 0.014805898718211962  acc:  0.9765625\n","ep: 1  bt: 120  loss: 0.013330458298973415  acc:  0.984375\n","ep: 1  bt: 140  loss: 0.7516227390455164  acc:  0.0078125\n","ep: 1  bt: 160  loss: 0.7331680629564368  acc:  0.046875\n","ep: 1  bt: 180  loss: 0.00814309457074041  acc:  0.9921875\n","ep: 1  bt: 200  loss: 0.01065917053948278  acc:  0.984375\n","ep: 1  bt: 220  loss: 0.9886703491210938  acc:  0.0\n","ep: 1  bt: 240  loss: 0.6224685751873514  acc:  0.0859375\n","ep: 1  bt: 260  loss: 0.013774367778197578  acc:  0.9609375\n","ep: 1  bt: 280  loss: 0.7909025938614554  acc:  0.0390625\n","ep: 1  bt: 300  loss: 0.007549680445505225  acc:  0.9765625\n","ep: 1  bt: 320  loss: 0.006491118799085202  acc:  0.9765625\n","ep: 1  bt: 340  loss: 0.5327095778092094  acc:  0.1875\n","ep: 1  bt: 360  loss: 0.46183208797288977  acc:  0.1875\n","ep: 1  bt: 380  loss: 0.004081279687259508  acc:  1.0\n","('<jaateeyoodyamamloo>__________', '<జాతీయోద్యమంలో>________', '<జాటీయోడ్యమంలో>________')\n","('<gauthama>____________________', '<గౌతమ>_________________', '<గాతమ>>________________')\n","('<praanthamuloni>______________', '<ప్రాంతములోని>_________', '<ప్రాంతములోని>_________')\n","('<guchavo>_____________________', '<గుచ్చావో>_____________', '<గుచ్చవ>>______________')\n","('<presentations>_______________', '<ప్రెజెంటేషన్స్>_______', '<ప్రేసెనటటిన్స్>_______')\n","('<kalusukogalagadam>___________', '<కలుసుకోగలగడం>_________', '<కలుసుకోగాగగం>_________')\n","('<nachhani>____________________', '<నచ్చని>_______________', '<నచ్చని>_______________')\n","('<pottalu>_____________________', '<పొట్టలు>______________', '<పోట్టలు>______________')\n","('<sakhyata>____________________', '<సఖ్యత>________________', '<సఖ్యత>________________')\n","('<vaatichuttuune>______________', '<వాటిచుట్టూనే>_________', '<వాటిచుత్తునే>_________')\n","('<yedhina>_____________________', '<ఏదయినా>_______________', '<యేధినా________________')\n","('<chimpanje>___________________', '<చింపాంజీ>_____________', '<చిమపంజే>______________')\n","('<dhulipala>___________________', '<ధూళిపాళ>______________', '<ధులిపలల>______________')\n","('<santanamlo>__________________', '<సంతానంలో>_____________', '<సంతనలో>_______________')\n","('<taginantagaa>________________', '<తగినంతగా>_____________', '<తగినంతగా>_____________')\n","('<yuddham>_____________________', '<యుధ్దం>_______________', '<యుద్ధం>_______________')\n","('<borouugh>____________________', '<బారో>_________________', '<బోరూగ్>_______________')\n","('<patraayani>__________________', '<పట్రాయని>_____________', '<పట్రాయని>_____________')\n","('<aamoedimchadam>______________', '<ఆమోదించడం>____________', '<అమోదించచడం>___________')\n","('<baelooru>____________________', '<బేలూరు>_______________', '<బీలోరు>_______________')\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 28, in main\n","    train(config, data, device, train_dataloader, val_dataloader, True)\n","  File \"/tmp/ipykernel_25/2918762232.py\", line 4, in train\n","    train_loop(encoder, decoder,h_params, data, data_loader,device, val_dataloader, use_teacher_forcing)\n","  File \"/tmp/ipykernel_25/970494238.py\", line 58, in train_loop\n","    val_acc, val_loss = evaluate(encoder, decoder, data, val_dataloader,device, h_params, loss_fn, False)\n","  File \"/tmp/ipykernel_25/168279160.py\", line 16, in evaluate\n","    decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/tmp/ipykernel_25/1544882599.py\", line 83, in forward\n","    decoder_output, decoder_current_state, attn_weights = self.forward_step(decoder_current_input, decoder_current_state, encoder_final_layers)\n","  File \"/tmp/ipykernel_25/1544882599.py\", line 111, in forward_step\n","    output, prev_state = self.cell(input_gru, prev_state)\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py\", line 1102, in forward\n","    result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 22.12 MiB is free. Process 2208 has 15.87 GiB memory in use. Of the allocated memory 15.13 GiB is allocated by PyTorch, and 433.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.30443\n","\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.02101\n","\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.00098\n","\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 22.0809\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdeft-sweep-33\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/j9w0dy3a\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_191556-j9w0dy3a/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run j9w0dy3a errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 28, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train(config, data, device, train_dataloader, val_dataloader, True)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2918762232.py\", line 4, in train\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_loop(encoder, decoder,h_params, data, data_loader,device, val_dataloader, use_teacher_forcing)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/970494238.py\", line 58, in train_loop\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_acc, val_loss = evaluate(encoder, decoder, data, val_dataloader,device, h_params, loss_fn, False)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/168279160.py\", line 16, in evaluate\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     decoder_output, attentions, loss, correct = decoder(decoder_current_state, encoder_final_layer_states, target_batch, loss_fn, use_teacher_forcing)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/1544882599.py\", line 83, in forward\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     decoder_output, decoder_current_state, attn_weights = self.forward_step(decoder_current_input, decoder_current_state, encoder_final_layers)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/1544882599.py\", line 111, in forward_step\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output, prev_state = self.cell(input_gru, prev_state)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py\", line 1102, in forward\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 22.12 MiB is free. Process 2208 has 15.87 GiB memory in use. Of the allocated memory 15.13 GiB is allocated by PyTorch, and 433.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jfbl7q88 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_191733-jfbl7q88\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgood-sweep-34\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jfbl7q88\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgood-sweep-34\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jfbl7q88\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_191733-jfbl7q88/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jfbl7q88 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 82fxex93 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_191758-82fxex93\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhonest-sweep-35\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/82fxex93\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhonest-sweep-35\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/82fxex93\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_191758-82fxex93/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 82fxex93 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7ohhoyz0 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_191824-7ohhoyz0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpleasant-sweep-36\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/7ohhoyz0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpleasant-sweep-36\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/7ohhoyz0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_191824-7ohhoyz0/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 7ohhoyz0 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0ko6luz0 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_191850-0ko6luz0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvaliant-sweep-37\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/0ko6luz0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvaliant-sweep-37\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/0ko6luz0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_191850-0ko6luz0/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 0ko6luz0 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2ikqrpc0 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_191916-2ikqrpc0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtwilight-sweep-38\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/2ikqrpc0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mtwilight-sweep-38\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/2ikqrpc0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_191916-2ikqrpc0/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2ikqrpc0 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mylroo5u with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_191949-mylroo5u\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcool-sweep-39\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/mylroo5u\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcool-sweep-39\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/mylroo5u\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_191949-mylroo5u/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run mylroo5u errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6gt10zdl with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192013-6gt10zdl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglorious-sweep-40\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/6gt10zdl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mglorious-sweep-40\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/6gt10zdl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192013-6gt10zdl/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 6gt10zdl errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a4f004cj with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192040-a4f004cj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mserene-sweep-41\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/a4f004cj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mserene-sweep-41\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/a4f004cj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192040-a4f004cj/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run a4f004cj errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9rd3o4e4 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192106-9rd3o4e4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmorning-sweep-42\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/9rd3o4e4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmorning-sweep-42\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/9rd3o4e4\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192106-9rd3o4e4/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 9rd3o4e4 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9x1lo65y with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192132-9x1lo65y\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenial-sweep-43\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/9x1lo65y\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgenial-sweep-43\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/9x1lo65y\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192132-9x1lo65y/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 9x1lo65y errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fdmm3b8c with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192157-fdmm3b8c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmagic-sweep-44\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/fdmm3b8c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmagic-sweep-44\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/fdmm3b8c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192157-fdmm3b8c/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run fdmm3b8c errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: agdpd5kq with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192224-agdpd5kq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mquiet-sweep-45\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/agdpd5kq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mquiet-sweep-45\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/agdpd5kq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192224-agdpd5kq/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run agdpd5kq errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 394m9g8y with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192249-394m9g8y\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclassic-sweep-46\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/394m9g8y\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mclassic-sweep-46\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/394m9g8y\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192249-394m9g8y/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 394m9g8y errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ei8go7e6 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192315-ei8go7e6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwise-sweep-47\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ei8go7e6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwise-sweep-47\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ei8go7e6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192315-ei8go7e6/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ei8go7e6 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ir9t4nfm with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192340-ir9t4nfm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfearless-sweep-48\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ir9t4nfm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfearless-sweep-48\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ir9t4nfm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192340-ir9t4nfm/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ir9t4nfm errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rdw73iph with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192406-rdw73iph\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msilver-sweep-49\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/rdw73iph\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msilver-sweep-49\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/rdw73iph\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192406-rdw73iph/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run rdw73iph errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xc4cw997 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192432-xc4cw997\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msnowy-sweep-50\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xc4cw997\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msnowy-sweep-50\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xc4cw997\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192432-xc4cw997/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xc4cw997 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k6yana0p with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192458-k6yana0p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhopeful-sweep-51\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/k6yana0p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhopeful-sweep-51\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/k6yana0p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192458-k6yana0p/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run k6yana0p errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7vyt1wsl with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192524-7vyt1wsl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mscarlet-sweep-52\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/7vyt1wsl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mscarlet-sweep-52\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/7vyt1wsl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192524-7vyt1wsl/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 7vyt1wsl errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: owfuwlpa with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192550-owfuwlpa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msunny-sweep-53\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/owfuwlpa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msunny-sweep-53\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/owfuwlpa\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192550-owfuwlpa/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run owfuwlpa errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vbzerd09 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192615-vbzerd09\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlight-sweep-54\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/vbzerd09\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlight-sweep-54\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/vbzerd09\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192615-vbzerd09/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run vbzerd09 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 23pe7zts with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192641-23pe7zts\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msoft-sweep-55\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/23pe7zts\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msoft-sweep-55\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/23pe7zts\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192641-23pe7zts/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 23pe7zts errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 70a4x6vl with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192707-70a4x6vl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpious-sweep-56\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/70a4x6vl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpious-sweep-56\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/70a4x6vl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192707-70a4x6vl/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 70a4x6vl errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 85311fkc with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192733-85311fkc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwhole-sweep-57\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/85311fkc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwhole-sweep-57\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/85311fkc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192733-85311fkc/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 85311fkc errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8dhpb6b3 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192759-8dhpb6b3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomfy-sweep-58\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/8dhpb6b3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcomfy-sweep-58\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/8dhpb6b3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192759-8dhpb6b3/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 8dhpb6b3 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z7bxk2s9 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192824-z7bxk2s9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msoft-sweep-59\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/z7bxk2s9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msoft-sweep-59\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/z7bxk2s9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192824-z7bxk2s9/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run z7bxk2s9 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8ef97tk1 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192850-8ef97tk1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomfy-sweep-60\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/8ef97tk1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcomfy-sweep-60\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/8ef97tk1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192850-8ef97tk1/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 8ef97tk1 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z8i9mzjh with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192916-z8i9mzjh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcrimson-sweep-61\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/z8i9mzjh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcrimson-sweep-61\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/z8i9mzjh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192916-z8i9mzjh/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run z8i9mzjh errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s4wsdy53 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_192942-s4wsdy53\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mworldly-sweep-62\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/s4wsdy53\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mworldly-sweep-62\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/s4wsdy53\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_192942-s4wsdy53/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run s4wsdy53 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hraiamil with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193008-hraiamil\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbrisk-sweep-63\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hraiamil\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbrisk-sweep-63\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hraiamil\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193008-hraiamil/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run hraiamil errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qfzbz892 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193035-qfzbz892\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mswift-sweep-64\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/qfzbz892\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mswift-sweep-64\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/qfzbz892\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193035-qfzbz892/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run qfzbz892 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pgeydt11 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193100-pgeydt11\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbreezy-sweep-65\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/pgeydt11\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbreezy-sweep-65\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/pgeydt11\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193100-pgeydt11/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run pgeydt11 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: datrg5ou with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193126-datrg5ou\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlikely-sweep-66\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/datrg5ou\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlikely-sweep-66\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/datrg5ou\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193126-datrg5ou/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run datrg5ou errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9furqiy1 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193151-9furqiy1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mresilient-sweep-67\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/9furqiy1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mresilient-sweep-67\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/9furqiy1\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193151-9furqiy1/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 9furqiy1 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 35csk7zj with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193217-35csk7zj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msandy-sweep-68\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/35csk7zj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msandy-sweep-68\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/35csk7zj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193217-35csk7zj/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 35csk7zj errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ksn89pdm with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193243-ksn89pdm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeft-sweep-69\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ksn89pdm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdeft-sweep-69\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ksn89pdm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193243-ksn89pdm/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ksn89pdm errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2f78bgeu with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193309-2f78bgeu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenerous-sweep-70\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/2f78bgeu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgenerous-sweep-70\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/2f78bgeu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193309-2f78bgeu/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2f78bgeu errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sp91nzpu with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193335-sp91nzpu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mworthy-sweep-71\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/sp91nzpu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mworthy-sweep-71\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/sp91nzpu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193335-sp91nzpu/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run sp91nzpu errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lbf2zx6m with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193401-lbf2zx6m\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msweepy-sweep-72\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lbf2zx6m\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msweepy-sweep-72\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/lbf2zx6m\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193401-lbf2zx6m/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run lbf2zx6m errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wppumw0q with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193427-wppumw0q\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmajor-sweep-73\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wppumw0q\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmajor-sweep-73\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wppumw0q\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193427-wppumw0q/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wppumw0q errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: puybab5e with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193500-puybab5e\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33melated-sweep-74\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/puybab5e\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33melated-sweep-74\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/puybab5e\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193500-puybab5e/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run puybab5e errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: izt5f5uq with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193526-izt5f5uq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mconfused-sweep-75\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/izt5f5uq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mconfused-sweep-75\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/izt5f5uq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193526-izt5f5uq/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run izt5f5uq errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sx8bz4ma with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193552-sx8bz4ma\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdenim-sweep-76\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/sx8bz4ma\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdenim-sweep-76\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/sx8bz4ma\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193552-sx8bz4ma/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run sx8bz4ma errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qxwz8gde with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193618-qxwz8gde\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mabsurd-sweep-77\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/qxwz8gde\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mabsurd-sweep-77\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/qxwz8gde\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193618-qxwz8gde/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run qxwz8gde errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 63pofnpd with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193651-63pofnpd\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstellar-sweep-78\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/63pofnpd\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstellar-sweep-78\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/63pofnpd\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193651-63pofnpd/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 63pofnpd errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8pvcenzw with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193717-8pvcenzw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msmooth-sweep-79\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/8pvcenzw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msmooth-sweep-79\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/8pvcenzw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193717-8pvcenzw/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 8pvcenzw errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5stz4g97 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193743-5stz4g97\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfast-sweep-80\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/5stz4g97\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfast-sweep-80\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/5stz4g97\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193743-5stz4g97/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5stz4g97 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wlq0o2z5 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193809-wlq0o2z5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwinter-sweep-81\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wlq0o2z5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwinter-sweep-81\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/wlq0o2z5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193809-wlq0o2z5/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wlq0o2z5 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hfrrlolv with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193835-hfrrlolv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mjumping-sweep-82\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hfrrlolv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mjumping-sweep-82\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/hfrrlolv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193835-hfrrlolv/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run hfrrlolv errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ght0qgwm with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193901-ght0qgwm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblooming-sweep-83\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ght0qgwm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mblooming-sweep-83\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ght0qgwm\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193901-ght0qgwm/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ght0qgwm errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xs0kul5p with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193927-xs0kul5p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhardy-sweep-84\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xs0kul5p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhardy-sweep-84\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xs0kul5p\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193927-xs0kul5p/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xs0kul5p errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1umpq4lq with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_193953-1umpq4lq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33molive-sweep-85\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/1umpq4lq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33molive-sweep-85\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/1umpq4lq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_193953-1umpq4lq/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 1umpq4lq errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r8yxvelx with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194018-r8yxvelx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrural-sweep-86\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/r8yxvelx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrural-sweep-86\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/r8yxvelx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194018-r8yxvelx/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run r8yxvelx errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2fgr9yab with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194044-2fgr9yab\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgolden-sweep-87\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/2fgr9yab\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgolden-sweep-87\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/2fgr9yab\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194044-2fgr9yab/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2fgr9yab errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kzdi76py with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194110-kzdi76py\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mworldly-sweep-88\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/kzdi76py\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mworldly-sweep-88\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/kzdi76py\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194110-kzdi76py/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run kzdi76py errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6un3e77b with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194136-6un3e77b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroyal-sweep-89\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/6un3e77b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mroyal-sweep-89\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/6un3e77b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194136-6un3e77b/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 6un3e77b errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5ij58xva with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194202-5ij58xva\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrim-sweep-90\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/5ij58xva\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mtrim-sweep-90\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/5ij58xva\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194202-5ij58xva/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5ij58xva errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 882jxb9c with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194227-882jxb9c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mazure-sweep-91\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/882jxb9c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mazure-sweep-91\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/882jxb9c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194227-882jxb9c/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 882jxb9c errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qr73yh5c with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194253-qr73yh5c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomfy-sweep-92\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/qr73yh5c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcomfy-sweep-92\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/qr73yh5c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194253-qr73yh5c/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run qr73yh5c errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4eizghg5 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194320-4eizghg5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrosy-sweep-93\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4eizghg5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrosy-sweep-93\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/4eizghg5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194320-4eizghg5/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4eizghg5 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: afm79ir2 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194346-afm79ir2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mquiet-sweep-94\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/afm79ir2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mquiet-sweep-94\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/afm79ir2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194346-afm79ir2/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run afm79ir2 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: Post \"http://anaconda2.default.svc.cluster.local/search\": read tcp 10.52.178.3:36320->10.55.247.53:80: read: connection reset by peer (<Response [500]>)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oz777o18 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194429-oz777o18\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenial-sweep-95\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/oz777o18\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgenial-sweep-95\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/oz777o18\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194429-oz777o18/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run oz777o18 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zsrr140u with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194456-zsrr140u\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33methereal-sweep-96\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/zsrr140u\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33methereal-sweep-96\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/zsrr140u\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194456-zsrr140u/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run zsrr140u errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jw3r88oj with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194522-jw3r88oj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgrateful-sweep-97\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jw3r88oj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgrateful-sweep-97\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jw3r88oj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194522-jw3r88oj/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jw3r88oj errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 71qri3oj with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194547-71qri3oj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33meffortless-sweep-98\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/71qri3oj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33meffortless-sweep-98\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/71qri3oj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194547-71qri3oj/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 71qri3oj errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xr4c43dj with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194613-xr4c43dj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcurious-sweep-99\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xr4c43dj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcurious-sweep-99\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xr4c43dj\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194613-xr4c43dj/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xr4c43dj errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mqbkv3hh with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194638-mqbkv3hh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlilac-sweep-100\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/mqbkv3hh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlilac-sweep-100\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/mqbkv3hh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194638-mqbkv3hh/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run mqbkv3hh errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s85j34cb with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194704-s85j34cb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33micy-sweep-101\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/s85j34cb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33micy-sweep-101\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/s85j34cb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194704-s85j34cb/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run s85j34cb errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eitk2s1k with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194730-eitk2s1k\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrevived-sweep-102\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/eitk2s1k\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrevived-sweep-102\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/eitk2s1k\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194730-eitk2s1k/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run eitk2s1k errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nyz7jjdl with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194756-nyz7jjdl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msleek-sweep-103\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/nyz7jjdl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msleek-sweep-103\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/nyz7jjdl\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194756-nyz7jjdl/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run nyz7jjdl errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r0xn4fls with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194822-r0xn4fls\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwandering-sweep-104\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/r0xn4fls\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwandering-sweep-104\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/r0xn4fls\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194822-r0xn4fls/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run r0xn4fls errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gszvzkpg with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194848-gszvzkpg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclean-sweep-105\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/gszvzkpg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mclean-sweep-105\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/gszvzkpg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194848-gszvzkpg/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run gszvzkpg errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dy7h8tks with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194914-dy7h8tks\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdriven-sweep-106\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/dy7h8tks\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdriven-sweep-106\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/dy7h8tks\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194914-dy7h8tks/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run dy7h8tks errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jf05uqvq with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_194939-jf05uqvq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfanciful-sweep-107\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jf05uqvq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfanciful-sweep-107\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jf05uqvq\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_194939-jf05uqvq/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jf05uqvq errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jjerczrb with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195012-jjerczrb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33molive-sweep-108\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jjerczrb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33molive-sweep-108\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jjerczrb\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195012-jjerczrb/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jjerczrb errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f1cs0spx with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195038-f1cs0spx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcrisp-sweep-109\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/f1cs0spx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcrisp-sweep-109\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/f1cs0spx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195038-f1cs0spx/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run f1cs0spx errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ff9v8ktc with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195105-ff9v8ktc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfresh-sweep-110\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ff9v8ktc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfresh-sweep-110\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ff9v8ktc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195105-ff9v8ktc/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ff9v8ktc errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fnbo7ak6 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195131-fnbo7ak6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mapricot-sweep-111\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/fnbo7ak6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mapricot-sweep-111\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/fnbo7ak6\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195131-fnbo7ak6/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run fnbo7ak6 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rfp6x4zg with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195156-rfp6x4zg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdazzling-sweep-112\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/rfp6x4zg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdazzling-sweep-112\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/rfp6x4zg\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195156-rfp6x4zg/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run rfp6x4zg errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: syznkk81 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195233-syznkk81\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtoasty-sweep-113\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/syznkk81\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mtoasty-sweep-113\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/syznkk81\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195233-syznkk81/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run syznkk81 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oyv8ruw2 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195258-oyv8ruw2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdutiful-sweep-114\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/oyv8ruw2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdutiful-sweep-114\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/oyv8ruw2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195258-oyv8ruw2/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run oyv8ruw2 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r85792dh with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195324-r85792dh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdecent-sweep-115\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/r85792dh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdecent-sweep-115\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/r85792dh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195324-r85792dh/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run r85792dh errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qkecjqvc with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195349-qkecjqvc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvital-sweep-116\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/qkecjqvc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvital-sweep-116\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/qkecjqvc\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195349-qkecjqvc/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run qkecjqvc errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2hlf3iz9 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195415-2hlf3iz9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33munique-sweep-117\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/2hlf3iz9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33munique-sweep-117\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/2hlf3iz9\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195415-2hlf3iz9/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2hlf3iz9 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w3esy4on with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195451-w3esy4on\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwandering-sweep-118\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/w3esy4on\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwandering-sweep-118\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/w3esy4on\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195451-w3esy4on/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run w3esy4on errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sys24qro with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195516-sys24qro\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhelpful-sweep-119\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/sys24qro\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhelpful-sweep-119\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/sys24qro\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195516-sys24qro/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run sys24qro errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: js1blajt with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195542-js1blajt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mworldly-sweep-120\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/js1blajt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mworldly-sweep-120\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/js1blajt\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195542-js1blajt/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run js1blajt errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mcgt85n5 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195608-mcgt85n5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvaliant-sweep-121\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/mcgt85n5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvaliant-sweep-121\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/mcgt85n5\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195608-mcgt85n5/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run mcgt85n5 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o5k6zcxu with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195634-o5k6zcxu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mabsurd-sweep-122\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/o5k6zcxu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mabsurd-sweep-122\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/o5k6zcxu\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195634-o5k6zcxu/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run o5k6zcxu errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v3boyv2o with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195659-v3boyv2o\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfaithful-sweep-123\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/v3boyv2o\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfaithful-sweep-123\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/v3boyv2o\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195659-v3boyv2o/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run v3boyv2o errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ac0ti0r7 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195726-ac0ti0r7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvolcanic-sweep-124\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ac0ti0r7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvolcanic-sweep-124\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ac0ti0r7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195726-ac0ti0r7/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ac0ti0r7 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jxzweo55 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195752-jxzweo55\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdrawn-sweep-125\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jxzweo55\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdrawn-sweep-125\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/jxzweo55\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195752-jxzweo55/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jxzweo55 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xpvl1zhx with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195817-xpvl1zhx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msweet-sweep-126\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xpvl1zhx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msweet-sweep-126\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xpvl1zhx\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195817-xpvl1zhx/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xpvl1zhx errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xpwgse0b with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195843-xpwgse0b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbreezy-sweep-127\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xpwgse0b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbreezy-sweep-127\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xpwgse0b\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195843-xpwgse0b/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xpwgse0b errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xu6rohso with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195909-xu6rohso\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mapricot-sweep-128\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xu6rohso\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mapricot-sweep-128\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/xu6rohso\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195909-xu6rohso/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xu6rohso errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ctq2g4a0 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_195935-ctq2g4a0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msandy-sweep-129\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ctq2g4a0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msandy-sweep-129\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/ctq2g4a0\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_195935-ctq2g4a0/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ctq2g4a0 errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v9fsupnz with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20240514_200006-v9fsupnz\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdriven-sweep-130\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/sweeps/f4esgkqv\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/v9fsupnz\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","  File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","    source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","  File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","    return torch.tensor(chars_indexes, device=device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdriven-sweep-130\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention/runs/v9fsupnz\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/jaswanth431/DL%20Assignment%203%20With%20Attention\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240514_200006-v9fsupnz/logs\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run v9fsupnz errored:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/3376784685.py\", line 27, in main\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/967532991.py\", line 14, in prepare_dataloaders\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 66, in preprocess_data\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data['source_data_seq'] = generate_string_to_sequence(padded_source_strings,  data['source_char_index'])\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 17, in generate_string_to_sequence\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_25/2795315630.py\", line 26, in get_chars\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.tensor(chars_indexes, device=device)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 2.12 MiB is free. Process 2208 has 15.89 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 430.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"]}],"source":["wandb.agent(\"f4esgkqv\", function=main, count=100)"]},{"cell_type":"code","execution_count":14,"id":"932fca18","metadata":{"execution":{"iopub.execute_input":"2024-05-14T20:00:27.94562Z","iopub.status.busy":"2024-05-14T20:00:27.944842Z","iopub.status.idle":"2024-05-14T20:00:28.422111Z","shell.execute_reply":"2024-05-14T20:00:28.421141Z"},"papermill":{"duration":0.889085,"end_time":"2024-05-14T20:00:28.42419","exception":false,"start_time":"2024-05-14T20:00:27.535105","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["15860"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","torch.cuda.empty_cache()\n","import gc\n","gc.collect()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4891846,"sourceId":8245488,"sourceType":"datasetVersion"},{"datasetId":4899019,"sourceId":8255378,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":5127.733868,"end_time":"2024-05-14T20:00:31.72193","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-14T18:35:03.988062","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}