{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://www.kaggle.com/code/jaswanth431/dl-assignement-3-v2?scriptVersionId=175612901\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"25f71506-5c69-4ad9-8ae9-7c3d144e11b6","_uuid":"790ab3c6-fa40-4fa9-a2b2-6c9e827583d0","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:51:58.750422Z","iopub.status.busy":"2024-05-16T17:51:58.750039Z","iopub.status.idle":"2024-05-16T17:51:58.757539Z","shell.execute_reply":"2024-05-16T17:51:58.756536Z","shell.execute_reply.started":"2024-05-16T17:51:58.750389Z"},"jupyter":{"outputs_hidden":false},"scrolled":true,"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","import copy\n","from torch.utils.data import Dataset, DataLoader\n","import gc\n","import random\n","import wandb"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T17:51:58.760147Z","iopub.status.busy":"2024-05-16T17:51:58.759596Z","iopub.status.idle":"2024-05-16T17:51:58.774773Z","shell.execute_reply":"2024-05-16T17:51:58.773810Z","shell.execute_reply.started":"2024-05-16T17:51:58.760113Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"data":{"text/plain":["True"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login(key=\"62cfafb7157dfba7fdd6132ac9d757ccd913aaaf\")"]},{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"96cccdd5-ba57-4899-b0a4-6c89b979a180","_uuid":"de579fbe-b990-4a9b-9772-39f6140fb8af","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:51:58.776461Z","iopub.status.busy":"2024-05-16T17:51:58.776111Z","iopub.status.idle":"2024-05-16T17:51:58.900637Z","shell.execute_reply":"2024-05-16T17:51:58.899486Z","shell.execute_reply.started":"2024-05-16T17:51:58.776430Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["# Set the device for training to CUDA if available, otherwise CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","# Define special tokens for start, end, and padding\n","END_TOKEN = '>'\n","START_TOKEN = '<'\n","PAD_TOKEN = '_'\n","\n","# Define the teacher forcing ratio, which determines the probability of using teacher forcing during training\n","TEACHER_FORCING_RATIO = 0.5\n","\n","# Paths to the training, testing, and validation CSV files\n","train_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_train.csv\"\n","test_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_test.csv\"\n","val_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_valid.csv\"\n","\n","# Load the training data from the CSV file into a DataFrame\n","train_df = pd.read_csv(train_csv, header=None)\n","\n","# Separate the source and target sequences from the training DataFrame\n","train_source, train_target = train_df[0].to_numpy(), train_df[1].to_numpy()\n","\n","# Load the testing and validation data from the respective CSV files into DataFrames\n","test_df = pd.read_csv(test_csv, header=None)\n","val_df = pd.read_csv(val_csv, header=None)\n","\n","# Separate the source and target sequences from the validation DataFrame\n","val_source, val_target = val_df[0].to_numpy(), val_df[1].to_numpy()"]},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"65d72dbd-08c1-417f-8101-7e4aa924a6b7","_uuid":"0c3f2154-423f-4ef0-a4a1-c7c1423e2050","collapsed":false,"execution":{"iopub.execute_input":"2024-05-16T17:51:58.904643Z","iopub.status.busy":"2024-05-16T17:51:58.904314Z","iopub.status.idle":"2024-05-16T17:51:58.925292Z","shell.execute_reply":"2024-05-16T17:51:58.924410Z","shell.execute_reply.started":"2024-05-16T17:51:58.904617Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Function to add padding to source sequences\n","def add_padding(source_data, MAX_LENGTH):\n","    padded_source_strings = []\n","    for i in range(len(source_data)):\n","        # Add start and end tokens to source sequence\n","        source_str = START_TOKEN + source_data[i] + END_TOKEN\n","        # Truncate or pad source sequence\n","        source_str = source_str[:MAX_LENGTH]\n","        source_str += PAD_TOKEN * (MAX_LENGTH - len(source_str))\n","\n","        padded_source_strings.append(source_str)\n","        \n","    return padded_source_strings\n","\n","# Function to convert source strings to sequences of indexes\n","def generate_string_to_sequence(source_data, source_char_index_dict):\n","    source_sequences = []\n","    for i in range(len(source_data)):\n","        # Convert characters to indexes using the provided dictionary\n","        source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n","    # Pad sequences to the same length\n","    source_sequences = pad_sequence(source_sequences, batch_first=True, padding_value=2)\n","    return source_sequences\n","\n","# Function to convert characters to their corresponding indexes\n","def get_chars(string, char_index_dict):\n","    chars_indexes = []\n","    for char in string:\n","        # Map characters to their corresponding indexes using the provided dictionary\n","        chars_indexes.append(char_index_dict[char])\n","    return torch.tensor(chars_indexes, device=device)\n","\n","# Preprocess the data, including adding padding, generating sequences, and updating dictionaries\n","def preprocess_data(source_data, target_data):\n","    data = {\n","        \"source_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n","        \"target_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n","        \"source_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n","        \"source_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n","        \"target_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n","        \"target_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n","        \"source_len\": 3,\n","        \"target_len\": 3,\n","        \"source_data\": source_data,\n","        \"target_data\": target_data,\n","        \"source_data_seq\": [],\n","        \"target_data_seq\": []\n","    }\n","    \n","    # Calculate the maximum length of input and output sequences\n","    data[\"INPUT_MAX_LENGTH\"] = max(len(string) for string in source_data) + 2\n","    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n","\n","    # Pad the source and target sequences and update character dictionaries\n","    padded_source_strings = add_padding(source_data, data[\"INPUT_MAX_LENGTH\"])\n","    padded_target_strings = add_padding(target_data, data[\"OUTPUT_MAX_LENGTH\"])\n","    \n","    for i in range(len(padded_source_strings)):\n","        for c in padded_source_strings[i]:\n","            if data[\"source_char_index\"].get(c) is None:\n","                data[\"source_chars\"].append(c)\n","                idx = len(data[\"source_chars\"]) - 1\n","                data[\"source_char_index\"][c] = idx\n","                data[\"source_index_char\"][idx] = c\n","        for c in padded_target_strings[i]:\n","            if data[\"target_char_index\"].get(c) is None:\n","                data[\"target_chars\"].append(c)\n","                idx = len(data[\"target_chars\"]) - 1\n","                data[\"target_char_index\"][c] = idx\n","                data[\"target_index_char\"][idx] = c\n","\n","    # Generate sequences of indexes for source and target data\n","    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings, data['source_char_index'])\n","    data['target_data_seq'] = generate_string_to_sequence(padded_target_strings, data['target_char_index'])\n","    \n","\n","    # Update lengths of source and target character lists\n","    data[\"source_len\"] = len(data[\"source_chars\"])\n","    data[\"target_len\"] = len(data[\"target_chars\"])\n","    \n","    return data\n"]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"64c71a02-dc40-465e-8aef-83951c8347f6","_uuid":"d6ff26fc-b5de-4249-88b2-ea263b8b37c8","execution":{"iopub.execute_input":"2024-05-16T17:51:58.927456Z","iopub.status.busy":"2024-05-16T17:51:58.926825Z","iopub.status.idle":"2024-05-16T17:51:58.947387Z","shell.execute_reply":"2024-05-16T17:51:58.946293Z","shell.execute_reply.started":"2024-05-16T17:51:58.927424Z"},"trusted":true},"outputs":[],"source":["# Function to get the appropriate cell type based on the input string\n","def get_cell_type(cell_type):\n","    if cell_type == \"RNN\":\n","        return nn.RNN\n","    elif cell_type == \"LSTM\":\n","        return nn.LSTM\n","    elif cell_type == \"GRU\":\n","        return nn.GRU\n","    else:\n","        print(\"Specify correct cell type\")\n","\n","# Encoder module for the seq2seq model\n","class Encoder(nn.Module):\n","    def __init__(self, h_params, data, device):\n","        super(Encoder, self).__init__()\n","        # Embedding layer for input data\n","        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embd_dim\"])\n","        # Dropout layer for regularization\n","        self.dropout = nn.Dropout(h_params[\"dropout\"])\n","        # Cell type chosen for encoding\n","        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],\n","                                                         num_layers=h_params[\"number_of_layers\"], dropout=h_params[\"dropout\"], batch_first=True)\n","        self.device = device\n","        self.h_params = h_params\n","\n","    def forward(self, current_input, prev_state):\n","        # Embedding input\n","        embd_input = self.embedding(current_input)\n","        embd_input = self.dropout(embd_input)\n","        # Encoding step\n","        output, prev_state = self.cell(embd_input, prev_state)\n","        return output, prev_state\n","\n","    # Initialize initial state of the encoder\n","    def getInitialState(self):\n","        return torch.zeros(self.h_params[\"number_of_layers\"], self.h_params[\"batch_size\"], self.h_params[\"hidden_layer_neurons\"], device=self.device)\n","\n","# Decoder module for the seq2seq model\n","class Decoder(nn.Module):\n","    def __init__(self, h_params, data, device):\n","        super(Decoder, self).__init__()\n","        # Embedding layer for target data\n","        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embd_dim\"])\n","        # Dropout layer for regularization\n","        self.dropout = nn.Dropout(h_params[\"dropout\"])\n","        # Cell type chosen for decoding\n","        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],\n","                                                         num_layers=h_params[\"number_of_layers\"], dropout=h_params[\"dropout\"], batch_first=True)\n","        # Fully connected layer for output\n","        self.fc = nn.Linear(h_params[\"hidden_layer_neurons\"], data[\"target_len\"])\n","        # Softmax layer for probability distribution\n","        self.softmax = nn.LogSoftmax(dim=2)\n","        self.h_params = h_params\n","\n","    def forward(self, current_input, prev_state):\n","        # Embedding input\n","        embd_input = self.embedding(current_input)\n","        curr_embd = F.relu(embd_input)\n","        curr_embd = self.dropout(curr_embd)\n","        # Decoding step\n","        output, prev_state = self.cell(curr_embd, prev_state)\n","        # Apply softmax to output\n","        output = self.softmax(self.fc(output))\n","        return output, prev_state\n"]},{"cell_type":"code","execution_count":13,"metadata":{"_cell_guid":"48023388-f9f6-43b9-9875-6957e3394472","_uuid":"eef0859f-56b6-4d6d-b89b-9bed47039138","execution":{"iopub.execute_input":"2024-05-16T17:51:58.949280Z","iopub.status.busy":"2024-05-16T17:51:58.948938Z","iopub.status.idle":"2024-05-16T17:51:58.963163Z","shell.execute_reply":"2024-05-16T17:51:58.962242Z","shell.execute_reply.started":"2024-05-16T17:51:58.949250Z"},"trusted":true},"outputs":[],"source":["# Custom dataset class for handling source and target sequences\n","class MyDataset(Dataset):\n","    def __init__(self, data):\n","        # Initialize with source and target sequences\n","        self.source_data_seq = data[0]\n","        self.target_data_seq = data[1]\n","    \n","    def __len__(self):\n","        # Return the length of the dataset\n","        return len(self.source_data_seq)\n","    \n","    def __getitem__(self, idx):\n","        # Get source and target data at the specified index\n","        source_data = self.source_data_seq[idx]\n","        target_data = self.target_data_seq[idx]\n","        return source_data, target_data\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T17:51:58.964742Z","iopub.status.busy":"2024-05-16T17:51:58.964402Z","iopub.status.idle":"2024-05-16T17:51:58.983670Z","shell.execute_reply":"2024-05-16T17:51:58.982759Z","shell.execute_reply.started":"2024-05-16T17:51:58.964712Z"},"trusted":true},"outputs":[],"source":["# Function for inference on the trained model\n","def inference(encoder, decoder, source_sequence, target_tensor, data, device, h_params, loss_fn, batch_num):\n","    # Set encoder and decoder to evaluation mode\n","    encoder.eval()\n","    decoder.eval()\n","    \n","    # Initialize loss and correct predictions\n","    loss = 0\n","    correct = 0\n","    \n","    # Turn off gradient calculation\n","    with torch.no_grad():\n","        # Initialize encoder hidden state\n","        encoder_hidden = encoder.getInitialState()\n","        # For LSTM, initialize cell state as well\n","        if h_params[\"cell_type\"] == \"LSTM\":\n","            encoder_hidden = (encoder_hidden, encoder.getInitialState())\n","        # Perform encoding\n","        encoder_outputs, encoder_hidden = encoder(source_sequence, encoder_hidden)\n","\n","        # Initialize decoder input with start token\n","        decoder_input_tensor = torch.full((h_params[\"batch_size\"], 1), data['target_char_index'][START_TOKEN], device=device)\n","        decoder_actual_output = []\n","        \n","        # Initialize decoder hidden state with encoder hidden state\n","        decoder_hidden = encoder_hidden\n","        \n","        # Iterate over each output time step\n","        for di in range(data[\"OUTPUT_MAX_LENGTH\"]):\n","            curr_target_chars = target_tensor[:, di]\n","            # Perform decoding for one time step\n","            decoder_output, decoder_hidden = decoder(decoder_input_tensor, decoder_hidden)\n","            topv, topi = decoder_output.topk(1)\n","            decoder_input_tensor = topi.squeeze().detach()\n","            decoder_actual_output.append(decoder_input_tensor)\n","            decoder_input_tensor = decoder_input_tensor.view(h_params[\"batch_size\"], 1)\n","                        \n","            decoder_output = decoder_output[:, -1, :]\n","            # Compute loss for the current time step\n","            loss += (loss_fn(decoder_output, curr_target_chars))\n","\n","        # Concatenate decoder outputs\n","        decoder_actual_output = torch.cat(decoder_actual_output, dim=0).view(data[\"OUTPUT_MAX_LENGTH\"], h_params[\"batch_size\"]).transpose(0, 1)\n","\n","        # Compute number of correct predictions\n","        correct = (decoder_actual_output == target_tensor).all(dim=1).sum().item()\n","\n","        return correct, loss.item() / data[\"OUTPUT_MAX_LENGTH\"]\n","\n","# Function to evaluate the model on validation or test data\n","def evaluate(encoder, decoder, data, dataloader, device, h_params, loss_fn):\n","    correct_predictions = 0\n","    total_loss = 0\n","    total_predictions = len(dataloader.dataset)\n","    number_of_batches = len(dataloader)\n","    \n","    # Iterate over each batch\n","    for batch_num, (source_sequence, target_sequence) in enumerate(dataloader):\n","        input_tensor = source_sequence\n","        target_tensor = target_sequence\n","        \n","        # Perform inference on the batch\n","        correct, loss = inference(encoder, decoder, input_tensor, target_tensor, data, device, h_params, loss_fn, batch_num)\n","        \n","        correct_predictions += correct\n","        total_loss += loss\n","    \n","    # Compute accuracy and average loss\n","    accuracy = correct_predictions / total_predictions\n","    total_loss /= number_of_batches\n","    \n","    return accuracy, total_loss\n"]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"b704c17c-662e-49d1-93df-5eefeadf24a5","_uuid":"f704842a-033b-40ea-af52-bccdea8a5014","execution":{"iopub.execute_input":"2024-05-16T17:51:58.985674Z","iopub.status.busy":"2024-05-16T17:51:58.985352Z","iopub.status.idle":"2024-05-16T17:51:59.008546Z","shell.execute_reply":"2024-05-16T17:51:59.007578Z","shell.execute_reply.started":"2024-05-16T17:51:58.985644Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["# Function to create strings from indexes\n","def make_strings(data, source, target, output):\n","    source_string = \"\"\n","    target_string = \"\"\n","    output_string = \"\"\n","    # Convert source indexes to characters\n","    for i in source:\n","        source_string += (data['source_index_char'][i.item()])\n","    # Convert target indexes to characters\n","    for i in target:\n","        target_string += (data['target_index_char'][i.item()])\n","    # Convert output indexes to characters\n","    for i in output:\n","        output_string += (data['target_index_char'][i.item()])\n","    return source_string, target_string, output_string\n","\n","# Training loop\n","def train_loop(encoder, decoder, h_params, data, data_loader, val_dataloader, device):\n","    # Choose optimizer based on the specified type\n","    if h_params[\"optimizer\"] == \"adam\":\n","        encoder_optimizer = optim.Adam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n","        decoder_optimizer = optim.Adam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n","    elif h_params[\"optimizer\"] == \"nadam\":\n","        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n","        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n","    \n","    total_predictions = len(data_loader.dataset)\n","    total_batches = len(data_loader)\n","    loss_fn = nn.NLLLoss()\n","    \n","    # Loop through each epoch\n","    for ep in range(h_params[\"epochs\"]):\n","        encoder.train()\n","        decoder.train()\n","        total_loss = 0\n","        total_correct = 0\n","        \n","        # Loop through each batch\n","        for batch_num, (source_batch, target_batch) in enumerate(data_loader):\n","            # Get initial state of the encoder\n","            encoder_initial_state = encoder.getInitialState()\n","            if h_params[\"cell_type\"] == \"LSTM\":\n","                encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n","            \n","            encoder_current_state = encoder_initial_state\n","            encoder_output, encoder_current_state = encoder(source_batch, encoder_current_state)\n","            loss = 0\n","            correct = 0\n","            decoder_curr_state = encoder_current_state\n","            output_seq_len = data[\"OUTPUT_MAX_LENGTH\"]\n","            decoder_actual_output = []\n","            \n","            # Determine whether to use teacher forcing\n","            use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n","\n","            # Loop through each output time step\n","            for i in range(data[\"OUTPUT_MAX_LENGTH\"]):\n","                if(i == 0):\n","                    decoder_input_tensor = target_batch[:, i].view(h_params[\"batch_size\"], 1)\n","                curr_target_chars = target_batch[:, i]\n","                decoder_output, decoder_curr_state = decoder(decoder_input_tensor, decoder_curr_state)\n","                topv, topi = decoder_output.topk(1)\n","                decoder_input_tensor = topi.squeeze().detach()\n","                decoder_actual_output.append(decoder_input_tensor)\n","                if(i < output_seq_len - 1):\n","                    if use_teacher_forcing:\n","                        decoder_input_tensor = target_batch[:, i + 1].view(h_params[\"batch_size\"], 1)\n","                    else:\n","                        decoder_input_tensor = decoder_input_tensor.view(h_params[\"batch_size\"], 1)\n","                decoder_output = decoder_output[:, -1, :]\n","                loss += (loss_fn(decoder_output, curr_target_chars))\n","\n","            decoder_actual_output = torch.cat(decoder_actual_output, dim=0).view(output_seq_len, h_params[\"batch_size\"]).transpose(0, 1)\n","            \n","            correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n","            total_correct += correct\n","            total_loss += loss.item() / output_seq_len\n","            \n","            # Backpropagation and optimization step\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","        \n","        train_acc = total_correct / total_predictions\n","        train_loss = total_loss / total_batches\n","        \n","        # Evaluate on validation set\n","        val_acc, val_loss = evaluate(encoder, decoder, data, val_dataloader, device, h_params, loss_fn)\n","        \n","        # Log metrics\n","        print(\"ep: \", ep, \" train acc:\", train_acc, \" train loss:\", train_loss, \" val acc:\", val_acc, \" val loss:\", val_loss)\n","        wandb.log({\"train_accuracy\": train_acc, \"train_loss\": train_loss, \"val_accuracy\": val_acc, \"val_loss\": val_loss, \"epoch\": ep})\n","\n","    return encoder, decoder, loss_fn\n"]},{"cell_type":"code","execution_count":16,"metadata":{"_cell_guid":"85d681a3-36e8-43ad-829f-2a14fb95a810","_kg_hide-input":true,"_uuid":"57e95586-3f42-4dad-aed7-0bb263f699df","execution":{"iopub.execute_input":"2024-05-16T17:51:59.011253Z","iopub.status.busy":"2024-05-16T17:51:59.010502Z","iopub.status.idle":"2024-05-16T17:51:59.021436Z","shell.execute_reply":"2024-05-16T17:51:59.020508Z","shell.execute_reply.started":"2024-05-16T17:51:59.011217Z"},"trusted":true},"outputs":[],"source":["# Training function\n","def train(h_params, data, device, train_dataloader, val_dataloader):\n","    # Initialize encoder and decoder\n","    encoder = Encoder(h_params, data, device).to(device)\n","    decoder = Decoder(h_params, data, device).to(device)\n","    \n","    # Perform training loop\n","    encoder, decoder, loss_fn = train_loop(encoder, decoder, h_params, data, train_dataloader, val_dataloader, device)\n","    return encoder, decoder, loss_fn\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-16T17:54:12.819640Z","iopub.status.busy":"2024-05-16T17:54:12.818916Z","iopub.status.idle":"2024-05-16T17:54:12.829484Z","shell.execute_reply":"2024-05-16T17:54:12.828520Z","shell.execute_reply.started":"2024-05-16T17:54:12.819607Z"},"trusted":true},"outputs":[],"source":["# h_params = {\n","#     \"char_embd_dim\": 256,\n","#     \"hidden_layer_neurons\": 256,\n","#     \"batch_size\": 32,\n","#     \"number_of_layers\": 3,\n","#     \"learning_rate\": 0.0001,\n","#     \"epochs\": 20,\n","#     \"cell_type\": \"LSTM\",\n","#     \"dropout\": 0,\n","#     \"optimizer\": \"adam\"\n","# }\n","\n","# Function to prepare dataloaders for training and validation\n","def prepare_dataloaders(train_source, train_target, val_source, val_target, h_params):\n","    # Preprocess training data\n","    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n","    training_data = [data[\"source_data_seq\"], data['target_data_seq']]\n","    # Create training dataset and dataloader\n","    train_dataset = MyDataset(training_data)\n","    train_dataloader = DataLoader(train_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n","\n","    # Preprocess validation data\n","    val_padded_source_strings = add_padding(val_source, data[\"INPUT_MAX_LENGTH\"])\n","    val_padded_target_strings = add_padding(val_target, data[\"OUTPUT_MAX_LENGTH\"])\n","    val_source_sequences = generate_string_to_sequence(val_padded_source_strings, data['source_char_index'])\n","    val_target_sequences = generate_string_to_sequence(val_padded_target_strings, data['target_char_index'])\n","    validation_data = [val_source_sequences, val_target_sequences]\n","    # Create validation dataset and dataloader\n","    val_dataset = MyDataset(validation_data)\n","    val_dataloader = DataLoader(val_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n","    return train_dataloader, val_dataloader, data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["config = h_params\n","run = wandb.init(project=\"DL Assignment 3\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config)\n","train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, h_params)\n","train(h_params, data, device, train_dataloader, val_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:54:06.975886Z","iopub.status.idle":"2024-05-16T17:54:06.976384Z","shell.execute_reply":"2024-05-16T17:54:06.976141Z","shell.execute_reply.started":"2024-05-16T17:54:06.976122Z"},"trusted":true},"outputs":[],"source":["#Run this cell to run a sweep with appropriate parameters\n","sweep_params = {\n","    'method' : 'bayes',\n","    'name'   : 'DL assignment 3 sweep',\n","    'metric' : {\n","        'goal' : 'maximize',\n","        'name' : 'val_accuracy',\n","    },\n","    'parameters' : {\n","        'epochs':{'values' : [15, 20]},\n","        'learning_rate':{'values' : [0.001, 0.0001]},\n","        'batch_size':{'values':[32,64, 128]},\n","        'char_embd_dim':{'values' : [64, 128, 256] } ,\n","        'number_of_layers':{'values' : [1,2,3,4]},\n","        'optimizer':{'values':['nadam','adam']},\n","        'cell_type':{'values' : [\"RNN\",\"LSTM\", \"GRU\"]},\n","        'hidden_layer_neurons':{'values': [ 128, 256, 512]},\n","        'dropout':{'values': [0,0.2, 0.3]}\n","    }\n","}\n","\n","sweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 3\")\n","def main():\n","    wandb.init(project=\"DL Assignment 3\" )\n","    config = wandb.config\n","    with wandb.init(project=\"DL Assignment 3\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config):\n","        train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n","        train(config, data, device, train_dataloader, val_dataloader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-05-16T17:54:06.978163Z","iopub.status.idle":"2024-05-16T17:54:06.978557Z","shell.execute_reply":"2024-05-16T17:54:06.978400Z","shell.execute_reply.started":"2024-05-16T17:54:06.978385Z"},"trusted":true},"outputs":[],"source":["wandb.agent(\"hw3b5jng\", function=main, count=100)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4891846,"sourceId":8245488,"sourceType":"datasetVersion"},{"datasetId":4899019,"sourceId":8255378,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
